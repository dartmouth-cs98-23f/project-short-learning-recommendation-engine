{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This file produces data for the model to use\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import time\n",
    "import math, random\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "## Load Config\n",
    "with open('config/videos.json') as config_file:\n",
    "    videos = json.load(config_file)\n",
    "with open('config/name_to_url.json') as config_file:\n",
    "    name_to_url = json.load(config_file)\n",
    "with open('config/alltopics.json') as config_file:\n",
    "    alltopics = json.load(config_file)\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klinalb/anaconda3/envs/CS98/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "CUR_DIR = os.getcwd()\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_KEY\"])\n",
    "index = pc.Index(\"pretechnigala\")\n",
    "DB_NAME = \"preTechnigalaClean_db\"\n",
    "COLLECTION_NAME = \"video_metadata\"\n",
    "MONGO_DB_CLIENT = MongoClient(os.getenv(\"MONGODB_URI\"), server_api=ServerApi('1'))\n",
    "OUTPUT_DIR = f\"{CUR_DIR}/data/validation\"\n",
    "QUERY_MODE = \"avg\"  \n",
    "VECTOR_MODE = \"avg\"\n",
    "PATH_TO_EMBEDDINGS = f\"{OUTPUT_DIR}/embeddings/{QUERY_MODE}\"\n",
    "PATH_TO_DB_VECTORS = \"/Users/klinalb/Workspaces/dartmouth/CS98/discite/rec_engine/inference/pipeline/data/outputs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PERFORMS QUERIES BASED ON JUST THE VIDEO EMBEDDINGS\n",
    "with open(f'{OUTPUT_DIR}/video_query/{VECTOR_MODE}-v_{QUERY_MODE}-q.yaml', 'w') as f:\n",
    "    for embedding_file in os.listdir(PATH_TO_EMBEDDINGS):\n",
    "        if not embedding_file.endswith('.pt'):\n",
    "            continue\n",
    "        name = embedding_file[4:-3]\n",
    "        query = torch.load(f'{PATH_TO_EMBEDDINGS}/{embedding_file}', map_location=torch.device('cpu')).tolist()\n",
    "        response = index.query(vector=query, top_k=5, include_values=True, include_metadata=True, filter={\"mode\": f\"{VECTOR_MODE}_pool\"})\n",
    "        \n",
    "        f.write(f'Querying {name}:\\n')\n",
    "        for i, obj in enumerate(response[\"matches\"]):\n",
    "            youtubeURL = \"None\"\n",
    "            doc = MONGO_DB_CLIENT[DB_NAME][COLLECTION_NAME].find_one({\"_id\": ObjectId(obj[\"metadata\"][\"videoID\"])})\n",
    "            if doc:\n",
    "                youtubeURL = doc[\"youtubeURL\"]\n",
    "            f.write(f'    Rank: {i+1}, Distance: {obj[\"score\"]}\\n')\n",
    "            f.write(f'        Title: {obj[\"metadata\"][\"title\"]}, URL: {youtubeURL}\\n')\n",
    "            f.write(f'        Topics: {[alltopics[topic] for topic in obj[\"metadata\"][\"topics\"]]}\\n')\n",
    "            f.write(f'        infTopics: {obj[\"metadata\"][\"inferenceTopics\"]} \\n')\n",
    "            f.write(f'        infComplexities: {obj[\"metadata\"][\"inferenceComplexities\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## History Query TESTS\n",
    "## attempt 3. Checking just two videos, and the distance between them. \n",
    "\n",
    "\n",
    "ATTEMPT = 4\n",
    "HISTORY_LENGTH = 2\n",
    "TOP_K = 15\n",
    "QUERIES = 10\n",
    "SINK = torch.load(f'{PATH_TO_DB_VECTORS}/65d8fc3f95f306b28d1b88fe/{VECTOR_MODE}_pool.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "os.makedirs(f'{OUTPUT_DIR}/history_query/{VECTOR_MODE}-v_{QUERY_MODE}-q', exist_ok=True)\n",
    "all_vectors = [] #NAME, VECTOR\n",
    "for embedding_file in os.listdir(PATH_TO_EMBEDDINGS):\n",
    "    if not embedding_file.endswith('.pt'):\n",
    "        continue\n",
    "    all_vectors.append((embedding_file[4:-3], torch.load(f'{PATH_TO_EMBEDDINGS}/{embedding_file}', map_location=torch.device('cpu'))))\n",
    "\n",
    "## Randomly make a \"watch history\" by taking the average of 5 random videos\n",
    "with open(f'{OUTPUT_DIR}/history_query/{VECTOR_MODE}-v_{QUERY_MODE}-q/{ATTEMPT}.yaml', 'w') as f:\n",
    "    f.write(f'Parameters: Attempt {ATTEMPT}, History Length {HISTORY_LENGTH}, Top K {TOP_K}, Query Mode {QUERY_MODE}, Vector Mode {VECTOR_MODE}\\n\\n')\n",
    "    for run in range(QUERIES):\n",
    "        f.write(f'\\n###################### Run {run+1} #######################\\n\\n')\n",
    "        history = random.sample(all_vectors, HISTORY_LENGTH)\n",
    "        ## Query is the average pool of the history vectors\n",
    "        query = torch.mean(torch.stack([vec for name, vec in history]), dim=0).numpy().tolist()\n",
    "\n",
    "        f.write(f'Distance Between History 0,1: {torch.dist(history[0][1], history[1][1])}\\n')\n",
    "        f.write(f'Distance from sink video to 0, 1, and query: {torch.dist(SINK, history[0][1])}, {torch.dist(SINK, history[1][1])}, {torch.dist(SINK, torch.tensor(query))}\\n\\n')\n",
    "\n",
    "        response = index.query(vector=query, top_k=15, include_values=True, include_metadata=True, filter={\"mode\": f\"{VECTOR_MODE}_pool\"})\n",
    "        f.write(f'Querying History {run}. Videos: {[name for name, vec in history]}\\n')\n",
    "        for i, obj in enumerate(response[\"matches\"]):\n",
    "            youtubeURL = \"None\"\n",
    "            doc = MONGO_DB_CLIENT[DB_NAME][COLLECTION_NAME].find_one({\"_id\": ObjectId(obj[\"metadata\"][\"videoID\"])})\n",
    "            if doc:\n",
    "                youtubeURL = doc[\"youtubeURL\"]\n",
    "            f.write(f'    Rank: {i+1}, Distance: {obj[\"score\"]}\\n')\n",
    "            f.write(f'        Title: {obj[\"metadata\"][\"title\"]}, URL: {youtubeURL}, ID: {obj[\"metadata\"][\"videoID\"]}\\n')\n",
    "            f.write(f'        Topics: {[alltopics[topic] for topic in obj[\"metadata\"][\"topics\"]]}\\n')\n",
    "            f.write(f'        infTopics: {obj[\"metadata\"][\"inferenceTopics\"]} \\n')\n",
    "            f.write(f'        infComplexities: {obj[\"metadata\"][\"inferenceComplexities\"]}\\n')\n",
    "        print(f\"Run {run+1} of {QUERIES} complete\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TOPIC TEST\n",
    "ATTEMPT = 0\n",
    "os.makedirs(f'{OUTPUT_DIR}/topic_query/{VECTOR_MODE}-v_{QUERY_MODE}-q', exist_ok=True)\n",
    "with open(f'{OUTPUT_DIR}/topic_query/{VECTOR_MODE}-v_{QUERY_MODE}-q/{0}.yaml', 'w') as f:\n",
    "    for topic in alltopics:\n",
    "            query = torch.load(f'{CUR_DIR}/data/topics/{topic}.pt', map_location=torch.device('cpu')).tolist()\n",
    "            response = index.query(vector=query, top_k=5, include_values=True, include_metadata=True, filter={\"mode\": f\"{VECTOR_MODE}_pool\"})\n",
    "            f.write(f'Querying {topic}. Topic Name: {alltopics[topic]}\\n')\n",
    "            for i, obj in enumerate(response[\"matches\"]):\n",
    "                youtubeURL = \"None\"\n",
    "                doc = MONGO_DB_CLIENT[DB_NAME][COLLECTION_NAME].find_one({\"_id\": ObjectId(obj[\"metadata\"][\"videoID\"])})\n",
    "                if doc:\n",
    "                    youtubeURL = doc[\"youtubeURL\"]\n",
    "                f.write(f'    Rank: {i+1}, Distance: {obj[\"score\"]}\\n')\n",
    "                f.write(f'        Title: {obj[\"metadata\"][\"title\"]}, URL: {youtubeURL}\\n')\n",
    "                f.write(f'        Topics: {[alltopics[topic] for topic in obj[\"metadata\"][\"topics\"]]}\\n')\n",
    "                f.write(f'        infTopics: {obj[\"metadata\"][\"inferenceTopics\"]} \\n')\n",
    "                f.write(f'        infComplexities: {obj[\"metadata\"][\"inferenceComplexities\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load all vectors to map from ID to vector\n",
    "def load_all_vectors(mode=\"avg\"):\n",
    "    all_vectors = {}\n",
    "    # if isvectorized is true\n",
    "    docs = MONGO_DB_CLIENT[DB_NAME][COLLECTION_NAME].find({\"isVectorized\": True})\n",
    "    for doc in docs:\n",
    "        all_vectors[str(doc[\"_id\"])] = {\"vector\": torch.load(f'{PATH_TO_DB_VECTORS}/{doc[\"_id\"]}/{mode}_pool.pt', map_location=torch.device('cpu')), \"topics\": doc[\"topicId\"], \"inferenceTopics\": doc[\"inferenceTopics\"], \"inferenceComplexities\": doc[\"inferenceComplexities\"]}\n",
    "    \n",
    "    return all_vectors\n",
    "\n",
    "vectors = load_all_vectors(\"avg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "##### UMAP Visualizations\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "## A list of torch tensors \n",
    "umap_vectors = [obj[\"vector\"] for obj in vectors.values()]\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform([vec.numpy().tolist() for vec in umap_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
