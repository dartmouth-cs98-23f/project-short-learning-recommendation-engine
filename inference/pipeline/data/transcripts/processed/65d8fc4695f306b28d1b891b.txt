from stanford university and like i said before im not you you have access to everything about tim in your program the rest of the the three speakers remaining are younger than me so i dont remember them from my graduate school but i suddenly remember each of them from the time they were graduate students and tim roth gardens see this sort of amazed me in its generality its about its about the price of anarchy i dont know if he is going to talk about this if it doesnt then you should find out since working on the price of anarchy tim went on to work on the prices of everything and more generally study economic mechanisms and their efficiency efficiency and computational perspectives tim also all two books one on an algorithmic game theory and one on on the price of anarchy i guess thats a book of his thesis if you like his talk like im sure we will you should check later hes the website where he has many more talks video talks of his lectures where he explains in much more detail the stuff we talked about today please welcome tim after check one two okay thanks very much evie so my goal for today is to introduce you to a few of the many points of contacts between theoretical computer science and game theory and more broadly economics and you know a lot of the times when youre preparing a talk you kind of know most of what you want to say and its just kind of the beginning its really hard to figure out like what goes on the first slide but but then when youre giving a talk thats its about computing its about game theory at the institute for advanced studies theres kind of like a mandate i think about how you have to start start to talk so its so what the game theory computing have to do with each other well for starters there was a single individual who have heard of courses here at the institute who played a major role in the founding of both and of course im talking about john von neumann who weve already heard about and even you know seen the same picture earlier this morning and you know while he had worked out a lot of his theory of games in the 20s in particular his minmax theorem really you know the game theory really became a major subject and started having a lot of influence in the publication of his book in 44 with oskar morgenstern and just a couple years later is when he started supervising the development of some of the first the worlds you know first computers so first as a consultant on the eniac but then also here as also professor valle i mentioned on the ias machine and so von neumann had some common motivations for both of these activities specifically in military strategy and technology you know but i dont think his his work of these two strands really interacted directly that much and indeed as the 20th century unfolded well there was tons of progress in theoretical computer science and also in theoretical economics those two communities really didnt interact or communicate much and this talk i really mean to contrast that with the 21st century where theres been a very lively and useful conversation between those two fields but before i fast forward to the 21st century i do want to just point out you know one sort of revolutionary idea from each of these two communities in the 20th century both of which really fundamentally shaped the the research thats preceded it including what ill be talking about so first on the on the economic side we have nashs theorem so in the von neumann and morgenstern book in 44 theyre really focusing on twoplayer games of pure competition okay also called zerosum games or when there are many players they were thinking about how coalitions might form and naturally founded non cooperative game theory or you can have any number of players and each player acts as an individual in its own selfinterest and the games werent necessarily competitive that could be or they could have aspects of cooperation so this non cooperative formalism really enabled game theory to in principle be double applied to many more situations that had earlier impossible but even better than that nashs theorem in some sense gave game theorists and economists everything they wanted it gave them a universal existence result namely knowing nothing at all about whatever game you care about just that theres a finite set of players and that each can take on a finite number of actions this game has to have at least one equilibrium point well nash called it an equilibrium point we now call them nash equilibria so im not going to define that formally i think you all have an intuitive sense of what equilibria look like and that will suffice for the talk pretty much you have a bunch of agents and given what theyre doing right now nobody wants to move theyre all perfectly happy and staying where they are now the one the one twist may be and in nash equilibria is you need to allow players to randomize but if you just think about two players playing rockpaperscissors it is clear you want to be able to allow players to hedge over various actions okay so nash equilibria a universal existence giving economists in some sense everything they wanted on the computer science side and this is now a couple decades later we have the invention and development of npcompleteness by cooke karp and levin so this is something obvious and basically np completeness was telling computer scientists that were almost never going to be able to get what we want so what a computer scientists want they want efficient computation they want good algorithms for solving fundamental problems what do i mean by fundamental problems well we want to be able to predict how our proteins going to fold so we can do better drug design we want to have more efficient scheduling of airplanes to make better use of airport resources we might want to in a big social network find meaningful patterns and most computational problems of this sort are whats called npcomplete and for the purposes of my talk you should just interpret empty completes as meaning very difficult to solve computationally at least in some general purpose way so in other words when a computational problem you really care about is npcomplete youve got to be ready to compromise compromise can take many forms and computer scientists have many decades of experience about different ways to do it so for example you can resort to heuristics you can solve your computational problem not exactly but only approximately or you can narrow the domain you can focus on small instances instances with special structure and so forth okay but these do not assuming the p not equal to p np conjecture that avi mentioned these do not admit generalpurpose efficient computation procedures and in some sense theoretical computer science has evolved completely in the long shadow of this widespread intractability and well see how that influences the kinds of contributions computer science has made to economics so what i want to do for the most of this talk now is highlights three i hope lustrated but certainly not exhaustive examples of points of contact again between theoretical cs and economics im gonna begin with a model which you know the one hand is near and dear to my heart but i also think it remains its still one of the most vivid illustrations about how in hindsight its really obvious that computer science and economists had to start talking to each other with the coming of the 21st century so its a application that involves routing traffic through networks now the reason computer scientists got interested this in the late 90s was theyre motivated by communication networks and of course the late 90s is exactly when the internet was exploded so you know engineers have been thinking about routing data for a long time but with the internet really we were for the first time thinking about routing through networks where you had multiple selfinterested users like you do in the internet and so that really called out for game theoretic or economic reasoning so that was the motivation but for the purposes of this talk i encourage you all to just think about something youre all probably very intimately familiar with which is vehicular traffic as youre just driving around on roads the fundamental principle principles are largely the same so let me introduce this model just through an example and this is a very old example discussed by ici pigua in his book the economics of welfare way back in 1920 so in the bay area where i live between stanford and san francisco there are two parallel highways theres highway 101 and theres highway 280 if its 400 in the morning you should definitely take highway 101 its going to be faster when theres no traffic but 101 is more prone to congestion okay so its more it gets its more subject to congestion effects as it gets traffic so we can have a cartoon of that as follows okay so heres one of the endpoints heres the other these are the two highways each edge is labeled with a cost function think of this as describing the amount of delay incurred by traffic as a function of the fraction x of the population thats using that road at any given time so highway 280 lets just say takes an hour no matter what no matter how many people are using it highway 101 lets just abstract it and saying if an x fraction of the traffic uses it it takes x hour so if half the traffics on the 101 its half an hour if everybodys on 101 it takes a full hour okay so the goal was interested in understanding what would selfinterested drivers do and whether or not what they did was a good thing or a bad thing so lets first understand you know what do we expect selfinterested drivers what do we expect the equilibrium to be and then lets ask could we do better if we hypothetically could coordinate their actions so if you were one of these drivers and you think about it for a second you realize its sort of a nobrainer what you should do the way ive set it up so every driver has whats called a dominant strategy one action which is always at least as good as every other and thats to take highway 101 the worst case scenario on highway 101 is that everybodys using it then it takes an hour and thats also the best case scenario on highway 280 so theres no reason not to take 101 so at equilibrium the outcome of selfish behavior we expect 101 to be fully fully congested everybodys taking an hour to get from s to t and thats going to be the only equilibrium otherwise people would want to which back from 280 to 101 so is this good or bad so it turns out this is an illustration of what an economist would call the congestion externality it turns out you could do better than this equilibrium and thats the and the reason its inefficient is because drivers dont take into account the extra congestion their presence causes for other users of the edge so in fact any other traffic pattern when in some sense be better than that green one but an altruistic dictator if who could hypothetically control everybodys routes would be wise to split the traffic fiftyfifty okay between 101 and 280 now the drivers on to idiot takes them an hour just what it took them before but the drivers on 101 get there pretty quickly just a half an hour so the average commute time has dropped from 60 minutes in the green traffic pattern to 45 minutes in the red traffic pattern okay so that shows you something which in hindsight is not surprising you let people do whatever they want the outcome need not be whats an altruistic dictator would have been posed on the system and so to measure this cost of selfish behavior could sue pearson pop dimitru introduced the colorful phrase the price of anarchy and as a mathematical definition thats nothing but the old commute im the equilibrium average delay compared to this optimal ie minimum possible delay ie 60 over 45 ie 43 okay so wed say the price of anarchy in pig news example is 4 over 3 theres a more wellknown example of this routing network which is sort of you know popular its sort of a good thing to know the cocktail party at least if its a sufficiently nerdy cocktail party so is there something called braces paradox this is from 1968 and so heres our initial network so again we have one origin one destination two parallel routes one an x means the same thing as before so one that always takes an hour x it takes proportional to the amount of traffic that are using it now by symmetry at equilibrium the drivers will just load balanced ok those theres no reason to prefer one route over the other so those split 5050 and the commute time will be 90 minutes each way so it seems pretty boring wheres the paradox well the paradox emerges you know suppose google x announces that the newest technology invention is a teleportation device midpoint of the bock map and moreover it has infinite capacity any number of people can use it so what effect does this have on the network well suppose youre one of these drivers its a nobrainer to use the teleportation device okay so if everybody else stays the same it used to take you an hour and a half now its gonna take you 30 plus zero plus 30 minutes thats an hour theres at 60 minutes 30 minutes improvement goris you know youre not the only one thinking this way so actually you expect everybody to make use of this new technology so the new equilibrium after adding the teleporter has everybody on the zigzag path as a consequence the congestion on the upper left and lower right edges theyre now fully congested they take an hour each so the commute times actually gone up from 90 minutes to 120 and this is actually the unique equilibrium in this new network its again the case that the zigzag path individually is a nobrainer strategy a dominant strategy so this is the paradox that when youre dealing with selfish individuals when youre looking at equilibria you intuitively can only add resources make a network only better and yet the outcome is somehow worse for everybody youll notice that if we think about the price of anarchy in the second network so what would be the difference between selfish behavior and the altruistic dictator its again four over three and thats because and even in this new network theres actually no way to make use of the new teleportation device to improve the traffic okay so well explain that coincidence of the second four over three in a second so one thing ill point out given that its a this is sort of general science audience is it braces paradoxes innocence ive shown it to you as a phenomena in traffic networks but its actually something much more basic the same equations that govern the equilibria of these traffic networks also govern the many other notions of equilibria like physical equilibria so you can for example and ive had classes of mine students of mine do this for extra credit you can take a bunch of strings and a bunch of springs and you can tie these strings and springs together into a physical contraption okay and you hang the top of the strings and springs you know from the from some heavy weight you know at the bottom of the table and then you hang a weight from the bottom okay so that stretches everything out right the weight exerts force stretches out the string stretches out the springs and if you build that just so you can take a pair of scissors snip a taut string from the middle of this contraption intuitively weakening it and yet that weight will levitate further off of the ground you can really demonstrate this search for braces paradox on youtube youll get some extra credit projects from my graduate courses now why does that happen one reason so one reason would just be its just braces paradox the equilibrium is governed by the exact same equations the correspondences in elastic strings correspond to these constants 1 0  1 elastic springs correspond to the x travel time corresponds to distance and the flow corresponds to force ok so cutting the taut string just corresponds to deleting the zero edge and recovering the original network a way to see it directly is that when you cut that taut string what really happens is it frees up two springs to carry awaits in parallel ok so to share it the force that it exerts rather than being forced to carry the weights exerted the force exerted by the weights in series okay so it allows two springs to share the load as opposed to each bearing the fulcrum brunt of the exhorted force so thats bryces paradox now lets try to think about you know weve seen these lower bounds on how inefficient things could be and for letting people do totally whatever they want obviously we would have loved it if decentralized optimization resulted in a fully optimal solution were not that surprised to see that its you know less than optimal but maybe were not that discouraged that its pretty close for thirds in these networks so if you have youre a graduate student looking for something to do looking for something to prove you could imagine you know a good rule of thumb is always to be optimistic so like i tell my students always you know whats the coolest thing that could be true but you havent it falsified so we okay we have two examples price of entropy is 43 and both maybe its never bigger than 43 thats sort of the coolest thing that i can figure that could be true at this point so thats pretty easy to show that thats a little bit too good to be true if we just modify piggys example in a very small way so we used to have an x on the toplink lets make that x to the d okay where d is big d is you know 10 20 whatever okay some highly nonlinear function on top so how does this change pickers example well the selfish outcome is no different than before for exactly the same reasoning the worst case here remains one the best case here remains one so again in the equilibrium we expect the top length to be fully congested and everybodys cost to be one whats different is that now with dictatorial control of the network we can do far far better how do we do that we remove an epsilon fraction of the traffic off of the top and reroute them on the bottom okay where epsilon is small d is big epsilon is small so what happens well if you think about it almost all the traffic gets from s to t almost instantaneously one minus epsilon to the d d powers that down to basically zero if d is sufficiently big so almost everybody gets there instantaneously a few martyrs are stuck taking an hour but they contribute very little to the average so as d grows large and epsilon goes small the best possible solution is going to zero so the price of anarchy which is the ratio is going to infinity so theres no universal upper bound on what this price of anarchy can be even in these simplest of routing networks okay but so you cant get this you cant get to gets discouraged in research you just have to sort of take a step back so now what do we know is true we know that the price of anarchy is small in these networks and we know that its big in this one okay and in general it can be arbitrarily big so could it be that there are relatively weak conditions on a network you could impose that guarantee under those conditions the price of anarchy is close to one that the outcome of selfish behavior is not significantly worse than what could be attained with perfect regulation so you know looking at these three networks again if you were feeling kind of very glib you might say well this ones bad it has a nonlinear function here are two different networks that dont have nonlinear functions theyre good so maybe all you need is to just say theres no highly nonlinear functions thats sort of the coolest thing i can think of right now that would be true okay and that actually is true turns out so this is a theorem you can actually prove so consider any network i should say the reason theres something to prove is because networks can get really big and complicated theyre not just two nodes in four nodes but no matter how complicated a gay they get as long as the cost functions have this affine form ax plus b a and b can be whatever you want for different links then youll never see a worse example than the two i just showed you so in particular pig is  no  link example already realizes the maximum possible efficiency loss amongst so these socalled selfish routing networks with a find cost functions and so again this network can be arbitrarily large any number of origins and destinations it doesnt matter you let people do whatever they want and it just wont ever drift that much worse than what you could do with full centralized control so i wasnt planning on proving this theorem but maybe just you know you let me give you sort of a morally why maybe something like this should be true just sort of a you know rough proof by analogy so instead of routing traffic through networks lets think about electrical current in an electrical network okay between two terminals so what do we know about electrical current well so we know that on the one hand they can be it can be thought of as an equilibrium it equalizes the voltage drop across any two paths between the two terminals on the other hand we also know that we can think of electrical current is optimizing something thompsons principle tells us that it minimizes the dissipated energy so thats really nice electro currents simultaneously in equilibrium and its an optimizer turns out electrical current and electrical networks is just a special case of traffic equilibria in these traffic networks it corresponds to networks where every link has a cost function of the form ax okay some of the x over here  x over there 4x over there corresponding the resistances an electrical current is just going to be correspond to the corresponding traffic equilibrium so the two things we know about electrical current actually says that the price of anarchy equals 1 if all the cost functions have the form ax thats what thompsons principle tells us so weve generalized the model a little bit we now have a find cost functions not just these pure linear ones but you might hope that you know we know that its not fully efficient but you might hope that it doesnt break down too badly and indeed traffic equilibria in these networks do still minimize some kind of energy function some kind of potential function it just happens to not be exactly the right one we care about the average traveled travel time of the traffic the good news is for these a fine cost functions those two potential functions dont differ by very much so because equilibria exactly optimize something thats an almost correct function they almost optimize the function which we really care about the average delay so thats sort of a proof by metaphor about why some kind of guarantee for equilibria should be true for these selfish routing networks so just to close the loop i said the computer scientists were coming from a communication network standpoint and to reason about communication networks you really want to go beyond a find cost functions and we know that this theorem as stated is false for a fund cost functions its no longer 43 but there is one aspect of the theorem that remains true for any types of cost functions which is the structure of the worst case example so one way to interpret this theorem is look i showed you pickers example i showed you the price of anarchy can be as big as 4 over 3 this theorem says it cant be any bigger than what you already see in this trivial to no tunick network and that statement is actually true whatever the cost functions no matter what the cost functions you care about go hunting far and wide for the worst network youre ever going to find the worst network is always one of these simple piglike networks to node to link networks ok so thats true even if theyre not a fine cost functions now knowing that you can actually compute this price of anarchy this magic number for any class of cost functions that you care about it reduces it just to a backoftheenvelope calculation so you know if you open up page 1 of a typical data networks textbook the first order bit of a cost function you usually use something like a delay function of them mm1 queue so heres you think of you as being some capacity the maximum amount of traffic that a link can support and this is a cost function which looks like this so theres an asymptote at the capacity the delay is very low when youre far from the capacity and it shoots up very suddenly once you get close to the capacity now people who actually manage telecommunication networks one of the standard heuristics they use to keep performance sensible as they look at link utilization and park is this is a pretty easy thing to measure so you have some cable you have some maximum amount of traffic that can be going through at any one time and you basically you know stick your finger in it and you see how much is going across right now and networks are often managed to keep link utilizations bounded away from a hundred percent because thats a good heuristic or its been empirically observed that that works well for keeping network performance good so the theory i just described can turn that into it into a theorem into a rigorous statement so to quantify link utilization let me use this parameter beta so this is the fraction of a link thats unused so beta equal point one would mean that all the links of your network have link utilization at most ninety percent okay so using the theory i just mentioned you can compute exactly what the worstcase price of anarchy is as a function of this beta as this function of your maximum link utilization and you know you know were all supposed to be in kind of quantitative sciences here and ive got you for an hour so i feel like i owe you at least one kind of obscure formula so heres your obscure formula but this is exact this is really the right answer so if all i tell you about a network is that the link utilization is at most 1 minus beta so if its beta overprovision and i tell you nothing else otherwise the network can be arbitrarily large and complicated then in fact the worstcase ratio between selfish and optimal behavior is exactly this so what is this okay well lets let one go to zero so beta go to zero so that says the link utilization is going to 100 this thing is shooting off to infinity as beta goes to zero but that shouldnt surprise us because right at the asymptotes we see that these cost functions look a lot like super high degree polynomials and we already knew that you could have undoubted efficiency loss for super high degree polynomials on the other extreme as beta goes to 1 that means your networks basically empty so theres basically no problem with selfish routing you get a price of anarchy of 1 the reason its nice having such a crisp formula is you can plug in quite modest values of beta and see what you get so if you plug in point 1 so again this is just keeping the link utilization down to 90 theres still mostly used its 90 or less already this ratio drops to something close to 2 and again this is a worstcase guarantee over all networks subject to this link utilization guarantee so what did we learn by applying the lens of theta or its basically the toolbox of theoretical computer science to these routing networks so the ma its an ageold economic model okay so piggy was discussing it almost a hundred years ago in his book it was made rigorous by war drop and beckon mcguire and winston in the 50s and theres been literally hundreds if not thousands of papers written on this model largely in transportation but before computer scientists started seeing about this problem no one was thinking about approximation indeed the first experience i can remember describing this result to an economist was here at the institute i was visiting in 2002 professor victor shin was kind enough to introduce me to eric maskin who since then is one of nobel prize in economics and eric very patiently listened to my description and he said its a very natural result i never would have thought to prove it and ive heard that over and over again from economists as i told them about this work and so you know i have ive sort of wondered you know why is that not true especially when i was a graduate student and i was terrified of you know finding the main result of my thesis you know in some 1972 obscure econ journal so i had to tell myself a story i needed a narrative why conceivably this might be a novel result and of course i still might be proved wrong in the future but the best ive come up with is that you know really even though this application had nothing to do with computation okay i wasnt didnt want an algorithm nothing was npcomplete still you know as a theoretical peter scientist ive lived my whole life in the long shadow cast by np completeness and ive absorbed the toolbox that weve all developed for compromising when you cant get what you want and so when youre just analyzing equilibria in games in the wild theyre not going to be fully efficient which is what you would really want but turning to approximation we can nevertheless salvage i think some very positive and kind of really new insights into economic models that are almost 100 years old i think thats what the seus lens gets you in this particular case for the second application i want to talk about something actually quite practical so this will be probably maybe one of the more you know immediately realworld things that we you know really engineering projects that we talk about today and so what i want to highlight here is how a novel and largescale auction which is being designed as we speak i want to point out all of the different ways in which the highlevel aspects of that design have been influenced by work in computer science so the story here starts in 2012 when congress cost a bill passed the bill which is sometimes called this spectrum act okay and the point of this bill was to authorize the fcc to run design and execute a novel type of auction and the goal of the option is to sell wireless spectrum probably mostly to telecoms for wireless broadband use now that by itself is not novel so whether whether you may or may not have been aware of this but the fcc for over 20 years has been running these socalled combinatorial auctions to sell wireless spectrum to in effect the highest bidder whoever could make the best use of that technology heres whats different this time around whats different this time around is the spectrum which is most valuable for wireless broadband is pretty much all already accounted for okay the government just doesnt really have any free spectrum lying around to give away and so on the other hand a lot of the people who own that spectrum are arguably not using it in particularly interesting or valuable ways in fact a ton of it is owned by just television broadcasters who frankly if they went poof overnight not many people would notice so were talking about broadcasters on the uhf channels 38 to 51 okay and regional broadcasters so the proposal here which is very interesting is to repurpose that spectrum for more valuable use so this is actually going to be a double auction which means that the fcc is responsible for simultaneously buying back licenses from television broadcasters and turning around and selling it to wireless telecoms okay for broadband use so what theyve been running for 20 years are socalled forward options so a forward auction is like when youre selling your house okay you accept bids and then you figure out who the winner is and what theyre going to pay like the highest bidder a reverse auction is like when youre hiring a contractor can you accept bids and then you you know you pick one of them okay theyre gonna provide a service to you like you picked the lowest bit of any of the contractors so the fcc is has a lot of experience with forward auctions selling wireless spectrum this will be the first time ever that theyve run a reverse auction theyve actually bought by spectrum back okay the numbers the estimated numbers are fairly eyepopping so the the most recent congressional budget office estimates that i saw was that they would be spending fifteen billion dollars buying these licenses back from the broadcasters and are hoping to make 40 billion selling them to telecoms probably some of you are wondering about you know its pretty big spread and as part of the bill passing thats already been earmarked really wont be surprised to hear so after covering auction costs and funding a new first responder network which is really totally cheap the bill claims to be hoping to have twenty billion of that go into budget deficit reduction okay and so i said one name was the spectrum axe the other name of the real name of the bill is the middleclass tax relief and job creation act because i mean are you really gonna vote against a bill called the middleclass tax room okay so the numbers are big and to thicken the plot further if you design these actions auctions badly terrible things can happen ok debacles really have occurred so one famous one was in new zealand this is quite a while ago 1990 and they were just selling licenses to broadcast on tv again it was a very simple setup they had 10 licenses to broadcast and they were basically interchangeable okay so you just needed to sell all ten of them and for reasons that i still do not understand they decided on the option format of simultaneous sealed bid second price auctions so a second price or vickrey auction in some cases is a really good idea okay here it wasnt a really good idea but so heres what a second price sealed bid auction is its almost equivalent to ebay so you write down a bit in an envelope you pass all of those to the seller the seller opens them up the winner is the obvious one the highest bidder the price may not be the first one you think about the price is actually the second highest bid okay so the highest bid by one of your competitors all right but if you think about it like you know in a art auction or on ebay the winner doesnt really pay their bid generally they pay the lowest bid necessary to beat out everybody else which is basically the second highest bid overall okay so this is a very sensible auction in some contests however now i imagine that those tennety is going on at once and youre somebody who just you just want a license okay you dont care which of the ten you get okay you have no use for two okay use one one and theyre saying oh yeah submit anywhere up to ten bids in these ten sealed bid auctions so what should you do well you could like pick your favorite number like seven and go all in for license number seven you could say well you know maybe not that many people are bidding for these right maybe theres only like fifteen or twenty people actually bidding and then maybe i should bid super low on a bunch of them hoping i get one at a bargain its another totally legitimate strategy and one rule of thumb is is if you have an auction where its not clear how to play theres definitely vulnerable to bad results in particular the new zealand government was really hoping to make a quarter billion dollars in this auction that was the projection they wanted making 36 million and to add insult to injury i guess for reasons of transparency they were required to actually publish the winning bids in the prices at which things were sold and remember this is a secondprice auction okay so the winner pays not what they said they would be willing to pay but rather the next highest bid one of these licenses the high bid was for a half a million so 500 thousand the second highest bid was for six not six thousand but six okay so someone got a broadcast license for six bucks now right and then if you scale that up to sort of modernday in the us were also talking about two orders of magnitude more money okay so the lesson being its that is people know they need to think very very carefully about these auction designs so let me tell you so this is double auction its currently slated to be run in early 2016 though all of the details have not been announced okay so some of this is speculation based on my conversations with people and meetings ive been to but let me tell you what seems has all has you know coagulated as the high order bits of the of the auction format and so im only gonna im gonna focus mostly on the reverse auction cuz thats just the part where youre buying licenses from tv broadcasters because thats the part which really has to be invented from scratch so it looks like theyre gonna go with a proposal by two economist colleagues of mine at stanford paul milgram in elia segal illya segal incidentally i also met for the first time at the institute that same visit in 2002 and so what they proposed is a descending clock auction okay and ill give you some more details later but first let me just kind of tell you at a high level how this works so theres a bunch of broadcasters participating in this auction and remember what were trying to figure out were trying to figure out which broadcasters were going to buy out and if we buy them outs at what prices okay so youre probably used to ascending auctions okay but here because were buying out people rather than buying goods we start high and we go low all right so initially so at every every round every broadcaster has offered some buyout price which they can accept in which case i can say yep im gonna take the money and run you can have the license or they can decline all right now in any given round if a buyer accepts the price that just means they proceed to the next round that doesnt necessarily mean theyre gonna be bought out at that price we may ask them again next round if theyre still willing to be bought out at a lower price however if a broadcaster ever refuses and says no im not willing to pay 400 im not willing to be paid 400 million dollars for my license then theyre kicked out of the auction forever okay and they get to keep their license and they got no compensation okay so the the bidder is still in the auction are those who still might get bought out by the fcc so what you do is you initially start with a huge buyout price for everybody probably were talking something like a half a billion dollars so that everybody in the auction contractually is obligated to accept that initial opening buyout price but then you know the fcc says well you know for the amount of spectrum we actually want to sell thats overkill we dont need to buy everybody out so in the next round were gonna lower some of these buy out prices were gonna try to get were gonna try to buy a bunch of licenses at a at a smaller cost and then that just continues so the auction keeps lowering prices and eventually some of these broadcasters refuse and say no im not willing to be bought out at that lower price so what i need to tell you more about is when the option stops okay so again you start high and you decrease the prices of everybody as the auction proceeds so intuitively what this auction is aspiring to do is its expiring its aspiring to buy a target amount of wireless spectrum as cheaply as possible so what would be an example of a target amount of spectrum so think of it in terms of tv channels and this this really is how they think about it so were talking about people broadcasting between channels 38 and 51 thats 14 channels lets say wed like to free up ten of them okay so were gonna retain four channels worth of tv stations but 10 of those channels will be freed up for wireless broadband okay now theres a really interesting issue which is that these television stations are scattered across 14 channels and theyre not just going to conveniently fall out of a particular 10 piece of channels ok these buyouts will happen across all 14 so a crucial part of this auction design is that after you buy out some of the tv stations you look at whos left whos retaining their license and then youre actually going to repack them youre gonna give television stations who still have their license a new channel ok so your favorite tv station might go from channel 37 to channel 40 thats totally possible in this option so you take whos left and you repack them in a small amount of channels let me show you how this works with a picture so suppose initially things look like this okay so here each circle represents a broadcaster the circle indicates the radius on which they broadcast and the color indicates a channel ok so theres three channels in this picture youll notice whenever two circles overlap they have different colors thats what they dont interfere with each other okay thats a constraint and anything about it if you have all of these tv stations theres no way to use fewer than three channels because theres three different stations here who mutually overlap okay so three three channels are really necessary to have all these broadcasters without interference on the other hand you could buy out one of the tv broadcasters they disappear and at the moment that didnt help because were still using three channels but now if we do a channel reassignment we can have them all not interfere with each other using only two channels okay so we just bought someone out and weve freed up one channels worth of spectrum after a repacking okay so thats how this auctions going to work its going to buy out a ton of people free up a bunch of space and repack the people who are left into a small number of channels and then the freedom channels will be sold for wireless spectrum okay so thats the gist of the auction let me tell you a little bit about how work in computer science has influenced this design and implementation so first of all the high level design these descending clock auctions these are actually a direct extension of some previous work by computer scientists specifically my first ever phd student mukunda rajan also with ronique mehta who at that point was at ibm and so the really nice property of these auctions is they have strong incentive properties so if youre one of the bidders in these auctions you have very strong incentives to behave honestly okay meaning to drop out of the auction exactly when the price reaches your value for that license okay so youre not gonna drop out early and youre not going to stay in too long ironically again even though these these types of mechanisms so these strong incentive constraints are inherited by the more general proposal of milgram and segal but the reason we propose these mechanisms again its because we actually wanted computationally efficient mechanisms we were aware of these incentives all of their incentive properties but we wanted also computational efficiency because of the np completeness intractability barrier and we wanted to harness the big toolbox of approximate algorithms that have come out of theoretical computer science turns out the definition we came up with is more broadly useful for this double auction design second connection and this involves a colleague of mine at stanford csu ive shown so i mentioned thats in each round of the auction you can decrease the buyout price of various broadcasters and know where the flexibility of the auction is that you can decrease different broadcasters by different amounts so some you can decrease really aggressively and others you can leave very high now why might you want to treat two different people asymmetrically well remember at the end of the day whoever you dont buy out has to get repacked okay and because some television stations interfere with others much more than some youre really scared about having a very difficult to repack television station dropout okay that makes your repacking problem really hard so if someones really hard to repack he might be much less aggressive about decreasing their buyout price whereas if someone doesnt interfere with anybody then you can decrease their price as far as you want you dont really care if they drop out and youre not gonna be able to buy their license from them okay so theres a question of how do you do that how do you choose how aggressively to decrease these buyout prices for different television for broadcasting stations and again the details have not been announced but what has been being proposed and the preliminary plans are very directly inspired by you know really aspects of approximation so heuristic algorithms for nphard problems which you off show them was dealing with in a forward auction context so exactly the same issues of trading off you know how much money versus how many conflicts does a does a bidder creates thats exactly the what these heuristics are meant to address and again the motivation here was because of the np completeness of a certain optimization problem alright so are the computer science toolbox has more than just approximation as far as ways of coping with np completeness sometimes you really want to solve a problem exactly and youre really willing to devote tremendous human monetary and computational resources for doing it and we have an example of such an npcomplete problem in this very auction format i showed you the repacking problem okay with all of these colored circles overlapping with each other thats an instantiation of a totally canonical and pcomplete problem known as graph coloring and its a difficult and b complete problem theres really no easy way to solve it efficiently on all instances so other parts of the toolbox are being opened to tackle this part of the auction design and this is led by kevin leighton brown as computer scientist at ubc and an expert in this kind of stuff and so he showed how these repacking problems can be formulated very nicely as instances of satisfiability or sats he showed how you know this a huge amount of satsang technology in computer science the offtheshelf ones still arent quite good enough to get the performance they need in practice so they really want to solve sad instances with like tens of thousands of variables hundreds of thousands of constraints and maybe a second or something like that so offtheshelf techniques arent quite good enough but if you encode additional domain knowledge about the interference constraints in this particular auction setting then you can usually get the desired performance so just a few seconds to solve really reasonable size set problems so thats a third ingredient from computer science which is really kind of a necessary condition for this auction format to be practically viable okay how much time to f ten minutes or 15 minutes or something okay all right so let me just what do you say five is better five is better okay five is better ill skip this part ill just mention this briefly at the end okay all right so that was the second vignette and so the goal of this thing yet was to show you how computer science ideas have been influencing not only the theory of economics but also the practice of economics so again the highlevel auction design even though its motivated by incentive issues the precursor actually emerged from work inspired by computational efficiency considerations you know design and greedy heuristics about how to decrease prices thats breadandbutter kind of techniques that come out of approximation algorithms that come out of theoretical computer science and then fast relatively fast exact sat solvers was also crucial for the by villa the viability of that format one thing i skipped and ill just mention very briefly is that in the forward part of the auction theres a lot of folklore understanding and economics about which auction designs work well which options design dont work well so for example can you do can you get away with simple auctions or do you need much more complex auctions with additional features such as package bidding and so theres a lot of empirical rules of thumb but really theoretic computer science gives the vocabulary to turn those empirical rules of thumb into theorems so for example one rule of thumb is that when items have complementarities so this means you have a bunch of items where you dont have a lot of value for them individually but you do have a lot of value for them in concert so think about say a telecom who has a business plan which only makes sense if it gets coverage all across the us and really as business plan falls to pieces if it has just spotty coverage all over the nation so now if you have a budget independent auctions its really hard to pull off bidding in them because you need to simultaneously win in all of them if you win in only half of them its a total disaster so thats why auctions can be hard when items can be confident have complementarities its hard to coordinate the outcomes of multiple independent auctions so the rule of thumb in the field then is that you really need complicated auctions if you want to be able to have good outcome when you have these complementarities among items but again it was not previously a theorem it can be made a theorem and it builds on a field of theoretical peter science known as communication complexity so its historically thought of as a you know a pretty hard core part of theoretical pure science but it turns out its exactly the right tool for making precise these well known rules of thumb among practitioners in these kinds of commercial auctions okay so let me go back to a sort of more theoretical even almost sort of philosophical example to conclude so let me return to nashs theorem which i mentioned early on in the talk i mentioned how in some sense this theorem gave game theorists and economists who are using game theory everything that they wanted in the form of a sweeping universal existence theorem and indeed i mean this this theorem is so general it honestly it spoils you if you work in game theory because it just sets the bar so high for the applicability of any other result so i dont even know anything about your game as long as i know its finite i know theres a theres an equilibrium so thats great but for many of the usual interpretations of a nash equilibrium from any of the reasons why you might care about nash equilibria its not enough to just know that it exists ultimately somebody maybe the players themselves or maybe some helper a designer or a mediator is gonna be responsible for figuring out what one of these equilibria are okay the players have to know what to play either by discovering it themselves or by being told by some third party okay but some boundedly rational agent has to figure out what this equilibrium is so what wed really like is wed like a sort of more constructive version of nashs theorem which indicates not just existence but how might one figure out how to play so nashs original proof actually had two proofs but they were both based on fixed point theorems and so those have a non constructive nature that dont really give any clues about about how to have a constructive version so this has been a this was a major open question for a long time and evidence started amassing that maybe there could not be universal efficiently efficient computation a fish on the computable version of nashs theorem the first instance i know of of someone really shining a spotlight on this question and also sort of raising alarm bells was michael rabin in a paper in 1957 so a computer scientist and the audience will already know many reasons why michael ravens sort of a hero of computer science for his work in atomic theory randomized algorithms cryptography and so on i guess its probably not that many of you know that in 1957 he wrote a paper on the intractability of computing nash equilibria but he did so he says its quite obvious that not all games can you know captured by the theory encompassed in theory can actually be realistically played by human beings remember 57 is well before we had np completeness 15 years before right so what could he have meant by sort of an intractable result well we did have undecidability in the sense of girdle and turing and thats what raven meant so he exhibited a game okay were actually the there was account it was accountably infinite action spaces where it was easy to prove the existence of a nash equilibrium but you could also prove that there was no decidable procedure that would give you that equilibrium okay so this showed that there could be big gaps between the types of existence theorems that could be shown and the types of constructive existence theorems that could be shown then later we did have np completeness and we could prove intractability results for finite games like the ones that are squarely in the domain of nashs theorem so gilboa and zemel proved that various decision problems around nash equilibria or npcomplete and there have a sequence of results showing that at least particular algorithms for example algorithms that attempt to quickly learn nash equilibria are doomed to fail do not in general work in a computationally efficient way so this sounds like its right in the wheelhouse now of computational complexity theory you have a computational problem namely given a game find a nash equilibrium we know one exists and were starting to believe that may be an efficient algorithm doesnt exist and again this is what many people in the community basically do for a living prove these kinds of intractability results and the first thing one thinks about oh maybe this is another one of those np complete problems okay like i mentioned earlier but the answer is actually a little bit trickier than that so megiddo pointed out thats actually nash equilibria are not npcomplete at least understand the complexity assumptions and its because theres almost a sort of type checking error between nash equilibria and typical npcomplete problems so remember what a typical npcomplete problem looks like i want to know in the facebook graph are there a hundred people who are all friends with each other this is called a clique is there a clique of a hundred people in the facebook graph the answer might be yes the answer might be no and i want an algorithm an algorithm to tell me which is which now if i give you a game and i tell you to find a nash equilibria its not a question of whether or not there is a nash equilibrium to be found we know the answer nash told us the answer the answer is yes the only difficulty only difficulty is in finding it okay so because of this lack of a binary decision problem underlying nash equilibrium computation it is in a provable sense not as difficult as npcomplete problems because theres guaranteed existence of a solution so papadimitriou then proposed to have a refined version of np to say okay so maybe we cant prove that computing nash equilibria is as hard as all this other stuff but maybe we can at least prove that its as hard as all the other problems that have guaranteed existence because of a fixed point argument okay like browers fixed points and several other problems okay so thats a refinement of the complexity class np papademetriou decided to call it pp ad so allegedly allegedly this stands for polynomial parity argument directed version christos a strata convinced me that he was unaware that this was a substring of the first five letters of his last name when he proposed it but if you believe that ive got a bridge to sell you so the 94 proposal was to say look if computing nash equilibria is going to be intractable and we want to prove its complete for some complexity class heres a proposal for what that class should be okay so that was just a proposal there was not a proof of intractability it was just really a program for in tractability and it took more than ten years but it was indeed proven okay mid last decade that indeed computing a nash equilibrium is as hard as any other problem with guaranteed existence coming from a fixed point theorem okay so formally this is a ppat complete problem okay so its like npcomplete but with this technical distinction because of the guaranteed existence of solutions this is a difficult result the proofs are real tour de force duskull optus goldberg papadimitriou chen deng and tang as far as how to interpret this i mean this is the way we would in theoretical pure science make precise what up till now had been sort of more of a folklore belief that there cannot be a constructive version of nashs theorem that has as sweeping universality as the existence okay so nash proved nash equilibria always exist assuming p is different than p p ad that cannot be the case that there is universal efficient computation of nash equilibria okay thats the meaning of this result okay so in particular if you want a constructive version of nashs theorem just like with npcomplete problems you have to make compromises and again in theoretical your science we have many ideas for how you can compromise you have approximation for equilibrium computation more realistically you probably want to narrow the domain okay so you probably want to look at games that are either not too big or have extra special structure where its easier to understand and compute what the nash equilibria might be so what did we get from the computational lens applying it squarely to the fundamental concept of nash equilibria well you know what is what is not a new idea is the idea that the nash equilibria has problems okay theres a long list of wellknown criticisms of the nash equilibrium concept for example already the fact that its not unique you can have lots and lots of nash equilibria in different games that already makes its predictive power not as sweeping as one might like okay so there are many criticisms but the computational lens has highlighted really an orthogonal and i think novel type of criticism of the nash equilibrium concept again it doesnt mean it should be removed from the pedestal of being the fundamental equilibrium concept it just means its just sort of advice about when it makes sense and when it doesnt make sense and the critique is that even with all of the setting all the other criticisms aside that predictive power is diminished by the fact that we cant have confidence that down at the rational agents or designer can actually figure out what one of these nash equilibria is at least not in the general case ok so again in specific domains you get a positive results but you cannot have a general purpose constructive version of nashs theorem ok so just to wrap up quickly so ive given you a highly non exhaustive but i hope representative set of the interactions that have been taking place on the boundary of computer science and economics over the past 15 years and i really dont think this is some fad i mean the interaction thats happening is only accelerating as we speak and i think it makes sense these are two communities that have a lot to benefit from each other and indeed you know arguably even need each other to a certain degree computer science right many of the 21st century applications that were struggling with really require economic reasoning to make good sense of them i tried to highlight this with a network routing application but if you look at the big data companies you look at microsoft and google they now have positions called chief economist okay up toward the top grass of the company so this is really i think a well understood new thing computer science needs economic advice in the other direction theoretical computer science is really uniquely situated to articulate computational intractability when it exists and whether we like it or not it exists pretty broadly in the world whether youre trying to run some auction at large scale like in the fcc auction or whether youre just trying to understand whether or not equilibria are generally going to be realized in games like we saw in the previous application with ppat completeness but the good news is is because of our long history coping with intractability we have a lot of ways of getting around it a lot of ways to still make progress and get insights into fundamental models even when theres intractability there so ill leave you with that thanks 