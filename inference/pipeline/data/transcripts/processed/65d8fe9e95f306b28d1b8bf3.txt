surface of cells that activate when photons hit them whether those photons are bouncing off of a physical object or being produced by your display there isn’t too much of a difference to the neural circuitry that makes up your vision system so the brain is already doing the hard part it takes a 2 dimensional image as input and on the other end you somehow have a near instant understanding of the 3 dimensional objects you are seeing we can create objects by constructing them out of many triangles to create an accurate 2d representation we need to project each of these triangles onto a flat plane from the perspective of a viewer it can be helpful to think of a computer screen as a window where light bounces off an object through the window and into your eye so to generate an image it’s almost as if we need to do the reverse operation if you shot a ray from your eye through each section of the window what color would you hit and this is the basic idea behind 3d graphics each pixel of the display is like a little window and the more pixels the clearer the image and it is up to our graphics engine to figure out what color of light would be passing through that pixel as if the game world actually existed on the other side the method i’ve explained where rays are projected into the world and collide with objects is the foundation behind ray tracing ray tracing has only recently made its debut into video games with the release of newer graphics cards containing dedicated ray tracing hardware on the other hand cpubased ray tracing has been around for a long time and has been used extensively within the movie and animation industries but is much too slow to be used in realtime applications a single frame in toy story 4 could take anywhere between 60 to 160 hours to render using cpu based ray tracing ray tracing is known as imageorder rendering since each pixel is first considered and then on a per pixel basis we find all the objects that could influence it the alternative is objectorder rendering which is much faster and what we have been using this is where we draw each object one after the other this approach maps much better to the gpu since we can apply the same instructions but on different vertices in parallel so we could find the location for each projected position within a vertex shader by calculating the line and plane intersection and this would work just fine but a much more effective way to do this is with a matrix transformation first though i’ll start by explaining orthographic projection in the previous tutorial i talked about vulkan’s canonical viewing volume only the objects that are within this 2 by 2 by 1 region will be displayed note that the positive y axis points down and positive z axis points into the screen an orthographic projection is a generalization of the view volume that keeps the view direction and orientation fixed but allows the view volume to be an axisaligned box with any dimensions and at any location we want an orthographic projection is a generalization of the view volume that allows us to specify whatever dimensions and whatever location we want but maintains the overall shape of the view volume and keeps the view direction and orientation fixed the orthographic view volume is defined by 6 bounding planes the left and right along the x axis the top and bottom along the y axis and the near and far along the z axis so these 6 values left bottom near and right top far define the orthographic viewing volume so to construct an orthographic projection matrix we need to solve the following problem how do you transform the orthographic view volume to vulkan’s canonical view volume and the good news is we already know how to do this we can combine two basic transformations first translate the box so that the center of the near plane is at the origin and then follow that with a scale transformation so the boxes have the same dimensions so the numerator will be the canonical view volume’s dimensions and the denominator will be the dimensions of the orthographic view volume if we multiply the matrices and simplify we get the orthographic projection matrix note that this matrix differs slightly from most other online resources the canonical opengl view volume is a cube with a z range of 1 to 1 additionally opengl uses a left handed coordinate system and looks down the negative z axis but we’re using a right handed coordinate system and looking down the positive z axis so just keep this in mind if you are following other resources with different conventions and that’s it by applying this transformation to our objects the objects that are within the region occupied by the orthographic view volume will be scaled and moved into the region covered by canonical view volume and therefore the objects will be displayed but the orthographic view volume doesn’t apply perspective to do that we require a viewing volume that is not box shaped but a shape known as a square frustum a frustum is the shape that captures a viewer’s line of sight as if they were looking through a rectangular window so what we want is a perspective matrix that transforms the frustum and any object it contains into the orthographic view volume doing so would apply perspective to all objects within this space the further away an object is the smaller it will appear to properly apply perspective we need to project each point onto the viewing plane for example to calculate the apparent height of an object on the screen we can use the property that the side lengths of two similar triangles are proportional to each other we can rearrange this equation to show that an objects apparent height on screen is equal to distance to the screen over the distance to the object multiplied by the objects true height this equally applies for the x direction when calculating an objects apparent width on the screen also note that just as with the other viewing volumes anything outside of the frustum will be clipped ok so what we would like to do is come up with some 4 by 4 matrix that when applied to a position vector the resulting x and y values equal the projected xs and ys coordinates on the viewing plane but there is a problem we have a division by the z component in the solution’s x and y components and a 4 by 4 matrix that can move a z value from here to a division here simply does not exist it is just not possible with matrix multiplication alone but what’s that it is homogeneous coordinates to the rescue we have one more trick up our sleeve so far the 4th component of all position vectors has been fixed with w always equal to 1 to allow for matrix multiplication to implement translations we will now additionally define w to be the denominator of the x y and z components this means that the homogeneous vector x y z w corresponds to the point at xw yw zw this also means that vector 1 2 3 1 is equal to the vector 10 20 30 10 and also equal to 2 4 6 2 these all represent the exact same position at 1 2 3 this division is automatically applied on the glposition variable output from the vertex shader and has always been occurring if i go into the vertex shader and update the homogeneous coordinate value to 2 and run my code my object becomes half the size because we now divide each position component by 2 therefore if the solution’s w component is equal to z it will be possible to calculate the projected coordinates of x and y onto the viewing plane with this knowledge we can start to construct the perspective matrix transformation the first two rows have the distance to the viewing plane on the main diagonal and zeroes elsewhere this will have the effect of scaling the x and y components by the distance to the near plane next the final row must be 0 0 1 0 to take the z component from the input vector and move it to the w component of the solution vector all that’s remaining is the third row and an easy mistake to make is to think this can also just be 0 0 1 0 to copy the z depth value into the solution the problem is every component will be divided by w including this one z over z is just 1 and now we’ve lost our depth information and won’t know which objects should be drawn in front of others so rewind and in reality we need the solutions z component to equal z squared since z squared over z is equal to z but in our matrix we only have two remaining unknown values to use if we multiply this out we get the equation m1 times z plus m1 equals z squared this is a quadratic equation meaning it has at most two real solutions and can’t be true for other z values therefore we add two constraints that this equation is only true when z equals n or z equals f this means that the transformation will not change the z values for points on the near and far planes but all other z values will be warped nonlinearly this results in two equations that we can use to solve for m1 and m2 m1 times n  m2 equals n squared and m1 times f plus m2 equals f squared solving these equations we get m1 is equal to f  n and m2 is equal to negative fn let’s plug this in and the solution’s z component becomes f  n times z  fn so lets take a minute to go over what this really means we wanted to have a one to one linear relationship between the input z value and output z value but the best we could do is make this true for points at the near depth and far depth the z depth values on the near and far planes remain unchanged but the relationship is no longer linear however this is still useful because the relative ordering of the z values between the near and far planes is preserved this means we can still use z to determine the order of objects within our scene an advantage to having the z depth values be nonlinear is that calculations at the near plane will be higher precision then calculations at the far plane reducing observable z fighting z fighting also known as stitching or depth fighting occurs when two surfaces are very close together this visual artifact is caused by the rounding errors of floating point calculations and can be avoided by reducing the distance between the near and far planes now back to the perspective matrix if you apply the perspective transform followed by the orthographic projection transform the combined result is known as the perspective projection transformation and this is what we’ve been after this matrix will transform objects contained by the viewing frustum into the canonical view volume and in doing so will make the objects appear in perspective we define the perspective projection matrix with the same values as used by the orthographic view volume the 6 values r l b t n and f but it’s often more convenient to use a simpler system where we assume the frustum is aligned along the z axis meaning that we are looking through the center of the window this implies that l  r and t  b which can further simplify the matrix finally it is common to specify a vertical field of view rather than the values for the bottom and the right planes the vertical field of view is the angle from the bottom to the top of the near plane relative to the viewers line of sight if we also know the aspect ratio of the window we can easily calculate values for the bottom right corner with some basic trigonometry so the bottom value is equal to the near plane distance times tan theta over two and the right value is equal to the windows aspect ratio times the bottom plug this in and simplify and we get the final form of the perspective projection matrix if you enjoy computer graphics content like this please consider subscribing thanks for watching and keep on coding cheers 