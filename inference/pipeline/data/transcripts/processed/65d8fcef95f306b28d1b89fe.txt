parallel programming were going to discuss the different types of uh programming paradigms and some different libraries and and application programming interfaces that are common uh and when were talking about parallel programming so just to take care of some terminology first um well well talk about a node you can think of this as a standalone computer its similar to like a desktop computer or even a laptop this would be comprised of multiple cpus cores processors possibly even gpus or many integrated core accelerators things that you would normally find in a high performance say desktop computer even though they dont have that form factor of a desktop computer theyre typically blades that are located in a rack and network together to find to form what we call a supercomputer right so a supercomputer is just a collection of nodes and each of these nodes is sort of a standalone desktop computer again they dont they dont look like a desktop computer but they have all the essential features that a desktop computer would have the next thing is you know these are all sort of synonym synonyms cpu central processing units socket processor core i mean technically theres a difference between a cpu and a socket as you can or a socket and a core as you could have multiple cores per socket but for the purpose of this class its fine just to use the same um you know or use them as synonyms so this is the singular sort of execution point in a computer and finally then we have tasks and a task would be a single instance of computational work so essentially what were going to do is often we have multiple tasks that we can do at the same time and were going to then run them on multiple processors or multiple cores simultaneously and those chords can exist on a single node or they can exist across a network of nodes on a supercomputer so theres different types of parallel architectures the the sort of most common music is a distributed memory you know most common form of a supercomputer would be a distributed memory form factor this is where again like i said you have a collection of nodes each one of these would represent a node where you have a cpu and memory and when i say when were talking about memory here were talking about ram random access memory not not hard disk space the hard disk space would be shared amongst the whole network but essentially youd have a whole network of individual nodes that each have cpus and some memory and then youd send the tasks off to each of those and communicate over the network when needed to share information between the different computers the the other form factor is whats called a shared memory computer these were pretty rare or very expensive until until very recently with the advent of general purpose graphics processing units and intel mini integrated core chips which are both a type of shared memory accelerator you would call them but you can have a more traditional vectorized cpu type computer that also has shared memory again memory being ram and in this case youd have multiple cpus that all have access to the same ram and you can use an advantage of that or you can you can take advantage of this to do different computational tasks on each cpu without having to communicate across a network when needed to share information between the tasks more often than not this is the type of architectures that were seeing nowadays in modern supercomputers that is basically you have a collection of nodes where each node then has multiple cpus spread out across a network or even music of the form factor where you have a network of nodes and each node has possibly multiple cpus and multiple say gpus graphics processing units which can be used to do numerical computations or even you know its not listed here but you could you could count many integrated core chips uh in a similar way to graphics processor units they both plug into the same sort of pci bus on on a uh on a computers heart uh motherboard so these are common programming models you would call these application programming interfaces or particular libraries and so for shared memory computing you have several posix threads which are a posix unix standard openmp openacc which is a very new thing that will probably one day encompass openmp as well if youre working on an intel machine you can use uh thread building blocks so these are application programming interfaces which would be a standard or a standard set of functions that you would call in a certain way or possibly instructions for the compiler that you would use in a certain way to say unroll for loops or do other types of things music and and a lot of them have have interfaces in multiple languages so fortran c c plus plus these types of things additionally since we talked about gpus gp gpus general purpose graphs processing processing units these are gpus that are intended to do numerical computations on and not just for for graphics processing there you have cuda which is this is a standard by nvidia its an application programming interface uh that only works on nvidia machines and they have their own compilers and such opencl is introduced by apple and was later open sourced but its a way to do gp gpu calculations on any gpu so its supported by nvidia but you could also use it on you know intel graphics processing units for example and again here you see open acc which is supported by nvidia and pgi compilers and others and this is a way to sort of standardize development between gpus and cpus its an effort to do that so that you could write the same code and have it exploit multiple threads on a cpu or the graphics processing units as well some some really new techniques if you havent integrated an intel mini integrated core chip you could use a technique called offloading where essentially youre going to move all the data from the cpu onto the minigraded core chip and have the work done there of course theres some commun communication costs associated with that and then even newer theres this idea of myo the myo is an acronym that stands for mine yours ours and uh this idea is that you can essentially write the code in one way and then the computation will be shared between the cpu or multiple cpus that share ram along with the integrated the the mini integrated core chip thats available on that node as well for distributed memory theres really only only one way uh mpi message passage and message passing interface this is the de facto standard for distributed memory computing and one nice thing about it is once you once you learn this you can actually use it to do computations on shared memory machines as well you may not get quite the performance boost that you would by using some combination or you know using the shared memory but the nice thing is it does work itll work on distributed memory machines and a shared memory machine and most of us now even our laptops for example have multiple cores that we could exploit so if you use mpi you could you know write code that could run well on a supercomputer but also take advantage of the multiple cores in your in your laptop without really any extra work and so you know again it stands for message passing interface this is an application programming interface this is not a programming language there are implementations of this uh in fortran c and c plus plus and then of course weve already learned how you can build wrappers so in this class well actually use a python wrapper to the c plus plus application programming interface of mpi and thats called mpi for pi and therell be a whole uh additional lecture on on that and on mpi itself and again as i mentioned earlier most modern machines are hybrids that is the you know they are most modern supercomputers are some hybrid and in that they have both distributed memory nodes uh as well as on node you have either some gpu or mini integrated core acceleration or at least multiprocessors and so theres an opportunity to do hybrid parallelization where you use mpi to essentially send data between the nodes and then on nodes use some type of posix threads or intel intel tbb or something like that to speed up the computation even further so uh theres several different ways to kind of design parallel programs the only one im going to discuss here is really with re because its its the most common uh in engineering applications and that is that were going to split the problem data set up so well have some problem data set this could be some large mesh associated with a finite element computation or some large data set associated with that you want to perform machine learning out on and so then were going to split that data up into individual tasks where the work will be performed individually and then often of course these tasks cannot be performed in isolation when they can thats good and its called an embarrassingly parallel program but often they cant be performed in isolation and so then theres communication amongst the tasks and thats what well use mpi for so finally id like to just end with the resources uh you know all the figures in this talk came from from this website but its also just a good resource and fairly uptodate with respect to uh parallel computing so if you have access to the live slides you can you can click on this hyperlink and take a look at that for more information 