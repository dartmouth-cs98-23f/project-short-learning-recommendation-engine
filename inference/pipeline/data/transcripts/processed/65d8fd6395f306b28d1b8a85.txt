the carnegie mellon vaccination database talks are made possible by autotune learn how to automatically optimize your mysql and postgres configurations at autotunecom and by the stephen moye foundation for keeping it real find out how best to keep it real at stevenmoyefoundationorg hi guys uh welcome to the last vaccination seminar talk for the semester were super excited to finish off with the professor eugene wu uh he is an associate sorry so serious assistant professor without tenure uh at columbia university um he did a phd uh under um sam madman and mit um so hes gonna talk about his his research on human data interaction and building systems around that space so as always if you have a question for eugene as hes giving the talk uh please dont meet yourself say who you are and where youre coming from and feel free to interrupt him at any time we want him you know want this to be a conversation and not him talking by himself and eugene hopefully you will introduce the uh the museum ps behind you before you talk okay sure yeah so thanks andy for uh inviting me to close out the vaccination season uh and this museum piece um you know is a a a prized um part of my art collection its um a tshirt signed by mike stonebreaker whos uh you know one of the the pioneers and illustrious researchers and uh in our field so with that um let me get started i also noticed by the way that andy added to my bio that i wear pants and i wanted to clarify that it depends on the temperature and situation uh whether or not its needed i also just want to clarify that you know in contrast to the rest of the series im not going to talk about a particular database system but instead share the vision that you know my students and collaborators have been working on uh in this area of system for human data interaction um so as we all know programming is powerful because users dont need a program right they could use human computer interfaces that we all use today but the premise of course of this entire seminar is that the center of gravity is shifting from compute to data to the point where data rolls everything around us and so theres correspondingly shift towards human data interfaces right theyre a center around visualizing and interactively analyzing and manipulating data and were seeing this kind of like trend across nearly every discipline and every task right across monitoring finance marketing science and industry as well as data tasks such as data wrangling data modeling data exploration analysis and so on and in fact if you think about databases themselves the reason why i have this seminar you know in this field at all is because they represented a leap in human data interaction from imperative programming to declarative specification and now the demand for better interfaces is not just from programmers but really from everyone and you can think of human data interfaces as both the front end interface right that renders data as pixels on the screen and let users point and manipulate this and underlying systems that transform and process the data in response and yet despite the active research in both of these areas right its still very difficult if you think about it to build and design data interfaces today and i wanted to start with two of the major reasons why which are a combination of scale and what i call design dependence so if you think about scale if your data set is small and you can just process it in the in the browser and the client then the problem is arguably solved and theres many many great solutions today but as the data size grows your architecture necessarily becomes more complex for instance you might need to shift processing out of the browser to your laptop and if the data gets any larger then you might have to use a remote server and database or even the cloud right and all of this introduces latency at every system boundary as well as network bottlenecks cloud api calls and so on and the fact that youre now kind of like implementing and running a distributed system but in addition to that right the interface itself hasnt really changed the user still expects the same amount of responsiveness because you know they dont really see any of this and in order to kind of like provide that then we often end up adding caches the high latency ever in the system and if the caches become quite smart right because you might want to query the data in a cache then they start looking like databases and its not to say that at every level of this kind of architecture there arent great libraries right theres really good libraries and systems to solve each of these individual problems but optimizing and combining them together is still nontrivial in fact if you squint this is kind of akin to the memory hierarchy and we kind of know programming efficiently against such a hierarchy is one of the fundamental challenges of computer science however in this particular case were asking designers to actually manually program and optimize against this hierarchy and combine disparate software systems and part of this signals a lack of great abstractions for specifying and describing the interfaces right and enabling this kind of optimization that we know and love now you might say well theres a lot of fast like and responsive visualizations out there so clearly if you know exactly what you want to build you can hire a crack team of developers and designers and just build and optimize it but for most cases actually interface design is an iterative process and the problem is that design choices affect the system architecture as well so lets consider a very very simple example well look at different designs of the left interface that that is just you know analyzing legislative votes from chambers of congress and then on the right well have a trivial architecture and just look at how what kinds of data structures you might precompute so if you want to add two buttons to provide interactivity to choose the decade then we its pretty simple right we can precompute both decades and then when the user clicks on one of them then we can send the precomputed results back to the user so thats very simple but what if we change the design slightly to a range slider well this would then require precomputing a quadratic number of views and so you would need a different data structure maybe a data cube or something else you might even consider another optimization to push that to the browser and then cache and process it in in place right but then maybe maybe the designer wants to try adding a radio button or something else and that requires yet another type of optimization right and this is just for a very simple example but the bottom line is that any minor change in the interface design directly affects the architecture and the types of optimizations that are needed and so we see here is that the trend in most application development today on web and on mobile is veering towards no code levels of simplicity when data is not really an iss issue yet the combination of scale and design dependence makes this increasingly harder for data intensive interfaces so what our lab has been focused focusing on over the past you know several years are three classes of problems under the umbrella of the data visualization management system at the foundations level were thinking about identifying system primitives that can more easily express and build data interfaces and enable optimization on top of that were also developing systems algorithms to more easily design interfaces themselves and then at the very top weve been thinking about if we can design interfaces easily and we can implement them easily then what other capabilities beyond displaying data and clicking on widgets could we think about right and so theres many projects at each of these different layers but what i wanted to do today is focus on one slice through this architecture two of these projects are going to showcase a decadesold concept called data lineage helps us innovate at the systems and interaction levels and then the third project shows how we might expand the range of interfaces that are designed and created today so ill start with smoke which reframes interaction as lineage operations this is work uh initially led by photosellidus and its now continued by hanyin and charlie so what is fine grain lineage given a workflow such as you know for example here i have two tables i want to join them by id or color and i want to aggregate them by id or color right so fine grain lineage corresponds to the record level dependencies between operator inputs and outputs concretely if we look at output 1 the orange record then it depends on the join results j1 and j2 and j1 course finally then depends on a1 and b1 in the base tables and this forms a graph that can tell you well which inputs contributed to any output records and vice versa and its very useful in terms of debugging privacy fixing distributed uh protocols incremental view main and some more right so its been around for several decades and so its very useful however actually materializing and using this lineage graph today is very very expensive on even recent papers and systems on analytical queries the existing overheads can be between 10 to over a thousand x slowdown of the query and it kind of makes sense because a very very optimized read query has now transformed into a massive write operation right and in addition lineage is often inefficiently represented uh for instance you know ill ill describe this next and so to give you a sense there here are two common approaches today the first is what we call the logical approach and we stay within the relational model and the goal is to just rewrite the base query so that output is annotated with this lineage information for example here in the output it would be the gray attributes right a and b now the nice thing is this is compatible with any database system but as you can see the output 101 has now been replicated for each combination of input records it depends on from the base tables and the query now needs to do a lot more work to actually compute these annotations because of this denormalized representation theres a kind of some inherent overheads in this approach now alternatively a physical approach which is popular in big data systems like spark is to modify each operator implementation to write pointers out to some external linear subsystem like a graph or a key value store um but it turns out that you know even just making these virtual function calls can be very expensive in any fast database system we experimentally see that just making empty virtual function calls can slow things down by over 2x in addition to the subsystem also replicates a lot of functionality that database provides and so in both cases you know like its its quite expensive now the reason we care about this and im harping on performance is because lineage is actually a natural way to express interactions and and define interfaces if we think about everything that you see on the screen thats data and you point to any pixel that represents data what youre really referring to is its lineage for instance if i select points in the scatter plot what im really selecting is the data that backs it or its lineage and similarly if this base data has been rendered in a different way for example like in a bar chart then we typically will see that the bar chart changes when we select data in another chart right this corresponds directly to either forward lineage or view refresh and so ultimately all of this is declarative right whats happening here uh is the user interaction is data that were joining with whats rendered in the scatter plot and were then getting its lineage and then refreshing the other views in other words if you think about interactive visualizations the initial charts are the output of sql and the interactions is then lineage and more sql beyond just visualization right we can data is manipulated and rendered in many other forms yifon wus uh with 2020 work explored how ref how you might want to for example reference lineage and code for instance you might want to query against the lineage that you selected and pointed to in some other visualization or chart or you might want to fit a model against it right so this is all programming at this point and it makes sense because what lineage really represents is the correspondence between your input data and what you see on the screen so ultimately lineage is great however existing systems are super super slow right its just unrealistic to use them to drive interactive applications and yet we know that fast visualization implementations do exist and so what smoke is trying to do is bridge this gap so what smoke is its a rowbased inmemory query engine during query execution right as you execute each operator it materializes the lineage information for each operator the idea is really to overlap lineage capture and operator execution by reusing data structures built such as hash tables and use it for lineage capture and what we do is were going to instrument physically these operators and use query compilation to remove that overhead uh if you know that you only care about endtoend lineage information we can actually then propagate this information as we generate the lineage so that you end up with just uh kind of like end to end information and avoid storing this per operator all right so this is an overview of smoke and what i wanted to do is give you two examples of how lineage capture works in the system and then you can extrapolate for something like filter what were going to do is populate the lineage information which is represented as this red integer array so for example lets take a look at the first row its going to pass the filter because 40 is greater than 20 in addition were going to store the input rows id in the corresponding offset in the in the integer array right the second row fails the filter but then the third row passes and so were going to store its corresponding record id as well so thats pretty much all we have to do lineage in this case is just a simple integer rate its cheap to create and populate now for something more complicated like group by it works in two phases during the build phase were going to augment each bucket for example here when i look at row one im going to create the bucket and then in addition uh augment it to also store a list of record ids right so then when i go to record two and i update that bucket im going to store its information and similarly for record 3 when i create the new bucket im going to also allocate this information and store its record id now in the scan phase all we do is just emit the output table as normal and for the lineage we simply create a top level array which we know the exact size of and it just points to the rid lists that ive already constructed right so all of this ends up being super cheap too and none of this is really additional work and additionally scanning the data to generate lineage information and piggybacks as much as it can off of query execution so rather than showing you like you know like performance graphs i just wanted to show this in action that it can actually power interactive applications so this is a cross filter visualization over 145 million rows of flight data and its going to be rendered as four charts each one is just a query result right now when you click on a state or a bar we filter the input table for its corresponding subset and then reaggregate and rerender each of these other charts and the benchmark will build whatever data structures it needs and then load these initial charts and then simulate every uh every one of the 18 uh you know 1800 possible interactions now the one popular approach thats dominate thats dominant today is to build visualization specific cube structures to accelerate these interactions however recent papers end up taking between four minutes to one hour to build a data cube for this data set right so its really not realistic for in adhoc cases and these data cubes themselves limit the types of queries that you can express so were going to show in this benchmark is instead of waiting for four minutes we design a custom cube it takes about nine seconds to build uh just for this benchmark in addition here in the top right uh this is going to show the cumulative time taken for each interaction including the time to load these initial charts all right so now the benchmark has started and what were doing is waiting for the data cube to be built and once it is built we can see that every interaction is super fast right and so thats why in the cumulative chart it starts fairly high along the yaxis and then its basically horizontal now to show you smoke in action what were going to do is when we generate these initial charts we just instrument it to capture lineage information right so as if we created all these charts in an ad hoc manner now when you click on a state we can use this backward lineage information to help us find the input subset and we can use the forward lineage information to incrementally update all these other charts so you can think of lineage now as an optimization that comes for free its helping us do selective view refresh so now the benchmark has started and you can see the dashboard basically loads very very quickly because lineage capture is actually very very fast right and it turns out that we end up finishing this particular benchmark faster than it took to build the cube and so what that suggests right is that lineage uh has the possibility of being fully interactive and practical for developing these interactive interfaces in addition we can capture and use this information on the fly so this is work that we did several years ago and the big question that everyone often asks is like that you know this is kind of a toy engine and no one in real life is going to instrument their their engines to actually do this right and who uses a road based query compiled engine for analytics blah blah blah so what about real engines and can you do it with minimal surgery so the idea that weve taken here is were looking now at polymer engines that are actually used for analytics and if you think about how something like filter works uh the idea is the following right so if you have something like quantity and you have a filter then the output ends up being a selection vector right where each bit is set depending on whether or not the row passed the filter if we stare at what this is as compared to the lineage that i showed you earlier then it turns out that theyre basically equivalent a selection vector is a dense representation of the same information that lineage was encoding as well and so what weve basically found at least in some late materialization engines is that query execution itself is not just easy to piggyback off of it is literally computing lineage in many cases and so what were doing actively right now and so this is unpublished work and were kind of like still working on it is were in instrumenting systems like duckdb which is an embedded column or vectorized engine to see how far we can get using this kind of idea so i wanted to show you is just like very early numbers of lineage capture on tpch with scale factor one for four of the queries these blue bars are just the smoke numbers copied from the original paper and so it all of these are relative right in terms of overhead to get the compiled queries essentially you can think of it as handwritten tight loops now smokeduck is duckdb with lineage capture the numbers now are relative to duckdb right so its not apples to apples but we end up seeing is that the overhead varies between 18 to 48 percent overhead in addition it turns out that the lineage instrumentation took about four to 20 lines of code per operator and because a lot of the code is just pinning existing arrays in memory and overall in terms of memory overhead its about 20 to 40 megabytes on tpch scale one right so what all of this means is that lineage is a critical piece for expressing interactive applications and interfaces but its got to be fast right and what weve shown is that lineage is interactive and is practical and so what were doing right now is studying the extent that these ideas can uh can apply to other late materialization engines and how to cleanly integrate lineage management into these engines as well and it turns and what we seem to see is that many aero based uh analytics engines like data fusion and aquaro and javascript are also amenable to some of these ideas so were very excited about this um and so this is kind of like how something from database theory can be applied to something in hci so now i want to shift to a usage of lineage and this is work led by lampreus focus young woo with collaborators janon and knuckle so this this animation is from the kai 17 paper same stats different graphs all of them have the same summary statistics on the right but obviously theyre very different right and visualization is exactly powerful because it shows you what a priori statistics cannot now our work seeks to empower users to identify unexpected patterns in the interface and be able to ask why these exist right and then to be presented with sensible explanations and this hopefully can help them debug data errors or simply better understand whats going on so i want to just share two examples via demos of our work the first demo is this idea of query explanations that we introduced in 2012 through the scorpion paper and so here like you can imagine a sensor deployment collecting a bunch of data um per minute right across like an entire deployment uh and we might want to group by hour and then compute the average sanitation temperatures and then plot on a visualization now the user might immediately then ask well like all of this looks kind of like periodic and it might look pretty good but why is this region look so high right whats going on there and wed like some explanation as to whats going on so this is kind of what i wanted to first demo if you can see my uh screen so this is the exact same data so the blue corresponds to the average temperature on an hourly basis and you can see theres kind of some daily periodic pattern right so what wed like to be able to do is literally just like in the slides ask what is going on here and specify them as anomalies and then say hey you know i thought they would look closer to these other points that i just selected and then i can run the system called scorpion and what were going to do is look for potential explanations as to why this might be the case um and so what well see here is uh uh you know i think i screwed up some demo no i didnt screw up a demo but i need to show you more information uh so you know what screw it i dont care um so here uh if we look at mode 18 and you revisualize just the data from mo 18 we can see that like the data is just totally crazy right and it turns out that if you actually rerun this which ill actually do right now um because this is a little bit better um is if you do exactly the same kind of like uh uh thing sorry im just gonna do this again because i cant show you all the coolness uh if im missing some of the interviews good keep going you got it all right so were gonna like reexecute this right and were going to look at different rules and see their effects if we remove the data so what that means is earlier we saw that sensor 18 is a potential kind of like explanation and if i hover over this now what im going to do is remove all of its readings and then revisualize the visualization and so we can see here is like the anomalies we specified disappeared and so this can be a useful heuristic uh for an analyst and try to understand whats going on right and so what the semantics are here is that weve generated this predicate rule that says centers with low voltage or you know sensor 18 and so on and what we mean is if we ignore all that data that matches the predicate and and reexecute the query then it would help address the users question right their complaint and theres been really great followup work by peter bayless by sudbury roy alexandra and many other areas of people in this particular area but what i wanted to focus show you another demo is a different example of explanation in the context of machine learning analytics in our recent rain work and the setup is very similar right however what we know is that machine learning is not only used for end user recommendations but its also often used in one step of a data analytics workflow for example here maybe what i want to do is first filter to just get the readings for faulty sensors right and i have some model that predicts whether or not a sensor is faulty and then i want to group them by hour and just count it so maybe this is for a dashboard or a monitoring system now here the user might see that theres been a drop in hours three and four and want to ask questions why now existing explanation approaches will help us find issues in the readings in the readings data set but even if that that data is fully correct we can still have these errors due to errors in the training data right because that can cause the model to mispredict in a way that ultimately causes what the user is asking about and theres not many good solutions for this problem normal data cleaning looks for errors in the training data but ignores kind of how the model or the downstream analysis is used and influence analysis techniques for machine learning relies on labeling individual model mispredictions but that doesnt really work if if prediction is just one step in this larger analytics process and so what rain is is the first work to perform training data debugging based on user questions of the analysis outputs and specifically what were doing is were estimating how much deleting different training records will affect what the user has asked about the users complaint so i just want to show you a quick demo of this in action so what i have here is just a very popular data set mnist and im going to group by its predicted label and its just going to count so this is kind of the visualization and we can see that for the most part we expect a uniform distribution in this training in this data set right but we flip some of the labels and so we like to be able to do you can imagine the user asks well why is seven so high so im gonna just specify that i think its higher than what i expected and id like to see what training data kind of might be the reason and so here ive returned the top 20 training records where if you remove it from the training set it would help it would most decrease this particular output value and you can see that all of these kind of highlighted in red correspond to one digits that have been mislabeled as seven right you can similarly click on the bar for one and say why is it so low and we would then identify is that oh actually you know like removing all of these missed labels is you know will help you with this particular question right so you can see that depending on the kind of questions you ask youll get kind of like better or worse results because they may be better more or less aligned with the type of error that actually affects what the user sees in fact you know an interesting thing is you can ask nonsen seemingly nonsensical questions for example you might ask you know what kind of data can i delete from my training set so that four actually is increased right the count of four predictions and what youll end up getting is a bunch of uh digits that look pretty much you know like closer to fours and uh than uh than other digits and if you delete them then that would help the model predict more force right and so its kind of nonsensical and so you can see how visualization and interfaces are an important component to trying to understand and interpret explanations all right uh this itself of course is not like a full or complete solution to this problem but hopefully it demonstrates the value of not just the algorithmic side but also the interface side now the reason why i talk about this at all is because these explanations that were talking about right end up being a function of lineage information because what we need is to know for what the user is asking questions about what data was it derived from and also how was it computed right and so that is precisely lineage and what is commonly called data provenance and so thats why smoke is useful right its not just for building and specifying interfaces but also for enabling these kinds of capabilities efficiently all right so im done kind of talking about the use of lineage and i wanted to switch gears and talk about this third project called precision interfaces thats led by guru chan with early versions led by tebow owen and haochi so we focus so far on building interfaces and expanding their capabilities but interface design itself is ultimately dictated by the analysis task right analyses typically start off very ad hoc you might be programming or like writing ad hoc queries and then we ultimately want to design an interface once we settle on the set of queries that would be actually useful for an analysis and yet if we think about how interface creation tools work they actually work in the reverse fashion right they limit themselves to a particular class of queries or analyses and then make creating interfaces for those easy to build so here if we think about the tools for creating interfaces at the xaxis here corresponds to the expressiveness of the analysis task and the yaxis is how easy it is to create interfaces using them now tools like metabase and tableau and excel and so on make it very easy to build interfaces that are essentially parameterized queries or data cube operations right so you can say that for these very very common classes of analyses theyre fairly limited but theyre very easy to use but if you want anything more complex towards the right then you basically enter the world of writing lots of code or paying someone and although theres engineering kind of like programmatic libraries to help with this its still just too difficult or expensive for the vast majority of potential interfaces right to be creative so oftentimes they just simply will not and so the the kind of like the longterm goal that we have for this project is to basically place present interfaces at the top right for any analysis test how can we make it you know nearly trivial to actually create the corresponding interfaces and the philosophy that we take is the following and this is you know just one approach if you think about what an interface is theres really two components theres a visualization that renders the output of some queries or programs and then theres interactions that let users change those queries right and when you change them then the interface updates now if you imagine listing all the queries that result from all possible combinations of interactions with the interface you could call this the expressiveness of the interface in other words the interface itself is a compact representation of the set of queries that it can produce and we we would ideally hope is that that set includes what we need for an analysis task all right so what our work is trying to do is given a sample of the analysis task that you would like we want to derive the latent interface right and the core challenge here is kind of uh you know actually took us a long time to figure out something reasonable which is how do you map query strings from the input to interactive fully interactive interfaces right we have a series of papers that look at different approaches and so what ill give you is just kind of a demo a walkthrough of the four main steps and what were currently thinking is a reasonable approach uh first were going to model queries as their parse trees so here im going to focus on just the highlighted parts of query one and query two right because thats the part thats different and we can see that these two parse trees are rooted with equals and have two children now the corresponding interface is going to be simple right because ultimately we have to execute these queries and render them so the simplest interface would be just render the output of these queries and then just visualize them but we want interactive interfaces sometimes and so we introduced a new class of nodes called choice nodes to encode subtree variations for instance here we can add a choice node to choose between these two subtrees right and so what that means is you can choose one of its k children and parameterizing it will result in a standard parse tree that you can execute so you can think of this as generalizing sergible kind of like queries that we that we know and love today the nice thing about this is choice nodes can be directly mapped to interactions for instance here clicking on the second button would correspond to choosing the second child of this choice note right and of course the buttons are not the only types of interactions you could map this to you radio buttons can also choose one of many and so we can rank the different candidates by borrowing from existing interface cost models that can account for things like usability complexity screen size and so on so thats the third step is costing and then the fourth step is to recognize that the middle tree here is the result of merging the initial pairs of trees right by adding this choice node so it went from two trees to one these types of tree transformations allow us to generate structurally different interfaces as well like we saw here now we could also imagine refactoring the highlighted equalities out right so that we have a tree rooted at equality but then now you can actually make independent choices for the left and right operands and so this ends up generalizing beyond the input two queries that we had this also maps to a different interface right because now we have two interactions instead of one and in practice of course you know we have many different types of transformation rules and types of choices choice nodes that we can express beyond this trivial example but hopefully this gives you kind of a a sense so to put these steps together you can imagine initializing the system by parsing the initial query input queries into a sequence of trees and then mapping both the tree execution results to visualizations the choice nodes to interactions and then the tree structure to a particular layout you can cost the output interface and then we can either return it or then transform the trees further and so this is the problem statement as well given a query log we want to search the space of possible trees and mappings for the lowest cost interface that expresses all of them and potentially more and you may just said this but what is what is the cost function here like what the cost function uh you know we borrow from existing ui literature you can you can think of things like personal preference usability how easy is it to express the analysis you know like the workload and so on uh how you know like whether or not like theres too many options in a radio button uh list and so on right um so you know theres different ways that you can cost it and what we kind of like use is both per interaction like cost as well as you know whats the expected effort that the user needs to take to express the input queries in sequence so like this is stuff you can compute offline you dont need to put humans in front of us and test it uh yeah so you could put humans in front of uh different can interfaces offline and then train some model and then use that right okay correct yeah and you know implementation wise we use monte carlo research its basically randomly searching the space and its a little bit better than that because it uses some reinforcement learning kind of ideas okay but you know the cool thing is a demo right so lets take a look at a demo uh here hopefully you can see the queries um so ill zoom in and here i just have a simple cars data set and maybe here initially i want to just uh look at you know different miles per gallon and uh and uh for a different horsepower and then maybe now the analysis kind of like weve changed this we want to see a different range for the horsepower right so this is fairly simple uh and what were going to do is kind of like run the interface a little bit and we can see that it kind of like immediately generates like an interactive interface that you can use to basically filter uh and avoid writing queries but then what if i actually have kind of a more complicated query structure for example now i want to actually change the subquery here im going to use a very simple example for um uh for demo purposes but lets say i just care about uh cars from the usa because you know were like super uh ill stop there um and so here you know what we can see is that weve actually generated a weird interface right it turns out that you know like for this particular three queries you know its actually might be best to just allow you to specify each of these queries individually um and maybe thats totally nonsensical right but if i now say i actually care about kind of like europe then uh then present interfaces in the background will crunch for a little bit and itll actually generate a different type of interface so you still have the slider as normal right this range slider but you can now select either just the full cars or you can manipulate the sub query structure itself right so what this basically does is when you click on europe it switches to the subquery that it corresponds to and then manipulates this this text string here and then now maybe you want to do something else right maybe you want to also look at horsepower and look at the average miles per gallon from this data set grouped by horsepower and if you run something like this then precision interfaces will chug for you know a couple of seconds and what itll end up generating is a multiview visualization where it actually visualizes both the horsepower versus miles per gallon here right and it allows you to interact with it directly and so you can use this visualization to then interact and see kind of like whats going on uh in the initial query right and you can also of course use these other kind of controls as well and so by just writing a few queries or you can imagine mining some existing query log we can identify and fully generate a fully interactive multiview visualization that you can use in lieu of programming right if it turns out to work well so thats a a demo of kind of like the types of functionality to show that it is possible to uh to do this right and the nice thing here is that you can have arbitrarily complicated queries as well as transformations of its substructures because what were doing is were just encoding all of this as transformations in these trees so just to kind of wrap things up ive talked about these three projects in this data visualization management system and hopefully you know ive convinced you that its pretty interesting i want to connect it now with the rest of our projects to see how all kinds of pieces together the interfaces that we generate in this last part are in this internal declarative representation its very similar to the trees and choice nodes that we saw earlier right because what we want to capture in interfaces is not really the front end rendering but the data flows right that back it because thats the hard part in a lot of these in modern interfaces physical visualization design is another project that i didnt talk about and it consumes its representation and its goal is to recommend a system architecture to optimize the interactions its very similar to physical database design if youre familiar with it but the data structures are created and stored outside the database and we viewed contacting a database as a slow path the reason is because theres lots of work on new custom systems and data structures and papers out there today that are often external to the database system itself right and it also tries to account for the client and communication now so what this allows us do is to design an interface right either manually or using this you know present interfaces point it at a snowflake or some remote database and just fill in the rest of the architecture and and of course this can now execute and leverage a lot of the kind of system parameters that weve been developing such as smoke for a lineage and interaction or communication management in chameleon as well as kind of query optimization in addition because were generating these interfaces we can imagine embedding these novel interactions and functionalities into the interfaces that we generate optimize and deploy and so in other words we think of precision interfaces and the reason i talk about it as a way of kind of like bootstrapping the use of a lot of these other projects that we have and that were working on and so just if theres one thing you take away from this talk is that data management and visualization and hci you know are very interesting standalone right but really the inner intersection of it theres lots and lots of really interesting and rich research problems that can benefit both communities uh weve tried to study some of them in this dvms like data visualization management system at the levels of functionality interface design and system primitives so at this point this is my uh url and a a bunch of pictures of ninjas all right thanks everyone okay awesome i i will clap on behalf of everyone else for eugene um so we have time for questions if you have any questions for jadine please meet yourself and fire away okay so my question is about the um the precision interface stuff uh that seems super cool like but youre taking whatever the sql queries they throw at you and trying to turn into interface like and i understand theres a cost function and maybe i dont fully understand where they can handle this but like what happens if users like writing stupid queries theyre like its not necessarily stupid queries but like theyre trying to figure out what the data even looks like to begin with without asking questions like how do you prevent them from polluting the interface and also because i can imagine also too what you really want to do is let a knowledgeable person write the sql queries and then the novice users dont get direct sql access they instead get the interview thats generated so how do you prevent its not an adversarial thing as if theyre trying to hurt you or hurt the interfaces they dont know what theyre doing definitely definitely you know like i i think of precision interfaces as like the the underlying mechanism right it just takes a sequence of queries and it generates stuff um now theres you can imagine many ways of using this so one that i demonstrated here is a knowledgeable person writes perfect queries that are well crafted for a demo um but another could be you know you do this ad hoc analysis at the you know command line or using other kind of like complex tools and then once youve decided like this is the analysis i want to share with other people or that i think ill do repeatedly then you want to basically say like those queries log those and generate something specialized right so thats another way that you can imagine using it um but you for for instance maybe like youre writing these queries and then you just mark the ones that you think are useful right or you could explicitly use it as like an interface generation tool by uh by directly uh writing queries that you know will be useful right so that is if youre using it specifically as a design tool um theres many kind of like limitations and opportunities too for example like we generate like one interface but theres actually like how much do you want to generalize beyond the queries that the user gave you is a parameter that you want to be able to control right and maybe you personally have like preferences on the layout or on the types of visualizations and so on and it turns out that like youd like to be able to maybe specify that and then use those as kind of hard constraints lets say um so i i think of this as an optimization problem and then what you can use it for so we we were kind of really like at the point where we think this is like a good formulation of the problem um and were were hoping to write it up like later this year yeah okay all right um and then sort of another question b is like you know sql theres you know you can write the same query in some in semantically the same queries in a bunch of ways in sql like does it how well it can handle that does it always generate the same interface or does it like if i use nested queries versus ctes but again the high level answer is the same thing like i asked my students for the first homework assignment they had to write queries they asked you know questions written in english and they come back with all sorts of crazy different ways to write it would your thing still generate the same interface or i think we probably generate something pretty um it its like as you can see right where we actually uh just look at the syntax and some database statistics and schema information so we know nothing about the internals right were not looking at the query plan or the logical plan or anything like that if you still look at the career plan what could you do so i think if we looked at the query plan it would allow us to do more canonicalization right so i think that would end up uh end up generating like equivalent interfaces if they have equivalent like you know the closer you are to the semantics the closer you are to kind of like canonicalizing um i i would say i imagine that you know like for like with any tool you can use it in like unintended ways and intended ways right um and and so i i would imagine that you probably dont want to use this as just like throw a bunch of random queries and generate something um you probably would end up using it um to either like mine for for example like you can i could imagine having like some interface complexity measure or some kind of like entropy measure where you can say oh this interface is actually reasonable with some criteria and then kind of like just uh scanning through like query logs and looking for like you know sequence sub sequences uh that kind of are selfconsistent right so thats like one crazy thing that you can imagine doing but ultimately like people will use tools based on where it shines and probably not um uh you know not for the corner cases necessarily so um it probably isnt good for like uh the setting you described at least now i wonder if you take like tableau and you you know they you love some public dates that they have and then set up a basic visualization uh and then but capture the queries and then run the queries you capture yeah weve done that in the past yeah okay and it generates one of our goals is to be able to synthesize like the tableau interface right because like the primary like degrees of freedom is the the group i clause and the where clause yeah right and the project clause so um but you know like so its really just degrees of freedom uh another way you can think about it right another like a baseline um would be if you if like how would you write in interfaces today you would literally write and and construct query strings yeah right and so at minimum here it kind of like generalizes sargeable things so for example we can generate only syntactically correct statements right and we could you can imagine doing some checks to make sure that you only execute queries that are not faulty right so those are things that you could do with this sort of functionality so the baseline i imagine is like the alternative to creating an interface which is writing a bunch of like query templates and then like filling in like query strings yeah yeah okay um i had a question about smoke too much pedals yeah so i mean smoke i guess if you i mean if youre running duck db it sort of answers my question but theres no theres no sort of level complexity of the query where you like when you would lose like how many like layers of nesting you know about embedded queries uh sub queries ctes where like you would lose when the information like all with feel accessible its all at the physical level okay so you dont lose anything yeah okay okay cool good good okay uh any quite any other questions from the audience because eugene is a oneyearold and im a oneyearold old im gonna take care of them although he stands office so he doesnt have to oh i think the nanny leaves in ten minutes so okay you gotta keep going okay any last question for eugene you 