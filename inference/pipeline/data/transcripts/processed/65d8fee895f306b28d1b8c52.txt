data science with game theory ill call it data science with game three flavor and ive been involved with research related this signature with computer science and game tv for many years by now and this particular initiative started already six seven years ago when i returned from my previous period in industry long time with industry where basically uh my feeling was that classical data science algorithms i call it data science because i refer to the real practical things that are used in ir applied ml and some things in llp and recommendation systems actually need to be revisited so i talked about a very bottomup approach really taking a very very basic algorithms and search accommodation prediction and more and the claim is that need to be revolutionized and i try to illustrate it in a very brief talking with one or two examples and i just like to say that in general if i had to have a much longer talk i need to talk about two perspectives at least one of them is what we call the agent perspective but we talk about predictions fragmentation persuasion but in competitive environments and the other one is what i like to call the mediator perspective mediators i think that the work for many years by now but when you talk about media it does the mediator is google is facebook its amazon or whatever where many publishers whatever comments there are have their own incentives and you need to mediate it while were one way or another and in both of them the claim is this particular notion of incentives needs to be really integrated in a very basic algorithms and ill be happy when we actually replace the current algorithms so a word about the agent story ill try to read very brief and i know how much i managed to do but ill try so the agent story is the following one here is a nice lady her name is gal and girls need to choose among two uh two applications that are very popular these days of guessing uh guessing your age and one application guesses that her age is 24 and one said that her age is 34 and as you know god is a very confident woman and therefore probably shell choose 34 because her age is 36 so its you know this one wins and from this point on she actually will use this application thats thats the reality uh so this refers to indulgence that ive been involved with people in theoretical computer science already 10 years ago that we call dueling algorithms we dont need to go over all of this slide especially who says the classical algorithms basically try to minimize some some error average error or some expectation while in stochastic process and in reality many cases you care about the probability of being better than the other so think about will be more specific you have many users user come to you and you know if you if bob with the new algorithm which compete with the classical written alice will be better predictor for you than you probably tell your friends and basically you basically try to compete and be better than alice on most of the instances anyway dont like to take a position here but there is some trade of bitcoin optimism optimizing instances to optimize incomes so thats very theory kind of thing but it really connects very very directly to data science kind of thing because we have a very similar situation in our data science applications we just dont have access to the distribution for example in regression task we may have access only to sample of points uh instances labels and the corresponding opponent predictions and be more specific so ill give an example from the early parts of this work with my student um so thats thats a very early work on that subject that we did so think about a realistic system this is the boston boston apartment kind of database that you need to basically predict the apartment value based on certain parameters here we just assume lets assume that it depends only on the top and sides and alice is the standard algorithm that use some lets say just some linear regression for a minute and dont need to respond to it to apply a best response to it so the key issue here is that when you respond there were some points that youll be better closer to bob and some that you are closer to alice and basically when you respond you like to design you have access to some points that you see what was the reality how alice behaves you can try it and then for new users most of the users didnt show yet youd like to be a better predictor for something that will come in the future so lets formalize it a little bit so the model is is starting its entering to classical park learning model it enters already some notions that come from games but as you see these are very practical ones there are instance domain some subset of our foreign a label domain some y and alice is using a strategy h bar which is some hypothesis in fact unknown to bob and there is a joint distribution over the following triplet the instances the labels and the disparity of alice how much alice was wrong on this kind of things or is wrong with her algorithm this distribution is unknown but bob can access this actually happens in the real applications there is access to a sequence of examples that come from this distribution this is all all that he has so belts payoff if he chooses a hypothesis of strategy age against his age bar is just the probability that its disparity or his disparity or his operative disparity will be lower than alice the spirit so you can define now the classical empirical payoffs given a sequence s of or sample bomb empirical payoff is just the sum of points that is better on it or the average number of points that is better than alice on this sample set and well call a star an empirical best response or ebr if we just this is that what is your strategy that maximize this payoff now we can ask the classical question should boy employ bob employ empirical best response and we have the classical problem maximizing the empirical payoff might result in overfitting and the solutions restrict the statutory space and so on and so forth but what wed like to emphasize in this first part of the talk well get to gathering somewhat later is that already we have a problem here of best response given a strategy set age which may say we say that some small agent h is an absolute best response if for every h prime in age it holds that what you actually get in your or your payoff is absolutely close to what youre obtaining in this h prime so you are epsilon optimal and the objective is given this general hypothesis space level approximates best response efficiently with high probability thats the first step that you need to solve in order to get to to be frank in real application even in communications its also the last step but well talk about the multiagent setting as well and in this particular problem the results were characterization of the back level strategy sets via the notion of visiting mention full analysis for linear predictors and experimental results showing that you actually can defeat a algorithm classical algorithms and realistic setting using this approach when i said best linear response id like to give you an example of things that we did i talked just on linear systems in order to be to illustrate these things but even this setting is not that naive because our lists can be a complex netherlands all that we need is an access to an example of the points of her predictions we dont need to say that its a linear predictor although if we know what is the power of alice we can do something else that ill mention in a minute so so what will be a theorem here so thats one of our theorem given epsilon delta we run the ebr algorithm the builder messaging algorithm on that number of points m points sample points were sample i idea from this d for this distribution on the triplets and after time that is polynomial and m m is the number of points in the sample it outputs its star such that probability at least minus delta we have something which is absolute best response what we mean is that for reasonable systems we can actually have this kind of of result and later we need approximation stuff like that so just to illustrate you what is going on here just its not a technical talk really about but id like to illustrate what is going on here think about alice as a very simple uh regression kind of algorithm were just using a bar x plus b bar to the to to predict y and there are three points so alis line here is appears here on the screen and its a point in the parameter space a b a bar b bar now what happens for each point there are two parallel hyperplanes that in between them you are better predictor than then alice if you if you like to compute the best response and this this goes for any pair of points but what we get here is what is called a hyperplane arrangement with many regions it turns out there are not too many regions and if you can go over these regions and thats the algorithm and select a point from each of these regions and until you select the region when you win the largest amount of points thats the idea the log and this region defines for you some a star b star which is the best response thats the idea in a very simplified setting of this best response algorithm and this can go far when we make to more complicated settings but id like to make a comment one thing that i started with is which i would criticize is what about alice you know here the welldefined algorithms you may be very bad predictable for particular instances so it turns out that this type of algorithm can be strengthened if alis power is similar to whoops to bobs power to our best with sport pump what i mean by that for this particular example of linear systems assume alice uses a linear predictor then the ebr algorithm can be adjusted to output h star such that it will be as before but performs arbitrary close to the opponents strategy in the classical setting in classical mean square arrow or average arrow stating that you use while having the same runtime complexity so we have some positive news here that we tried on in various kind of setting i started with boston which is simple database but statistician love it so ill not go over this this what it means is what its quoted here is that these are the results basically any number that you see here which is above 05 means that this is better than alice alice gain which means this is the proportion of users that youll be better for them and there are various kind of settings that have been studied here and these numbers is the ratio between what happens under the classical optimization criteria or mean square o in everyday error and the factor close to one means that even on this factor you dont lose so you are behaving like similar to the classic algorithm if you wish but you win a much larger proportion of these users and this as many as we know from real applications that we are involved with as a dramatic verification many of the users try only once these two systems or whatever and decide what to do okay one word about game theory in this kind of example and the other example that i give on our agenda will be much more game theoretic uh work on dwelling algorithms that weve involved already 10 years ago has been extended to best response from best response analytically to minimum strategies in the game so its not the case that you need to give better response and the other guy just sits here so if you if you if you now make a better search engine for for for being by being a better ranker so google may respond and similarly work on best response regression has been extended to what we call regression equilibria just to give you the following because this is a very applied kind of of a motivation the fact that its needed think about a cloud provider a big one that basically needs to provide prediction services to several competing parties so you may he he this company this huge company should think about how they respond to one another if you have something which can satisfy several users like that and here comes the idea for questions in bolivia but they will not get into so back to the regular agenda the big agenda here is called data science with miss gate theory and in fact what is written here is the kind of concrete problems in ir applied and l exploration exploitation whatever that we face with and we just gave you an example of what one of the examples that fit into the agent perspective id like to best respond to a classical algorithm and you know and and basically defeat now we like to give an example of the mediator perspective the media story is huge because when you talk about mediators we can talk for many times and for a lot of time and much of this has to do with the power of mediators so we worked on in the past in uh under the gateway head under various kinds of mediators media that can enforce some kind of behavior like in social laws for artificial aging societies mediators that just put the setting like in programs or whatever but and media does they can recommend so the issue here for me is that i see mediators everywhere on the web and actually many of them should be revisited given the gt flavor and ill report on one of them that ive been working more than others because it already showed some practical success and its a its a i i owe uh owe a lot to my colleagues in the information retrieval community that i learned from them a lot so our example will be well take a smaller smaller theoretical example or stylus example but the actual example that we care about are rankers so for example like that lets take the foreign quote from google self its written here there that in a fraction of the second google search algorithm so through hundreds of billions of pages in in the in the search engine to find the most relevant useful results for your you are looking for and in fact thats true wonderful but as you can see if you look at that and if you work on on that in the industry at the end you have your game the publishers your strategies which are the documents they write and their payoff are basically at least partially determined by the ranking for different queries there is a ranking function that is used by a mediator this mediator is conceptually very simple its highly complicated it does something very efficiently but it actually implements what is called the probability ranking principle prp if you look at nlp by the way thats my what im doing in the last year its you have been burning for this and embedding for this and deserving lecture the closer one but at the end its the probability ranking principle the prp the most central concept in professional retrieval states the document should be ranked by the relevance to abilities and care about the highest ranked documents and its not trivial in the paper already six years by now i feel all this it turns out that the prp fails miserably and what is called adversarial layout setting establishers are search engine optimizer search engine optimization is one of the biggest industries that we have because they optimize their utility it can be shown that if you formalize it a bit and if often even if you experiment with it it can be shown that in equilibrium they will tend to write on similar or popular subjects you know it from the tv channels of course ignoring others which will at things that and decrease the total use of social welfare and in some general settings under other ranking functions employing welldefined randomness in the ranking can be proved to be better with regard to social user welfare inequity these are big claims so id like to illustrate you i just said that someone would like to to read about this i i i there are many papers that we have here most of them are which i are guys and in our forms and i i if you like to read something without much ir then i refer to you to cacn paper from 2019 that actually talk about rethinking search engine and recommendation system and game theoretical perspective uh that will distract you the theory practice there here ill again talk about a paper that talk about recommendation systems i fully agreed like i said before you know the computation is a big thing the ecosystem accommodation systems that illustrate you the same kind of phenomena in recommendation systems so think about blocks again realistic situation people need to decide bloggers they need to decide on the type of content to write and they are users and there is something that connect them which is lets call it satisfaction matrix if you write write about a particular content and the user likes it then even he or she will click so think about these numbers player one or publisher one can write about content x one or x two why player two can write only about contact f3 and what will happen here lets assume the player one will write about content x1 and see what happened here okay the classical system the prp the top system will give it you know the user that the content which is more satisfying the social welfare will be you can sum up two and but player one is happy lets assume that you just like to maximize impressions for this stock it has two users notice that if he would uh write on that content the social welfare will be higher but he will be less happy because hell get only one user so the question again is what should you do what what is fair here what is the dynamics here what is the user utility or so should we talk about price of anarchy or whatever so the approach that we have taken because we wanted to come with a concrete suggestion is like first the model the model really states in what we said before you have a set of users a set of players player j selects one item from the set of possible content content types there are profiles as a game theories which is the strategy profiles are the possible content types each users satisfaction function from each content has particular satisfaction and immediately though resume mediator take this topic of qualities and decide which one to show perhaps it can decide not to show anything and the payoff is just the expected number of uh times that you your content is exposed basically thats thats no one will argue almost with that so thats thats the model and what can we do now what will be a good system ive shown you that there is some issue with social welfare versus i dont know the success of publishers so basically we start with design properties one is called fairness which i dont like now this term fairness because therefore this is used for real things important things here no one will argue with our furnace properties furnace properties here the most extreme one here says little mortality the most satisfying items are displayed with higher probabilities than the least satisfying items so thats something that is minimal dont be crazy in your in your properties or by symmetry if this publisher is fully symmetric response satisfying the users theyll be shown with the same probability the other one is a big lesson from the many years in the ad industry is called stability youd like the system will be stable you dont like to fluctuate all of the time and people will send change the content every every man a minute it means that any under any set of players users and items the used game should possess a pure equilibrium in fact youd like more youd like that any dynamics here will converge to some equilibrium that will not get into crazy fluctuations so basically we started with these properties and wed like to find the mediators that satisfy these properties and immediately when we put the this assumption with the call completeness that we show an item is displayed with probability one we get an impossibility theorem that says no mediator can satisfy furnace stability and completeness and in fact this rule out also the prp that you know it cannot work it doesnt satisfy firmness and stability and we should not try a different approach and one of the lessons that id like to give in the very few minutes that i have is this approach that i think it can go far i call it the shuttle immediately uh basically bear with me we focus on the arbitrary user you and strategy profile x which is a topic of contents now given a subset of the players or subset of the content c we can define xc to be a restriction of this profile to the entries in c so user eye satisfaction from the items in the players in c is naturally defined as the maximum satisfaction from this content so thats all very natural but in fact notice that it use a comparative game you have you have the old possible coalitions and you have a number at end which is associated with each uh with each coalition and you can look at the sharply value and basically the shuffling mediator will be the mediator of the set display probabilities according to the sharply value it will call the sharply mediator and this sharp remediator has nice properties it satisfies fairness then it also turns out that the game induced by sharpie mediator is a potential game so this means that any better response dynamic converge so it satisfies furnaces stability so the other context as a computer scientist ive seen sharply used typically there is a problem because in general computation of the shuffling value it shall be hard and wed like to be particular so in this setting computing dispersabilities under the shuttle mediator can be done in linear time so we are good in fact we have nice mechanisms and if you like to be theorists for a minute and you like you believe in efficiency defined as the display probability is the maximum satisfaction level in fact you get that the only mechanism satisfying furnace stability and efficiency is the sharp remediator and to the game theorists among us there are many i hope this is not the result of the sharpie characterization its a different characterization of bishop so so this is more or less what i wanted to say but id like to emphasize more now about few minutes about the general picture so we have many aspects very realistic aspects where game theory and i call it data science not because im not a statistician but i think id like to do one to call it just machine learning you can deploy machine learning ive seen it mostly and they are in nlp and also in things related to explore and exploit one of the things that i if i had few more minutes ill tell you that another type of mediator is like plays or whatever that need to mix exploration exploitation because there is another issue the people that does the exploration is the audience and i guess that many of you when you go to a recommendation going somewhere you may have refused it because i say its probably just an exploration so we need to solve this problem so there are many concrete problems that come into play in search recommendation systems in regression in segmentation and all of them has concrete issues that we really need to bring into practice i think it will be better for the social welfare so ill finish with a practical side and say what if this is implemented somewhere so we can implement and already implemented some best response ai services for prediction segmentation persuasion we didnt talk about translation today uh and another thing which is that the robust search recommendation ecosystem basically it means that its an ecosystem that takes into account uh the publisher incentives the user incentive and everyone and like every all of these to live together uh other thing that we didnt manage to talk and we are involved with this is in welfare maximizing extraordinary exposed services and i think the best is the relics i ill just look here 