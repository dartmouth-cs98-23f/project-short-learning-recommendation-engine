database management systems course   in iit madras online bsc program   in the last week we took a wide look into how to   develop applications based on databases using  various programming languages tools frameworks   we tried to give you an overview  of the rapid development process   what it needs to do applications on  the web as well as on the mobile   as i told earlier we would now again  step back into the core of database design   and if you recollect what we have  primarily done in the earlier weeks   we have started by focusing on the logical design  we talked about two primary design aspects one is   the logical design which is conceptual  which deals with entities relationships   different constraints their representation  the algebra calculus everything   and the other aspect is a physical  design as to how will the bits and   bytes be organized so that you can have an  efficient storage and an efficient access   so it is now time to start discussing that   for that we will have to study some of the storage   options or not some maybe a wide range of  storage options that have evolved over time   and what is available today we have to study how  in this typical storage you can organize the data   how you can make the accesses and storage  efficient by things like indexing and so on   so keeping that target in mind i just thought  that it will be good to give a little bit of recap   to you on what is efficiency what we mean by  efficiency of access what we mean by efficiency   of storage how can we compare different ways of  doing accesses different ways of organizing data   in terms of its access time in  terms of its storage requirement   this truly this is these are  topics in the algorithms course which   you may have already studied so i have kept it  very brief over the first 3 modules of this week   and i will start today by defining just i mean  most of it will be recapitulation for you but   just putting in context from the algorithms  course as to and the data structure course   as to what we will frequently need in the design  of the physical databases so we will start by   looking at algorithms and programs and  then the different issues of analysis and   particularly coming to the asymptotic notation  and some complexity of common algorithms     so this is uh the module outline  the points on the left     so as you know formally an algorithm is  a finite sequence of welldefined steps   the sequence has to be finite  and every step has to be   well defined unambiguous to solve  a class of specific problems   or to perform a computation you want to achieve  a task so you define a sequence of instructions   which are already known to you and those  which are unambiguous and well defined   and by doing those steps one by one maybe in  just a sequential order or maybe repeating them   you will be able to achieve that task now more often the instructions are computer   implementable but we also write algorithms which  are more like for human understanding and you can   find an equivalent representation algorithm  which is computer implementable but the key   point about an algorithm is and this question  is often asked in the interview as to what is the   difference between an algorithm and a program the  key point of an algorithm is it must terminate   an algorithm given an input must give me the  output in finite time that time may be very long   that time may be milliseconds it could  be seconds could be hours months or even   years millenniums but it provably  must terminate that is a key point   in contrast a program is a collection of  instructions very similar to algorithms which   are necessarily to be executed by a computer  that is the reason the computer programmers   write programs in a certain computer language  so we say that programs implement algorithms   and the characteristics of a program which would  often distinguish it from an algorithm is the fact   that a program may or may not terminate now for example you will ask i mean   how is it that a program may not terminate  it is quite possible for example think about   the operating system working in your desktop  or in your mobile phone they never end   if the operating system ends the system is dead  so they as long as you have the computer system   you have an operating system running  your database system does not stop   it will have to keep running so these are  programs which do not terminate but they embody   algorithms which must terminate   that is the reason mostly we talk   of algorithms only and programs come in terms of  realizing what the algorithm is trying to do     now the question is we often know about  analysis of algorithms we often talk about   analysis of algorithms so there are a couple  of w questions on this which is very typical as   to why should you analyze what is the motivation  why do you actually analyze an algorithm then the   question is what do you analyze what identify  what all factors that need to be analyzed   once you have done that then you need  certain techniques tools to analyze so   you try to answer how to analyze then you ask the  question where to analyze what are the scenarios   would you analyze it for all possible inputs or  for some sample inputs what are the scenarios   where do you identify the scenarios to analyze  that is where to analyze and the last question   is would you keep on analyzing an algorithm  all the time or there is a point when you   stop analyzing algorithms and finding newer  algorithms because you may be able to realize   whatever best is even theoretically  possible that is when to analyze   now leaving aside when to analyze because  it involves quite a lot of theory in terms   of lower bounds and you know np completeness  and all that so which is not relevant for our   database discussion you must have studied them  in the algorithm course i would try to take you   quickly through these four points which you i  am sure know in some way or other already     so why you analyze because resources are scarce  i mean because we always want to do more with   less we greed for do more with less we want to  avoid performance bugs so these are the reason   we will try to analyze so the core issues is  to predict performance as to how much time does   binary search tree take or binary search take  if i have two algorithms for the same task like   for sorting an a set of data in an array  you have innumerable algorithm quicksort    mergesort buck bucket sort insertion  sort selection sort heap sort   you you name it so you need to compare the  algorithms and decide which one should we use   then you want to prove provide guarantees that  okay if i do it in this way if i use a redblack   tree then i would always be able to insert a key  insert a record in my database in order n log n   type whatever that means we want to understand  the theoretical basis that is what i was saying   lower bounds and all that so there are these  are the primary reasons why you analyze     now the question is what you analyze you analyze  for resources taken given an input size so your   parameter is the input size which could be the  number of elements in the input which could be   the size of the input which could be the value  of the input which could be multiple inputs   but that is your parameter and what do  you measure what do you want to measure   is the resource which is important for you so the whole history of computing analysis   started with analysis of time it started with  the analytical engine of which i have just   put the picture here is fun so the most  common analysis factor is the time but there   could be several others for example storage  space is also very critical in many places   for example in handheld devices in databases  because databases are very very large    so if you can save some even 10 percent space  you save a lot of money but there could be   other factors to analyze also for example  power i think i mentioned it earlier as well   that handheld devices are power i mean  power constrained so you need to possibly   trade off other resources in favor of power  you may want to optimize for bandwidth because   you may not have enough bandwidth a 4g  bandwidth to connect to your server you   may have a 2g bandwidth so you may want to   minimize on processor resources in a extremely  multithreaded distributed application and so on   so there are various different things to analyze  in this context for the context of database   that we are targeting we will only focus  on the time and space for now      so what you analyze this is just taking an  example this is a very standard algorithm for   written in terms of a c program  for making the sum of n numbers   so this n is your input size so that  is the parameter and what do you measure   you want to measure how much time it will take now you cannot measure each and everything   in an algorithm because there is a time to create  the stack for this particular function there is   time for making this assignment there  is time for taking the comparison   doing this decrement and so on but what you try  to identify are the key tasks key operations   that you must do in doing this algorithm and that key here since you are trying to find   out sum is the addition operation and you can  also kind of reason that the number of times you   need to add your number of comparisons and  number of decrements here will be related to that   because every time you go through the for  loop you do a comparison you do a decrement   and if it succeeds you do an addition so these are very closely related so   approximately i can say that if i know the  number of additions being done i know how much   time will it take the number of additions  times the typical time of doing an addition   multiplied by a maybe a scale factor so if  i just count that this loop will go over n   times so the time is tn which is n additions  and what is the space space is simply the n where   you keep the number and the storage s of course  there is additional space in terms of temporaries   and all that which we grossly ignore in terms  of doing this analysis because it turns out   to be comparable to the user variables that you  have so you have two variables so space is two   so this is this is what you analyze   so if we take another example quickly   this is finding a character in the string  so what you are doing is you are writing a   this is the pointer to the string this is the  character that you want to find and you go over   that string starting with the index 0 so you  are trying to check str 0 str1 like this   and how far do you go you go up to strlenstr  which gives you the length of the string   so those many characters are there  those many times you have to do so   those many times this loop will have to execute  and for each c if it if the character is c   equal to c or not if it is equal you return  you are done if it is not then you return a 0   saying that you did not get it now given this so your what is the   input complexity input complexity is the  size of the string the longer the string   you need more time so your complexity parameter  is n now what is that you get to do naturally   again the key you are trying to see for match  so the key operation is compare so you have   and this loop can go on variable number of times  depending on where you find the character   so what you try to say you try to say okay  what is the maximum possible what is the worst   possible scenario i can get into which we  will discuss in subsequent slides also and   for that you know that if the character  does not exist at all then you will have   to go through the entire string so  you said that it could be at the worst   it could be n comparisons n comparisons now every time you go over this loop you do   strlenstr so it also has the n times it has  to do strlenstr so whatever time that takes   you will also have to take that now  what is strlenstr it has to count   going one after the other there is no other  way you have to count or keep on seeing   keep on incrementing an index till you get a  null character which is a c string terminator   so every time you do this you have n so  what you get n comparisons plus n times   n increments because you have to go through that  index and every time you go through the loop you   have to do this so it turns out to be n plus  n square and if n square starts going large   then i can say this is approximately n square  so we will say this is a quadratic algorithm   what is your space your space requirement  is the input is str and i c and you have   just the i so space requirement is still  very negligible now that is where you can   see the benefit of doing the analysis for  example you are just checking in the string   so the string does not change as you go over the  loop so strlenstr value that you will get   the first time the second time the third  time the nth time will all be the same   then you are basically losing efficiency but  by computing it several times unnecessarily   so what you could do you could remove this  and before that loop you could say len assigned   strlenstr and make this length then what  will happen this will get computed in n   and you will not need this strlen time after that  so this will turn out to be n plus n which is   which will be n so from quadratic it will become  linear a very simple one statement change can   drastically improve your algorithm and that is  what you analyze for the time and the space     so there are a couple of more examples   that i have provided here like finding  the minimum of a sequence of numbers   like in in different ways so that you can see  how you are actually getting the improvement   and you can go through them and practice so  this is what this is what you look for     now the question question is how  do you do it how do you given a   given an algorithm or a program source how would  you do the analysis it is not a trivial question   now there are different models one is counting  model like whatever we were doing is a counting   model we are just observing and seeing okay this  is this is what likely likely to be you know   more dominant let us count how many times it  can happen it is not always easy to do that   also given the given the all different  kinds of instruction you could have   the counting could turn out to be a very complex  formula so we do something of an approximation   which we say is an asymptotic analysis  so these are some of the basic methods   and then you have particularly since several  algorithms are recursive it is very difficult   to do accounting reasoning on them  so what you do you write corresponding   recurrence relations use generating functions to  solve them and get an idea about the complexity   number or there is for a large class of  problems large class of recursive decomposition   you have a basic theorem known as the masters  theorem so this is dominantly useful for   recursive algorithms and and since we are not  doing an algorithms course i am keeping this   aside because we just need the  basic results to go forward     so we have already seen some part of the  counting model where we try to find out the   frequency of different operations get an estimate  about the cost and then your total running time   or total space total running time is a sum of  cost times frequency for all operations and for   this we use a simple randomaccess machine  model where we know the input data and size   we can identify the operations which are known  a priori and we can identify the intermediate   stages and we look at output data and size and the maximum of these that is certainly you   will need to read the input not certainly in  most cases there are problems where you do not   need the need to read the entire input  and you will need to write the output   and you will need to do the operations  so the maximum of these three   will give you the complexity and that is  the basic approach to analysis because if   you have to read the input then you cannot have  a complexity less than the input size if you   have to write the output then you cannot have an  complexity which is less than the output size   so the first one we call input  complexity based on this i am sorry   this is called input complexity this is called  because you will have to write the output   in any case complexity and whatever operations  complexity so the max of these or basically   the sum of these i am saying max because  something will dominate anyway so is the total   complexity and that is how you you analyze   so this is just a factorial example this is   written recursively i am not using generating  function but it is very easy to see that you have   to find factorial n it is n times factorial  n minus 1 so this will go down to factorial   0 or factorial 1 factorial 0  which i know explicitly as 1   so n times this call will be made it is very  clear so n times a call will be made means   this multiplication will be done n minus 1 times  because the first call with n does not have a   multiplication so it is n minus 1 multiplication  but it needs a lot of space because as you keep on   calling none of these functions are returning so the call stacks it is called i mean   i am sorry activation records or stack frames  keep on staying on the stack so there are   n plus one stacks the the the caller the n  invocations of the factorial will be there so   there will be the space will be of the order  of n plus 1 times a constant because every stack   frame will take a size n minus 1 is the number of  multiplications times a constant is your time   but if you write this simply in an alternate  iterative form where you start not from   n down to 1 but you just keep on multiplying from  1 up to n if you just do that as in here then   obviously again you will have n multiplications  but you will just need one function call in this   function itself so all that you need is n and t  so you can see that just moving from this to this   you make a great advantage by reducing your space  significantly so this is earlier we showed an   example where you were able to reduce your time  here is an example where you can show that you can   reduce your space so this is how you analyze  typically in terms of a counting model     now the core idea is that we cannot  often just do comparison in terms of   whether it is n or n plus five and things  like that the comparing actual times   is not a good idea it is not possible the actual  time will depend on the processor resource i   mean processor clock speed it will depend on the  particular processor architecture it will depend   depend on the bus speed and so on so forth so comparing actual i am sorry   comparing actual time is not an option so what  you compare is growth that is what hurts you   because you do not if n is small if your input  size is small you really do not care how much time   a database table will take to look it up because  it will be less anyway whatever way you do it   if n is large you really care so you want  to know that if you keep on increasing n from   a small value to a large value how is your  time and space going to grow it is the growth   that is what you are interested in and you are  not interested in a specific number or a specific   function because you cannot compare them  so you want a approximation of that   and that is what is known as the  function approximation which we   call which we denote by a bigoh notation there  are other complexity notation like bigomega   bigtheta which relates to the theory  of lower bounds so we are not getting   into those often you solve a recurrence in the  growth function to to get the growth function   but we will just show you simple examples   let us say this is an example of   there is there is what is this trying to do is  is going from 1 to n you have an array a and   then for every i it is going from i  plus 1 to n and it is checking so   it is basically checking on combinations to  check if the sum of any two elements is 0 or not   or how many times it is it is 0 how many  such sums are 0 so that functionality   you do not really bother about now the question is if we do a   counting model on this then you will see  that how many what is the frequency of that   you have it one so i am just trying to look  at the variable declarations the number of   times you will have to hit at the declaration  one is here one is here which will happen   once and this will happen multiple times  every time you go do the outer for loop   you will start the inner for loop  so you have a declaration to process so   the outer for loop will happen n times so the  inner for loops int j declaration has to be   processed n times so this is one this is one  and this is n which makes the total as n plus 2   the same thing can be said  about the assignments    so these are the assignments actually  we call it initialization but in here   it will act as an assignment which is n plus  2 how many times you do less than comparison   this is this because you do it for every time you  do it for the outer you do for the i times for   the inner so it is it is 1 plus 2 plus like 3  sum of natural numbers so it is what it is   equal to comparison will happen as  many times the inner loop can happen   inner loop iteration can happen  this is this will be the array axis   double of that two times of that  because you have two axes here   and increment will depend on what are your data  it will not happen always these increments are   fixed but the increment here would vary depending  on the data so this is this is what you get   so this is your total set of analysis  so if you put sum all of them together   you will get a complex quadratic you can  see that n square is a maximum so you will   get a quadratic expression that is fine  but what we do we do we do an approximation   we say that the growth of n plus  2 and the growth of n are similar   they do not significantly differ in the growth  they will grow in a similar way similarly the   growth of this expression and the growth of half  n square or for that matter n square is similar   they will just differ by a small factor  but the growth will be the similar   so in this way we what we do  is in simple terms if it is a   these these all happen to be polynomials so  what we are doing is in simple terms we just take   the dominant term the highest degree term  and say that is my tilde approximation   or asymptotic approximation so all that  you get all are these approximations so   what i do i add up all of them so i  end up having alpha n times beta n square   where alpha beta will get values  so this is the approximation   and then we say that well n square will  grow much faster than n n is linear   n square goes like this so i can ignore   for large n i can ignore alpha n because  if n is small they are comparable and i   do not care what happens with the small input  size i do not care what is the complexity   if it grows large then this this gap keeps on  increasing this gap keeps on increasing so   what matters is the quadratic term  beta n square so i can say that   alpha n plus beta n square what is the  tilde here is actually beta n square   ignoring the linear term if i say that then  i can say that that is actually n square   because beta is just a constant factor   so that is the whole idea of saying  that it is asymptotically n square   we will write this as order n square what this  tilde function really means is f is approximation   to g is an approximation to n if the  limit of f by n n tending to infinity   is 1 and you can check in every case here  that that limit actually is what holds     so this is just a chart to show you how does  the typical known complexities of 1 which is   constant log n which is here logarithmic  linear linear logarithmic that is n log n   quadratic cubic you may not be familiar to see  these curves in this form because it has been   plotted in a log log curve your x axis is log  as well as right axis is log so that as you   grow in the polynomial basically your line always  remains same you are just changing the gradient   it is just rotating like this and that  tells you how the cost is increasing     if you would like to look at it in your familiar  graph plotting then this is how it will look   like so you can see that if something is order n  it goes like a straight line if something is log   n it is much less almost like constant n log  n goes like this but powers of n go very far   fast exponential of n are almost vertical  that with a very small n it explodes so   you cannot these are what you cannot  afford this is maybe c greater than two   should be avoidable this is preferred  this will be good and this is excellent   so that is what along this axis  that is what we will thrive for   and that is the basic take back from this   here i have given a list of these complexities   and typically a description just focus on this  description when you study to know as to where   you can typically find this complexity   so this is the formal definition i am not   very concerned about the formal definition  you have done it in the algorithms course   this is just for your reference   and finally what do we where do we do the   analysis there could be different situations but  we will focus primarily on these two the worst   case that is if i take all possible cases what  is the input that gives me the worst complexity   and also if i take a any random input assuming  inputs are distributed in a certain distribution   then what is the expected running time we will  get we will typically talk of these two these   are other forms of complexity analysis that  exist but primarily databases or things used   in databases we will primarily focus in  the worst case and the average case     so i will end with a complexity chart this is  not for you to remember but to refer where for   the common data structure some of these i am  going to discuss in the next two modules what   are their different access times so if you this  is just a quick reference that you can make     and another is for the sorting algorithms what  are the different complexities you get in the   different scenarios both in terms of  time as well as in terms of space     so with that quick introduction or  the i mean i should say the revision of   algorithms and complexity we conclude this  module thank you very much for your attention   and we will meet in the next module and  talk about some of the data structures 