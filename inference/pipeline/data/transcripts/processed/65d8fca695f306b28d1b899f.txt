ai say i want to get a cookie from a jar that’s on a tall shelf there isn’t one “right way” to get the cookies maybe i find a ladder use a lasso or build a complicated system of pulleys these could all be brilliant or terrible ideas but if something works i get the sweet taste of victory and i learn that doing that same thing could get me another cookie in the future we learn lots of things by trialanderror and this kind of “learning by doing” to achieve complicated goals is called reinforcement learning intro so far we’ve talked about two types of learning in crash course ai supervised learning where a teacher gives an ai answers to learn from and unsupervised learning where an ai tries to find patterns in the world reinforcement learning is particularly useful for situations where we want to train ais to have certain skills we don’t fully understand ourselves for example i’m pretty good at walking but trying to explain the process of walking is kind of difficult what angle should your femur be relative to your foot and should you move it with an average angular velocity of… yeah never mind… its really difficult with reinforcement learning we can train ais to perform complicated tasks but unlike other techniques we only have to tell them at the very end of the task if they succeeded and then ask them to tell us how they did it we’re going to focus on this general case but sometimes this feedback could come earlier so if we want an ai to learn to walk we give them a reward if they’re both standing up and moving forward and then figure out what steps they took to get to that point the longer the ai stands up and moves forward the longer it’s walking and the more reward it gets so you can kind of see how the key to reinforcement learning is just trialanderror again and again for humans a reward might be a cookie or the joy of winning a board game but for an ai system a reward is just a small positive signal that basically tells it “good job” and “do that again” google deepmind got some pretty impressive results when they used reinforcement learning to teach virtual ai systems to walk jump and even duck under obstacles it looks kinda silly but works pretty well other researchers have even helped real life robots learn to walk so seeing the end result is pretty fun and can help us understand the goals of reinforcement learning but to really understand how reinforcement learning works we have to learn new language to talk about these ai and what they’re doing similar to previous episodes we have an ai or agent as our loyal subject that’s going to learn an agent makes predictions or performs actions like moving a tiny bit forward or picking the next best move in a game and it performs actions based on its current inputs which we call the state in supervised learning after each action we would have a training label that tells our ai whether it did the right thing or not we can’t do that here with reinforcement learning because we don’t know what the “right thing” actually is until it’s completely done with the task this difference actually highlights one of the hardest parts of reinforcement learning called credit assignment it’s hard to know which actions helped us get to the reward and should get credit and which actions slowed down our ai when we don’t pause to think after every action so the agent ends up interacting with its environment for a while whether that’s a game board a virtual maze or real life kitchen and the agent takes many actions until it gets a reward which we give out when it wins a game or gets that cookie jar from that really tall shelf then every time the agent wins or succeeds at its task we can look back on the actions it took and slowly figure out which game states were helpful and which weren’t during this reflection we’re assigning value to those different game states and deciding on a policy for which actions work best we need values and policies to get anything done in reinforcement learning let’s say i see some food in the kitchen a box a small bag and a plate with a donut so my brain can assign each of these a value a numerical yummyness value the box probably has 6 donuts in it the bag probably has 2 and the plate just has 1… so the values i assign are 6 2 and 1 now that i’ve assigned each of them a value i can decide on a policy to plan what action to take the simplest policy is to go to the highest value that box of possibly 6 donuts but i can’t see inside of it and that could be a box of bagels so it’s high reward but high risk another policy could be low reward but low risk going with the plate with 1 guaranteed delicious donut personally i’d pick a middleground policy and go for the bag because i have a better chance of guessing that there are donuts inside than the box and a value of 1 donut isn’t enough that’s a lot of vocab so let’s see these concepts in action to help us remember everything our example is going to focus on a mathematical framework that could be used with different underlying machine learning techniques let’s say johngreenbot wants to go to the charging station to recharge his batteries in this example johngreenbot is a brand new agent and the room is the environment he needs to learn about from where he is now in the room he has four possible actions moving up down left or right and his state is a couple of different inputs where he is where he came from and what he sees for this example we’ll assume johngreenbot can see the whole room so when he moves up or any direction his state changes but he doesn’t know yet if moving up was a good idea because he hasn’t reached a goal so go on johngreenbot explore he found the battery so he got a reward that little plus one now we can look back at the path he took and give all the cells he walked through a value  specifically a higher value for those near the goal and lower for those farther away these higher and lower values help with the trialanderror of reinforcement learning and they give our agent more information about better actions to take when he tries again so if we put johngreenbot back at the start he’ll want to decide on a policy that maximizes reward since he already knows a path to the battery he’ll walk along that path and he’s guaranteed another 1 but that’s… too easy and kind of boring if johngreenbot just takes the same long and winding path every time so another important concept in reinforcement learning is the tradeoff between exploitation and exploration now that johngreenbot knows one way to get to the battery he could just exploit this knowledge by always taking the same 10 actions it’s not a terrible idea  he knows he won’t get lost and he’ll definitely get a reward but this 10action path is also pretty inefficient and there are probably more efficient paths out there so exploitation may not be the best strategy it’s usually worth trying lots of different actions to see what happens which is a strategy called exploration every new path johngreenbot takes will give him a bit more data about the best way to get a reward so let’s let johngreenbot explore for 100 actions and after he completes a path we’ll update the values of the cells he’s been to now we can look at all these new values during exploration johngreenbot found a shortcut so now he knows a path that only takes 4 actions to get to the goal this means our new policy which always chooses the best value for the next action will take johngreenbot down this faster path to the target that’s much better than before but we paid a cost because during those 100 actions of exploration he took some paths that were even more inefficient than the first 10action try and only got a total of 6 points if johngreenbot had just exploited his knowledge of the first path he took for those 100 actions he could have made it to the battery 10 times and gotten 10 points so you could say that exploration was a waste of time but if we started a new competition between the new johngreenbot who knows a 4action path and his younger more foolish self who knows a 10action path over 100 actions the new johngreenbot would be able to get 25 points because his path is much faster his reinforcement learning helped so should we explore more to try and find an even better path or should we just use exploitation right away to collect more points in many reinforcement learning problems we need a balance of exploitation and exploration and people are actively researching this tradeoff these kinds of problems can get even more complicated if we add different kinds of rewards like a 1 battery and a 3 bigger battery or there could even be negative rewards that johngreenbot needs to learn to avoid like this black hole if we let johngreenbot explore this new environment using reinforcement learning sometimes he falls into the black hole so the cells will end up having different values than the earlier environment and there could be a different best policy plus the whole environment could change in many of these problems if we have an ai in our car helping us drive home the same road will have different people bicycles cars and black holes on it every day there might even be construction that completely reroutes us this is where reinforcement learning problems get more fun but much harder when johngreenbot was learning how to navigate on that small grid cells closer to the battery had higher values than those far away but for many problems we’ll want to use a value function to think about what we’ve done so far and decide on the next move using math for example in this situation where an ai is helping us drive home if we’re optimizing safety and we see the brake lights of the car in front of us it’s probably time to slow down but if we saw a bag of donuts in the street we would want to stop so reinforcement learning is a powerful tool that’s been around for decades but a lot of problems need a ton of data and a ton of time to solve there have been really impressive results recently thanks to deep reinforcement learning on largescale computing these systems can explore massive environments and a huge number of states leading to results like ais learning to play games at the core of a lot of these problems are discrete symbols like a command for forward or the squares on a game board so how to reason and plan in these spaces is a key part of ai next week we’ll dive into symbolic ai and how it’s a powerful tool for systems we use every day see you then crash course ai is produced in association with pbs digital studios if you want to help keep crash course free for everyone forever you can join our community on patreon and if you want to learn other approaches to control robot behavior check out this video on crash course computer science 