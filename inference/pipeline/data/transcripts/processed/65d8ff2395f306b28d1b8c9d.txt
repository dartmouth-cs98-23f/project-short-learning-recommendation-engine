im andrew eddins an ibm quantum researcher and coauthor on the paper and in this video well talk about what we did in this experiment how we did it and in particular why quantum error mitigation is poised to play such an important role in nearterm quantum computing so what did we run in this experiment we used a 127 qubit processor to run a simulation of 127 interacting spins with each qubit playing the role of a spin and to do this we ran a quantum circuit with as many as 60 layers of two qubit cnot gates and remarkably we were able to measure reliable results at the end of the circuit which is exciting progress because it was only about a year ago that we started being able to run circuits with 100 qubits at all and the number of gates in these 60 layers or the 60 layer depth of the circuit is roughly double our previous record reported last year at ibm quantum summit in 2022 so even though todays quantum computers are not perfect they have some noise in the hardware were still able to extract useful results or reliable results using a class of techniques known as quantum error mitigation and so this is giving us space to start exploring what we can do with these devices even before the era of fault tolerance and long term quantum computing and in particular in this experiment we used a technique known as zero noise extrapolation or zne so how does zne work first well run our circuit and get some estimate of our observable so we want to learn some observable property o and we want to look in particular at the average or expectation value of that property we run our experiment and we get some results however this result may be made inaccurate by the presence of noise on the quantum hardware ideally wed like to get an estimate of what the answer would be if we ran this if we solved this problem without any noise so how do we correct for this inaccuracy brought about by the noise on the hardware well first well go and learn what the noise is actually doing how its behaving on the device so well take the problem that were studying well break it up into layers and then for each layer well further decompose that into two pieces one that captures the ideal behavior of that layer and another representing the noise and so by doing a bit of additional work we can go and measure how all of these noise pieces in the circuit are behaving and once we have that information although its hard to turn down the level of noise thats happening on the hardware we are able to use that knowledge to turn it up so by repeating the experiment in the condition where we increase the noise we can then get additional results which we can use to extrapolate back and estimate the true value in the case of no noise on the hardware so thats a bit about the basic theory underlying our experiment and with that out of the way ill pass past things over to my colleague and coauthor to further explain details of the experiment thanks andrew my name is youngseok kim researcher from ibm quantum like what andrew said im going to talk about a little bit more detail about what we did so to make a long story short what we did is we perform an experiment on spin dynamics of transverse field ising model so we perform experiment on our quantum processor and we work with our collaborators at uc berkeley and they produce corresponding results in classical computer and we compare our results against each other to build a confidence in our method so we use zne as our error mitigation method we use our ibm kyiv 127 qubit processor to study these spin dynamics to be more specific we map our spin lattice to our hardware topology which is heavy hex topology and this spin is governed by nearest neighbor interaction j and global transverse field h and as you can see here we have large parameter space to explore among this parameter space we have some parameter that results in clifford circuit meaning we can efficiently simulate this circuit thereby we obtain ideal value so we utilize this nice property to examine our results so heres the circuit 127 qubit depth of 60 twoqubit gates and since we know the exact solution along the way we check our results from quantum computer and that agrees well with each other so theres one check of course they are large parameter space which results in nonclifford circuit which is in general hard to verify instead what we did is we take the parameter that results in the nonclifford circuit equal shallower circuit thats depth of 15 and we examine low weight observable in this scenario we realize that theres a light cone where all the qubits within this light cone really matters for this particular observable and heres where our collaborator from uc berkeley comes into play they realize that using the qubits within this light cone they can use brute force numerics to produce exact solution so we compare the exact solution and our results from quantum hardware and compare against each other we realize that they have a reasonable agreement so heres one more check so we are building this confidence we go one step further so this time taking the same circuit we examine high weight observable which eventually accrues more qubits within its light cone this time our collaborators realize that brute force numerics are not feasible instead they use numerical approximation method specifically tensor network method they realize that using this method they still can obtain exact solution so we compare their exact solution against our results from quantum computer and they again agree with each other reasonably well so theres another check so note that all the results of over here are verifiable circuit meaning we have exact solution using classical resources its crucial step to do this work to build our confidence on our method so as a next step we would like to go a little bit farther namely we take the same circuit and we progress one more time step to make the circuit a little bit deeper effectively and we reexamine similarly high weight observable that eventually includes more number of qubits inside light cone so in this scenario our collaborator realized that its no longer feasible to obtain the exact solution even using numerical approximate method so now we are comparing to approximate solution against our results obtained from our quantum machine in that scenario actually what we ran is the following so again revisiting the part of the space there are results in clifford circuit here and we are actually tweaking our parameter that includes nonclifford circuit as well clifford circuit to verify our results by looking clifford circuit so in this scenario we looked at clifford circuit results and there we see a reasonable agreement between ideal solution and results we get from quantum computer but for numerical approximation solution from classical computer we start to see some deviation from its ideal value of course we dont have exact solution here so any results from nonclifford circuit represents an unverifiable circuit so we did this we did the same practice very similar practice but this time we go all the way to depth 60 and we look at low weight observable which eventually covers all the qubits within this light cone and we observe very similar behavior that produces reasonable but unverifiable results looking ahead we believe that some researchers will find a way to verify our currently unverifiable circuit thats good because it means quantum is driving innovation to classical computing using their technique if they prove that our results are reasonable thats again good because it means noisy quantum computer can produce reliable estimate on observable with interest and of course as hardware innovation progresses and our hardware gets better and better well have further access to deeper and larger circuits and we believe that this type of research eventually bring us one step closer to a day when a quantum computer can tackle a truly useful problem i hope you like this video be sure to like and share this video thank you for your time 