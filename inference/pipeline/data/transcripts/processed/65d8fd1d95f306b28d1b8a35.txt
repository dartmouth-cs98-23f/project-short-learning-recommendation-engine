or two days already this is our last session we usually say best for the last so our first speaker for this afternoon session is professor yajin wong from the study department at the university of wisconsin medicine and yazon was also peters student so but i i dont know young young number you know a long time still that is yeah and yeah hes also currently the department chair at the university of wisconsin at medicine so yeah john flores yours thank you well thanks hiya for the introduction uh according to the uh the student the peter lisa on his web page im the number 39 and so its thats the sources i know theres some kind of ambiguity or the missing data but that my source is from the hidden webpage so id like to thank peter for the uh uh many years inspiration and help and support and uh to my uh career and thanks peter happy birthday and i also like to thank the uh organizer for uh putting together this conference and uh for inviting me for this opportunity to give a talk and uh uh another things i like that you think is the simon institute and they organized the institute institute organized many quantum computing programs over the year i participated many in person before the pandemic and uh even more during the pandemic uh field of zoom and i feel is the kind of wonderful benefit maybe for the coving i can kind of move freely to attend those uh those uh quantum computing uh programs so i will uh for this todays audience i feel is for the statistics audience and i will i know the the barrier about the quantum computation and the concept i will try to use a minimal introduction of quantum computations to illustrate my point uh i i will keep like the focus at about my pocket at high level for the simple cases so that we can better understand uh understand the uh uh the things from the statistics aspects now uh whats the uh like first lets talk about the quantum advantage or the quantum uh supremacies uh the way we think of like when you compare quantum uh device combination device usually you do a theoretical proof okay on the theory another one is to build up device for the experimental demonstration okay now for quantum cases over the years theres a lot of like a theoretical study proposing uh quantum algorithms and actually you can prove uh there are many quantum algorithms which can be uh much faster than the classical algorithms like a wellknown like for example shorts are factoring large numbers and the algorithms which has been proving uh to be exponentially fast than the best known classical algorithm okay also the groove uh search algorithms which you can uh have been showed is uh quadrotically fast to the best proofed aggregate classical algorithms and actually the quadrotic fast is the optimal and that so basically it has been optimal in the quantum sense opening in a classical sense and you can compare to still have a quadruple speed up now we have all of those theories uh method algorithm proved in based on quantum theory now well we demonstrated can we demonstrate that one so we needed to build up a device try to run these algorithms to actually see what happened and there so in principle we could do is try to fight build up a device try to uh compete against the best super computer i need to see what happened there okay now unfortunately uh uh because of the technology difficulty what we can uh created are the quantum computers uh a much smaller uh scale and which cannot run those uh kind of like a theoretically proving uh algorithms and to show those quantum advantage so the way is can we based on what we have do something completed with the best i like that super computer which is the similar to like we show like the apple cars against the laptop and you try to find out some algorithms uh some computational problems you run on both to see what happens okay now uh for the case for the cases the fundamental computing cases against the best supercomputer its basically pretty much the same of this comparison its just to try to uh find out suitable problems uh to demonstrate uh some of the things and you may have heard about it that like the recent landmark project demonstrating quantum supremacies basically saying take 200 seconds computational job for the fundamental computer and that one would take 10 000 years or 06 billion years uh for this best super computer at that time and you finished at the 200 uh 200 second job and the fundamental computer did so the two uh project one is the google and the publishing nature uh its basically they find out uh socalled uh random quantum circuits whats that one it is a statistic assembly problem which i will illustrate and at the moment and basically show that one at that time the best fundamental computer as best the classical super computer would take it ten thousand years and the second project that we is published in the science uh 2020s by a ustc china and uh project the base build up another quantum computer and again a statistic example sampling problems is called both on sampling and they showed for data that that problem it would take the computational uh problem it would take the best of the super computer at that time to take maybe that point six billion years to finish that job so so what i will illustrate in my talk is trying to explain the statistics and uh associated with those projects now uh i have to to say that one is when i refer to classical computer okay and it means what we are currently using a electronic base okay and the second one i like to like it and say that one is although we call it quantum computer like a classic computer both a computer and either you be may not believing that the difference between the quantum computer and the current classical computer is much bigger than that these two like the uh the laptops and accuracy because of the functioning underlying mechanism so lets uh go down to the bottom and see whats the uh quantum computing uh its basically its like the classical computing all started with bits like zero and one and in the quantum cases we have a quantum beat and we call it qubit and its also zero and the one we use this uh cat these kind of like notations uh to to change it in the quantum bit okay now unlike it in the in the in the uh classical cases you only have these two points quantum theory allow us uh qubit in the superposition state is basically you have a different one any linear combination of these is also a state and so that that those two coefficients are amplitude they have these kind of unit normal constraints okay and uh in a moment you will see this will have that probability sticks and uh consequence uh for for for for the uh rental nature of the quantum cubit theory now if you from these superpositions you can see if you identify those two or the base okay so any linear combinations of this one superposition state corresponding a point in the two dimensional space okay so the superpositions is corresponding all of those fields at the point every point corresponding to a state unlike the classic classification they only got a two point is in the classical cases either on or off like a coin like a pro and the cons okay now in the quantum cases you can think about it that coin which have all the state from the pro and anything continues and and of this one so you will see that in the much bigger space okay now because of the superpositions you can think of interpret that one is uh in the classical cases either head or tail okay and uh in that quantum case you can have a both head and tail both zero and one cases continue into the simultaneously now in the classical cases we now it is zero and one and uh you know which state is you always easy to check now in the quantum cases you cannot directly exempt the qubit to find out those coefficients but there is a you can measure that qubit and then theres a stochastic random outcome here so what happens is if you make any measurement of the qubit what you would see it will be collapsed to the two points beta 0 and 1 in the random cases now whats the probability of observation 0 is that corresponding coefficient amplitude a modulus square okay and the with 1 is for this coefficient modulus well so thats the constraint of equal to one allow these probability uh interpretations thats well uh the probability uh comes into play in the quantum and in the cubic cases so remember that one is anything you have a qubit you want it it will if you wanted to do measurements and get a result like the like digits and then theres a probability there okay you can think about the probability distribution and because of this one all the quantum computations algorithms and things are random algorithms and the distribution associated with that one okay now moving to the multiple qubits okay and in that two cubic cases you would see there will be four of them zero zero zero one one zero zero one one okay so that any superposition will be linear combination of those four so if you think about those are the bases it occurs the space you need to handle the two grouping itll be four dimensions if you have three cubits eight animation so uh if you have a vehicle it will be true to the power of b dimension you can see the exponential scale up okay the superposition expansion scale so thats the uh cubic now uh similar to that to the classical computer cases we need to build up a gate circles uh to get make the computation moving so in the quantum cases you can also uh build up the gates and like for example you can change the zero to one one to zero okay but remember we have a superposition okay so the quantum theory is saying that all the cubic degrees are must be unitary the transformation must be unit of any of this one unlike the classic basic it must be unitary and that one is represented by a unitary matrix okay so that any of these you would see uh compare with this one you transform the state and then any supervision will happen same with similar transformations these are unitary matrix so if you think about a gate is a unitary matrix okay now the circuits quantum server its just connecting a circuit of a gates together quantum gates together okay uh so that one is this circuit is kind of a performing uh if you carry out this circuit and do a bunch of those unit universal information you are performing the certain computational task and that task basically if you you can think about all those connected if each of the gates have a unitary matrix you multiply together so you will have a big unitary matrix for these circuits so in the end you want to see whenever we talk about the circles you can think about a unit or matrix big unit or matrix corresponding to that one okay and so this is the combination now whats the exciting uh oh here is just some pictures illustrated that the cube is here and you can apply the case and you can uh one or two cases and if you carry all of them it corresponds to a quantum circuit now whats exciting about it the quantum computation is the possible speed up of fast algorithm to uh performing certain computational tasks which is due to the many quantum uh property which is in the classical cases we dont have like supervisions entanglements and so on and so forth so if you have any algorithms uh on the on the buildup on the circuits uh carry out that one youll have under result automation everything is random okay so its basically its like the mcmc algorithms your real ones it may give you the correct or incorrect solution okay but there is certain probabilities you will get the correct solution okay so you now you repeat youll eventually get the correct solutions so that any algorithms correspond the end result will have a probability that the retinal outcome solutions they will be have a distribution associated with that one okay now when we say the compile compare the classical computer and uh and the quantum computer you would see uh whats the against these two its basically that using youre using the classical computer to solve a class a computational problem if the problem is the same you just use the given device to solve the same problem okay now the classical computer can only run the classical algorithm quantum computer can need to run the quantum algorithm now the speed up of the mission that is the uh the interesting about these things is that potential quantum speed up there are many theoretically proven quantum algorithms which has uh have uh speed up uh over the classical algorithm and i wanted to put that one not always as a matter of fact and the many cases we dont fundamental computers dont have to speed up its just for certain problems it may have quantum speed up now again whats the issue here is because we dont have the large scale quantum computer to uh render theoretically proven uh algorithms which we can demonstrate the quantum advantage and in the practice what we have is pointed we can build up a strategy with a much smaller quantum computer and which may not be uh imperfect and we refer to the near term quantum computer we still wanted to find out some problems which we will be using calculate like the performing uh on the on the quantum computer against the best supercomputer to solve that kind of problems trying to show theres a quantum advantage an advantage so this is the kind will refer to quantum supremacy okay now uh the two uh projects automation early on are the uh one is google another one is the uh ustc projects and google they build up a quantum computer is based on the super conducting and qubits with the 53 cubits at that time and the designer problems is uh either refer to the random quantum circles and the statistical sampling problems and the rent on this uh on this uh this device and and that you solve that problems and you will see i would in a moment i will explain whats the random quantum surface and the usgc is based on the uh optic quantum optic systems uh photons and uh buildup uh is uh its in the 20th 2020s and uh they also uh solved our sampling problems and i will explain this one whats that bozon random sample uh both on sampling problem there are both are statistical problems to solve for that one now why they choose assembling problems uh automating that one that because of a limitation the quantum computer so far we have is relatively small and its uh its noisy and so we needed to find out the problems are very suitable for those device okay to to perform but on the other hand we wanted to use that problem its very hard for the super computer okay to solve so we can demonstrate advantages so that the two things so easy relatively easy for the quantum computer and the very hard for that uh super computer now lets lets explain whats the uh random quantum service and basically that the google computer we have at that time have 53 qubits basically you have all those qubits they apply the gates okay randomly select those gates apply on these each of those qubits from a set of gates apply on the other one and these stop the other other single qubit and those one are the like two cubits basically you can think about its kind of like that uh like its like the unit of a matrix multiply on a cubic basis and they will repeat okay you think about one round then regular sampling another round and theyre giving the m wrong so why they do the div even wrong because they wanted to increase complexity increase the hardness for the classical computer to do the sampling okay now in the end after you apply all those gates the quantum circuit that the qubits in this quantum computer in this case you make measurements okay you will get the sum of the each cubic that will be one zero or one so in the end you will get like in this case the 53 length of one zero bit stream okay so thats supposed to get that thats your data okay so in this case if you have a b bits those bit string will be belong to this space length of a b and zero one okay now how many number of this one is two to the power for b so exponential uh increase you want to see if you make these these like the bigger eventually and make these steps like the cycles more eventually you will make it so hard for the super computer to calculate the simulated leads okay now for the quantum computer what you do is you just carry out this one in the end make a measurement you will get the data okay so if it turns out these problems can be illustrated of the size biased sampling uh uh approach scheme in that cases its just in this probability space okay this way the probability space do the sampling now whats the sampling in this case remember for each realization of these reynolds you have a circle you can make measurements there is a probability distribution over there okay and then theres another layers of because we apply those gates random okay so theres another randomness there the two layers of retinol means there so its like its like the uh two layers of reynolds so you you know make that the classical combination so hard because of that one so there is idealize the cases like they can make it argue that one if that adapts go to infinity and you will approach it your socalled the asymptotics is the ideal product times distributions in that case now what is the scheme you can see this corresponding to if you trade this this circuit is corresponding a unitary matrix on the 2b times the 2b although the unitary matrix is this one this idealized as a methodical limit sense either corresponding to a you if you put a higher measure on these all of those unitary matrix and you sample according to this how measure okay and what what happened in that one each fixed sample of the qubits sorry unitary matrix corresponding one realization of these qubits you make the measurements basically that they order the cubits in those states so if you make a measurement you observe any of the bit string in this space it will correspond in the probability of an amplitude square automation early on now differentiated here this amplitude is also random okay theres another retinal knee scale so well call the random circuits so this renault knees under the idealized cases in illustrator at this point you sample iid from exponential distribution okay and then they normalize this one and make this amplitude square corresponding to this probability okay so what happened is that observed data is the bit stream its essentially uh the bit stream uh those bit string youre observed according to this probability two layers of probability scheme and in these cases okay space now as i mentioned that our cubits are not a perfect theyre the law is still this is the ideal case you have this probability distribution of quantities actually there is added additive noise there so that in the real cases corresponding this one the model is this probability observed those strings is equal to some epsilon times the idealized probability plus one minus epsilon and here we put some noise and this one is the uniform noise on this space okay so that is this epsilon is the fidelity basically the measure uh whats the uh like closeness of the sampling distribution with this ideal distribution or is it how many uh like the percentage of the data streams are really from the output distribution from those uh carried out to the circuit now if you forget about all those quantum lists you can summarize in the idealized case whats the sampling problem the sampling problem is like a biased assembly you have this probability space discrete with the cardinality of the equal to b okay you have a sampling scheme the assembling scheme is basically is all of those bit string in this space you draw one of each of those bit string is according to this probability okay now whats this probability is the weight of this uniform and those from the exponential okay carry out the probability okay the issue is uh the b increase okay the real problem is the size increase as the depth increase the simulated this board using that classical computing is so hard remember the classical computing you when you need a sample from something that you need to know the true distribution okay to find out whats the true distribution its very hard so hard as you increase the b and that in an increase m at that time the best the super computer can do is about it at b equal to around 50 m is about a 20 okay beyond that one we dont in principle there is the truth there but we dont know and and in the super computer cannot calculate that one remember the quantum computer do disassembly is relatively easy they are not calculating the underlying true probability they just ran this process uh to get the sampling okay so in a sense is that is the physics okay now you would see that when we talk about the computing everything is about physics and because we dont pay much attention if you do the sampling and using our current computer like mm mcmc basically you are using the electronic physics to do that simulate sampling and in this quantum cases they use in the quantum theory to calculate to do the assembly so the cases here is that the sampling problems also push this uh away it will be so hard for the uh for the classical building but quantum computing you can sense that one is eventually we will well get there demonstrate that things uh similarly for the uh for the boson sampling is using the uh the process from the uh optical systems you build up the photons passes through the beam sleeve and uh splitter and uh and the phone resources feed split basically build up uh operating systems and theres a m like a photons passes through this network at the end the photons will be appear here and there youll find out where they are because the retinal needs the location of this you detect the location will have random so the random there will be have a underlying distribution okay and that underlying distribution similar to the random quantum surface is output distribution okay now in this case i we can write down exactly whats the output distribution the sample space is basically you assume that the m photons and m mode and also the state will be the m of those digits and the only constraint is those uh number must be equal to total number of and so you can easily find out the cardinality of this one is equal to this combinatorial number okay also remember this that this is the operating system is that there is a urinary matrix represented on these four systems like the circuit cases so there is a unitary matrix size of m times m and represented the whole system okay now with those things we can write down the probability of the true probability distribution so basically being each of those states this way the probability is equal to this denominator easy the numerator is youre using this state to from this this big matrix youve got the sample matrix from this one then you need to calculate the permanent okay not a determinant permanent is like the determinant except that the no signs minus i plus j here the no signs its just all possible of all possible summation over here so it turns out unlike the determinant much easier to calculate the permanent is the p sharp complete complete uh complexity its very hard so essentially its the for the quantum computer you just ran this process you get the detected photons you got the sampling but for the classical computer in order to do the sampling you need to calculate those probabilities okay and then according to the probability due to sampling and calculating those probability is very very hard so also we increase the certain size of a certain point and you will see the quantum computer the classical computer wont be able uh to do that one okay so this is the similar the cases is the automation why those are sampling are easy for the quantum computer so hard for the classical computer okay now the one is uh here is you can see it as we in principle right now as were pushing we can find out the way we can move in these things to that regime that is part of supercomputers wont be able to handle within the intractable regime now in that regime we dont know whats the truth because we cannot calculate quantum computers simulate the samples now how do we now you build up a quantum device you draw some data okay and how do you know this one is truly from that one the distribution which we wont be able to calculate so that to to demonstrate the claim of economists and premises you would need a statistics okay again thats a statistic to come to play is the one how do we benchmarking to prove uh quantum speed up in that case the sample you generated from that device indeed is from the uh true uh in targeted probability distribution now here there is the issue like you have the input and you have a quantum computer with resources you have the uh input from a classical computer with those uh resources you get an output quantum cases the output following the certain distributions and in the classic phases you may use the randomization and if its another retinal and with the distributions uh to indicate the quantum speed up basically for the classical computer youll need much more resources like the computing time piece and uh those kind of things then the quantum computer and then you can prove that there is a uh there is a quantum speed up now here automation how to uh prove these cases uh we need to uh generate a sample basically you need some kind of a benchmark apply on those distributions generate a result classical quantum cases and those are the performance on those benchmark two things we need to show one is statistical efficiency another one is the computational efficiency whats the statistic efficiency remember those quantum distributions of distribution are grow exponentially with the weightless with the size so if you wanted to try to estimate those distributions okay or anything uh like the fancy enough for those distributions you would needed an exponential uh sample size to estimate those things so there was the distinct efficiency we wanted means it went only allowed to have a polynomial symbol size what would allow us to estimate those things because in the interactive regime we dont know the truth we only have a sample from quantum computer we need to use statistics to estimate those benchmarks okay and you will later on you will i will show that on many like usual things wont be able to have that uh that statistic efficiency now for the computational efficiency it means we can calculate those uh benchmark in our polynomial time okay what are the explanation time so those one are the issues associated with that one now in this very simple cases in the factoring problems they you can easily say that both statistical and computational division like if you go to the integration of prime you put a number there the solution in the correct order just multiply those together simply find out its cracked or not and so the computational efficiency is very easy so the distribution is also easy because you estimated those probability you dont need a database experiment you only needed a polynomial number of an assembly to estimate those those things now in the cases of the noise quantum computer okay remember the one what do we have its not a perfect fundamental computer its the noise version instead of the noise there so we can we can define the similar case if you put a star here to illustrate it they are the noise version of the idealized cases there is a noise uh version of that one so we needed a benchmark we need to apply the measure that noise uh like the uh generate from noise quantum computer generated data and the classical uh generated data away and again we need to handle statistical efficiency and combination efficiency for those uh benchmark uh automation if youre using the usual things like for example the distribution of these distribution theres a true distribution okay and of of this one for the truth basically all of those targets were trying to looking for the true distribution okay now if youre using compare this one using all those distance okay usual distance and it turns out uh the proof basically using the quantum thermography and audiencing you would require exponential number of sample size to and illustrate to estimate those uh those difference and and estimate so it wont be a statistically efficient and the google did define the socalled cross uh entropy benchmark and using some of the as a reporting like i mentioned that toms portland port thomas regime distribution to come up with that one we showed its a it is a statistical division that only needed that polynomial sample to estimate this benchmark uh computationally none of them are computationally efficient is because quantum computer im not trying to calculate the underlying true distribution the classical computer wont be able to calculate uh do the sample calculate the true uh distributions over the for that for the over the like the quantum supremacy regime like once at that kind of intractable regime uh no one knows whats the true underlying output distribution so you can only like stop this will will work so largely to rely on the statistics you build up uh using those samples but you didnt need the calculator the two of them need to calculate you need exponential number of times so what you do you use in the statistic data uh to build up statistic models and do the extrapolations and illustrate this point now whether this is a case is sure that one is still there that theres another uh uh enough uh things to do research on this part so this is the one of them that illustrate uh that point how much time do you have okay five minutes alright so uh this is the basic uh uh scheme and to study for this one now whats the uh result like the the two uh project like the google and ustc and they showed that uh quantum supremacies so basically they all of those results are amazing that they are based on circumstantial uh evidence because theyre intractable regimes we really dont know whats whats the truth and uh on the other hand we want to claim that it will happen or that happened okay so basically what they do is they build up and use the engineer physics and simplify the models and uh collect the data and analyze what happened in the interactive regime okay and then trying to infer what happened in the uh sorry tractable regime what happened in the interactive regime you see what happened there now in the like the google studies they uh for these are simplified like circles and they collect the data sample and theyre using quite a sophisticated statistics based on 30 million of the data and in the end of the estimate it is a benchmark and its 02 uh percent what it means uh in that sense the claim is about that uh 02 percent of the bit strings are truly come from the idealized output distribution okay now if youre using the using the super computer okay classical super computer trying to generate a bit stream with match up with at least with this kind of like that uh like the uh fidelities like this much of the data come from output distributions uh they illustrate that one is with a different with 53 uh qubits okay fifty three cubes remember that uh is the stated omega is the two to the power of 53 thats a big size okay and with the depth 12 they were estimated two hours to calculate this one and by the classical computer and increase based on these models statistic confidence bootstrap all those things now in the end of the estimate at the trendiest that one we dont we the supercomputer cannot calculate what happened there the estimator build up this model you see that one require 10 000 years to do that like the sampling and the 200 uh a second assembly december by the way one minute one billion bit stream okay and to assemble one billion businessmen it will take the ten thousand years by the uh ambassador super computer to that time now again if you wanted to calculate this one this part is that really if you want to truly calculate it you can see the break they did in the calculator what happened it would take millions millions of years more years to actually you calculated those uh underlying distribution this is just the sampling project now once this result published theres many immediately like like the challenge reviewed about the claims result and with the main is ranging from the device noise the new algorithms come out and the combination models the statistical uh issues the evidence all of those things are related to that one and so theres a lot of like the debate discussion and continuing uh phone against on both sides and about these are issues now for the ustc project and they actually argued uh is theres a 099 uh percent of fidelity basically theres not much noise theyre trying to see that in the simplified cases or the smaller cases they can check with against the underlying distributions and find out very uh feet and then they just said well in the in the intractable regime should be should we work and also the buildup for the classical computings whats the time required the sample data are 200 second jobs and basically it estimated it will be like that 06 billion years at the best quantum computer okay now of course those are tons of like issues and the debate and even largely is because they existing a wide range of uh views about the quantum computing ranging from the hype to the uh skepticism of skepticism about the fundamental complications and even for these quantum supremacies uh they illustrate even just from the purely statistical point of views you would see that a lot of like uh issues to be uh resolved so if we expect a longterm race between the uh fast like the classical algorithms and the improved device so it will be continue probably until we build up a quantum computer actually can run those like short algorithms and that you immediately get to to demonstrate that theoretically proving advantage maybe at that point things can be uh settled now this is just one cases furthermore is that if theres a lot of more of the interface between the statistics and the fundamental computer remember to build up a quantum computer you are needed like the many cases like that theres quantum systems that so many parameters models and youll see involve a lot of issues due to for building uh for building quantum computing in in the involved statistics and also you build up the device you will see how do now the device will performance according to your design so that the certifications based on the many uh issues is highly related the quality inference and in that part so another on on the other hand if we have uh like the quantum computers you would see the greater potential to revolutionize many of the statistical computations and the machine learning things now much more of of the much beyond these statistics and a lot of things going on in that quantum computer thank you any questions for you adam i really mean not true that i understand you compel quantum computer to general prepares computer what you call classical general purpose computer to be fair you have to compare it to a silicon based machine that is specially proposed built to solve a specific problem can you what would be the result then thats a fair comparison otherwise i would ask you to compare quantum computer to leave computer which is the best computer i know to where to compute the path of a leaf falling in the freezer yeah thats real fair questions and i think that the one we are not at that stage i think because of that uh like at this point we have all those theory they are approved and we just people are trying to say can you show some things and in the in the in the real calculations and this one again is just one of the uh demonstrating the concept maybe you can say in this like in the statistic sense and uh uh to show that either cases we can compute that one like that that seems you you you mentioned yes it failed but we are not that yet far from there yet okay so is there a program that people can turn every classical algorithm with a quantum computing computer algorithm is there a systematic approach to do that yeah its basically that all the classical algorithm can be traded as a quantum algorithm and because of that all those that like a gate operation can be realized by the quantum gate the real issue in the quantum computation well in order to say that that those can be feed to the fundamental combination we are looking for that fast speed so if you can for the certain things even because you can convert it that way if it is no faster speed no speed up so we dont lock it that way so we are looking for the fast way you you can you for the certain problem you do the quantum ways can compute much faster so you dont know which uh give your argument you cannot tell me which ones faster uh in in well the quantum computer can never be slower than the the classical computer because of automation all those classical uh algorithms can be traded to that uh quantum algorithm they cannot be slower but that whether they can be faster or not its its is it obvious its more energy efficient uh well thats no yeah like the cost and uh like the cost the energy efficiency thats not effective the couple of slides back the the point two percent um measure statistics that you had would you call that good or good or bad well uh the one is i the one is remember the goal of that one is trying to have some problems uh theres no private practical purpose that can be used in practice it doesnt create a problem to demonstrate these concepts yeah thats my question so if we restrict our attention to quantum supremacy experiments and hopefully in the next few years there are a couple more coming out what do you think are the outstanding statistical questions that needs to be answered that havent been answered so far in those papers i feel that even for the quantum supremacy uh project like for example there is a lot of like that yeah if you trying to lets say using that fda like that in the pharmaceutical company we develop a new drug way theres a lot of statistics you go to the fda you need to check all those analyses if you use that one of the golden standard okay uh probably many of those analyses wont be able to pass that thatll stand up okay so theres a ton of issue i think probably that like for example in these problems like in the google study problems and like theres a noise model for uh the noise model there right and the quantum computer to generate the data from that noise models and like i and we and the others they try to look at it does the data really feed it to that the model the physical model there does not statistically test basically reject uh the hypotheses does not fit into the model but we know now uh because theres so many complicated issues there and we we couldnt find out any suitable model defeating the data but on the other hand theres so many uh automation that you should say yeah that might be used to guess what the life form of the noise model of data beyond the global depolarized error model yeah its not a i dont have a solution to that one thats why we need more statisticians to work in this field and hopefully that resolves many so i see there are no more questions so lets start together again for this 