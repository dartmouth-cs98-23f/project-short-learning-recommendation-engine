but we are surrounded by cpus or processors and the computing they do for us they touch every aspect of our lives cpus are in your laptop in the machines you use to check out at the grocery store in the electronics that power the instruments in your car more efficiently they enable our artists and scientists to create things that were unimaginable only yesterday cpus are everywhere and shape just about everything we do welcome to architecture all access cpu architecture part two hi my name is boyd phelps and over the last 23 years ive had the privilege of working on some of the most well known chip designs in intels history from the architectural definition and design of the pentium 4 to designing the nehalem westmere haswell broadwell and tiger lake processors and many more today i help lead the development of intels client engineering teams where i oversee the development of current and future products in part one of the cpu architecture series we talked about what cpus are the history of the cpu the concept of computing abstraction layers as well as the instruction set architecture the instruction set architecture is what we normally refer to as the architecture of a cpu today we want to delve into what the microarchitecture of a cpu looks like or in other words once an isa is defined how you might go about implementing that isa into a microarchitecture and what the main building blocks of the functions of that microarchitecture would be so lets dive in a microarchitecture is an implementation or specific design of an instruction set architecture the first step is to fetch or retrieve the instructions from memory so that the cpu knows what the program wants executed this is called the fetch stage the next step is to decode the fetched instructions into native operations this is called the decode stage sometimes this means taking the instructions and breaking them down into multiple internal operations once the instructions have been decoded the cpu needs to execute them there are many different types of operations the cpu can perform math such as add subtract multiply divide or perform boolean operations such as and or xor not the cpu also compares data makes decisions about where to go next in the code we call these decision instructions branches as they can steer code to different places there are many other operations cpus perform depending on the isa and what we call the execute stage finally the cpu will store those results sometimes those are saved locally in a register and sometimes theyre stored in memory this is called the write back stage these operations make up the basic building blocks of every modern cpu when put together they are referred to as the cpus pipeline now that we have described a basic pipeline what do modern microprocessors look like over time the average number of pipeline stages has grown a pipeline is very similar to an assembly line the more stages are added the less is done in each individual stage now when henry ford wanted to drive down the cost of the model t he built an assembly line with many stages this made each stage simpler allowing workers to specialize in one specific task this in turn allowed workers to do their tasks at the same time in a pipeline fashion the result was an automotive revolution that put cars into the hands of more people at a lower price cpus are very similar in general the more pipeline stages you have the faster each stage can run and the more stages are being done in parallel a modern microprocessor has around 15 to 20 stages the fetch and decode stages typically have six to 10 stages collectively these are called the front end of the microprocessor execute and write back have also grown into roughly six to 10 stages these are called the back end of the microprocessor a cpus pipeline is synchronous and what we mean by that is each pipeline is controlled by a clock and each data goes from one pipeline stage to the next as a cpu clock completes a cycle the number of stages partially determines what the peak frequency of a cpu is now modern day cpus can run over five gigahertz the amount of logic in each of these stages determines how fast the stages or clock can operate if a cpu runs at five gigahertz this means that the stages each need to complete in fivebillionths of a second remember a hertz is one cycle per second now henry ford would be pretty impressed with the assembly line speed of todays modern cpus if you recall our basic pipeline at the beginning we fetch an instruction and towards the end we execute the instructions some of these instructions are called branches these represent a decision point or fork in the road like an exit on a highway do we want to keep going or do we need to exit now to take a different path when you execute a branch you are making that decision as the pipeline depth grows you get farther and farther away from the answer of which path to take when the branch says to take a different path we need to tell the beginning of the pipeline to redirect to a different instruction the work that was in progress needs to be thrown away this is both bad for performance as well as for power since we have been spending time executing instructions that were not needed for the programs execution we could avoid speculation simply by stopping every time we saw a branch and just wait for it to execute and tell us the correct direction in the code to go this would be safe it would also be slow however there are a lot of branches in most code and that means a lot of time spent waiting so we guess where to go next and when we see a branch and if were wrong well simply throw away all the work after the branch once we know we were wrong if we were right we celebrate and keep going as pipelines get longer the penalty for guessing wrong gets worse fetch becomes further away from execute which means it takes us more stages to realize that we are executing on the wrong path to solve this microprocessors invest heavily in design to make accurate predictions or guesses at the beginning of the pipeline we call this the art of branch prediction when we see that we have gone down the wrong path we can update or refine the prediction with what the right path was then the next time we see that address the branch predictor can tell us to go to a different address modern cpu architectures can often predict branches with a near perfect accuracy that makes them seem almost clairvoyant when a microprocessor executes newer instructions than a branch without knowing if that branch is taken or not it is referred to as speculative execution this is a fundamental component in modern microprocessors for achieving great performance ok so lets look at some simple code to explain how speculation works here we have a very small simple c program all were doing is counting to four hundred and the way we do this is we have two loops we have an outer loop that loops one hundred times and we have an inner loop that loops four times and every time we get into the inner loop we increment this c variable and so since we loop 100 times on the outer times four on the inner we increment 400 times so here we have the instructions run by the cpu this is the native language of the cpu where weve taken that c program and weve compiled it down into the language that it understands and well just highlight each of these instructions and tell you kind of what they do at the top you have a mov instruction for example that moves 0 into this i variable remember if you look at the c program this i variable is the counter for our outer loop we have a cmp instruction this is a compare instruction it compares that i variable to 99 and right after that we have jg a jump greater than instruction so if that is true if the compare of i is greater than 99 then were going to jump or were going to branch to label2 if thats not true then we ignore and we just move down and look at the mov instruction again we see for the inner loop another compare instruction were comparing j the counter for the inner loop to 3 and we have a branch instruction right after that compare that says we jump if j is greater than 3 and when we do that we increment the outer loop by 1 thats that add i 1 instruction and then we jump back up to the top of label5 now on the inside of the inner loop where we move c into eax we add 1 to eax we move eax into c and then we increment that outer variable so the c to eax 1 to a eax and eax to c those are the move instructions where were moving from memory to an internal register but you see the add instruction there were adding 1 to eax theres where were actually incrementing that inner variable of c when we get to these jump greater than instructions this is the first time its seen its branch and so we can either stop the execution we can allow the compare in the branch to actually go through and we can wait until they execute they resolve and then we know where to go to next or we can actually make a prediction and keep fetching now lets assume that we had decided to start our prediction by assuming all branches are taken meaning that theyre true and that were going to go to their label if we had done that the very first time that we go through this code when we see the jump greater than go to label2 if we assume that was true we would jump to label2 in the code we would figure out that was wrong in the execution of the pipeline and then we would flush all of the pipeline all of the work that weve done in the pipeline we would resteer the front end and then we go tell our branch prediction algorithm hey you predicted wrong that branch was actually not taken and then we would update our algorithm and then the next time we came through we would see that branch and it would say hey jump greater than and we would say nope not taken  thats our prediction and we would move fast and we would fetch the mov instruction and we would come down and execute the inner loop and we would go through the same thing with the the branch the compare on the branch on the inner loop now whats interesting here on this inner loop we go through that loop four times the first three times through the loop that branch is not taken on the fourth its true because j is greater than 3 and so therefore we do want to jump out of that inner loop so our prediction algorithm it would actually learn it would actually learn that the first three times for that inner loop its not taken and on the fourth time that it is taken so in our speculation algorithm we may stumble through this the first time but we execute this loop a hundred times by the second iteration of this loop weve learned how this branch behaves so again this is a simple code example from c that we compiled into instructions that the machine can understand so you can kind of highlight and see how these branches control the flow of the code and how it is that the machine has to make different decisions at different points of time in the code think of speculation and speculative execution as when i visit my aunt in the countryside as im driving on a country road and i get to a fork where i need to decide if i turn left or right and i really havent been there before and i can call my aunt but i know she normally takes up a while to pick up and even when she does shell get talking about a few topics before i can ask her the question should i go left or should i go right so i look around and i see some houses on the right at a distance so i decide to go in that direction and on the way there i call my aunt and sure enough it takes me a few minutes before i can get my question in and it looks like i choose correctly so by the time she confirms i went the right direction im almost by her house and obviously if i had guessed wrong i would have to go back to the fork and go in the other direction this is what speculation on a cpu is like the cpu makes a prediction on what direction the branch might take and begins executing instructions based on that direction and as i said modern cpus can often predict branches with a near perfect accuracy that makes them seem almost clairvoyant now that we have a good understanding of pipelines and speculation lets talk about what functions go into the first half of the front end of a microprocessor now branch predictors have become incredibly complex in order to improve their accuracy while still being able to steer fetching of instructions at a high frequency branch predictors today can oftentimes record and understand and learn the past history of hundreds sometimes even more than that thousands of branches before them in order to make a single prediction of the next branch and where it is going the sophistication of modern day branch predictors is really kind of a precursor if one might think in terms of artificial intelligence in terms of learning from past behavior how the future will behave theyve become so accurate that they are now in charge of deciding which address to fetch next even if the prediction ends up being keep calm and carry on now cpu frequencies have increased much faster than memory speeds this means it takes longer to fetch data from memory and to help offset the long round trip time to main memory and back we keep local copies of main memory internally in structures called caches the front end has an instruction cache so that it can read instructions in just one to two cycles instead of the hundreds it may take to go to main memory to optimize both power and performance many adjacent instructions are fetched at the same time which are then handed off to the decoders if the instruction cache does not have the data then the data is requested from the memory subsystem and well talk more about this later the main goal of the front end of a cpu pipeline is to ensure that there are always enough instructions available for the back end to execute and to avoid the idle time spent waiting for instruction bytes from memory or time spent fetching instructions that will end up being thrown away due to a bad branch prediction the second half of the front end is where the programs instructions are decoded into the microarchitectures internal operations which are called microoperations or uops for short this is the strongest connection between the instruction set architecture and the microarchitecture as we explained in part one of the series in general instructions consist of an opcode the operation to be performed like add and a number of operands the data to be operated on like add a in register x to b in memory location y isa instructions often also include additional bits of data that give the cpu more information relevant to the operation which the cpu uses to decode and execute the instruction in an efficient way according to its microarchitecture microarchitectures are typically built so that most instructions map directly into a single uop but not all this helps us to simplify the back end of the pipeline however there are often some instructions which are more complex and may need to generate multiple uops for instruction instructions like these helped reduce the number of instructions required for the program which in turn makes the program code smaller and easier to store in memory we simply expand them into the uops needed inside the cpu conversely decoders can also fuse multiple adjacent instructions into a single uop this fusion can allow a micro architecture to do more work at the same time leading to improved performance and efficiency for example most branches are usually preceded by a compare instruction just before the actual jump is executed whenever we see these two instructions together we can simply fuse them together into one instruction for the back end of the machine for more efficient execution since the compare and jump use separate resources so dont worry if you dont understand all of this right now but just know that the instructions and the isa can get broken down into multiple uops or combined or they get simply decoded as simple single instruction uops the front end is always looking at how to decode and prepare those instructions to be executed efficiently some microarchitectures create a decode cache and save these uops for the next time they need to be decoded this can save the energy required to decode them and improve performance when onetomany expansion is common after the instructions are decoded or read from the decode cache they are then passed to the back end of the pipeline before we dive into how the back end of a microprocessor is built lets take a moment to understand two important topics the first is superscalar execution the simplest form of execution is an arithmetic logic unit or alu for short a basic alu can perform operations like adds and subtracts if we have a single alu this means we can do one add at a time which is referred to as scalar execution modern microprocessors now implement many alus and when those alus can operate in parallel this is called superscalar execution the number of operations that can be executed in parallel is one way to measure what we call the width of a microprocessor all modern microprocessors are superscalar this increases the demand on the front end which is why its important to design a front end that can feed instructions quickly to the back end now depending on the target usage designers can always decide to increase the superscalar ability of the soc by also adding more cpu cores in addition to more functional execution blocks within those cores in a basic microarchitecture the uops are executed in program order which is called in order execution however in order to provide the best performance and energy efficiency executing them out of order is actually better allowing code to execute faster as we remove unnecessary wait times imagine youre running a restaurant you have tables full of customers and you have many cooks in the kitchen and you want all your customers to be happy you could let the first table seated order and eat first and then move to the second table but you would likely go out of business quickly as everyone watched the first table eat instead you hire a wait staff who take each order from each table as the table is ready to order these are passed off to the kitchen where the order of the dishes are planned appetizer main course dessert and theyre handed out to the different cooks the food is brought out to the tables as it is ready allowing your wait staff to execute in parallel this is out of order execution you take the order from the front end you look at the individual dishes the uops and you decide how and when to make and serve the food or execute those instructions you have happy customers and a fast efficient microprocessor this is what it would look like with actual cpu instructions the first step in an out of order back end is to take the uops provided by the front end and determine their dependencies a uop is said to be dependent on an older uop when it needs the result of that uop in order to do its own operation this dependency is tracked by a process called register renaming this is the first step in out of order execution this step takes up to somewhere between two and four stages in a modern microprocessor the uop information is also written in a structure called a reorder buffer or a rob even though we execute instructions out of order we still need to have a way to put them back in order the reorder buffer gives us the ability to understand the original program order this process is called allocation and is another way in which the width of a microarchitecture can be determined this is the most common definition since there are alus in an out of order microprocessor we must determine which alu can execute a uop there could be multiple instances of the same alu and there can also be different types of alus that can perform different types of operations some can do integer operations or whole number math while others may do floating point operations or decimal or fractional based math some alus can operate on multiple elements at the same time referred to as vectors branch operations may also have their own execution unit the uops must be assigned to an alu that can execute their particular operation they are then sent to the uop scheduler for delivery to the required alu a scheduler is a place where uops wait until their dependent operations have executed and once these dependencies have been resolved and theyre cleared the uops are then free to go and could be sent to an available alu and execute when a uop has executed and becomes the oldest uop in the back end the rob decides that the uop can be written back safely this process is called retirement let me illustrate that with an example here we see six instructions that are being fed to the back end of our cpu the first three are integer operations the others are some vector operation some divides and multiplies you can see that the second instruction depends on the result of the first one since both of these use the same operand g1 so we can schedule the first instruction have the second operation wait in the scheduler but since the third operation isnt dependent on the first two we can assign that to an alu already and let it go similarly the fourth instruction can be sent to a vector alu right away the fifth instruction will need to wait for the fourth one to execute but the sixth one is not dependent on anything so it also can be sent right away and so you would see something like this where the four instructions that are not dependent on anything can be sent all at once to different integer and vector alus while on the next cycle we can have the next set of instructions execute we were able to execute many of these instructions in parallel and out of order thereby increasing the performance of the cpu while still making sure that the dependencies were kept so we get the correct answer and the correct results at the end obviously this is a simple example with six instructions but you can probably imagine how useful it is to have these out of order and superscalar capabilities in real code that runs millions of instructions operating at billions of cycles per second looking at thousands of instructions so lets recap what we discussed in the second part of our cpu architecture module we covered some of the basic building blocks of cpu design such as the von neumann cycle of fetch decode execute and write back how a pipelines depth is determined by the number of stages those basic functions are broken into and how speculation is used to increase cpu performance trying to always guess where code is going in order to avoid wait times we then dove into the cpu front end where the cpu predicts what instructions are needed next fetches them and decodes them into microoperations or uops from the front end we move to the back end which takes the decoded uops and uses super scalar execution and out of order execution to efficiently assign uops to execution units i want to thank you for your time and i hope weve not only been able to give you an appreciation for both the science but also the art of cpu design its truly a creative endeavor weve come a long way since the days of the eniac and vacuum tubes but perhaps what excites me the most is the rate at which innovation continues to progress in both cpu and systemonchip design engineers across the various disciplines of architecture design process technology memory packaging and software development are collaborating in new ways that will drive performance and solutions to problems of the future that we can barely imagine today if the last few decades have been a marvel i think its fair to say to you you havent seen anything yet this has been architecture all access cpu architecture part two 