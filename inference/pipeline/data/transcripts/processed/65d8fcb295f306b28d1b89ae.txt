pbs digital studios hey i’m jabril and welcome to crash course ai language is one of the most impressive things humans do it’s how i’m transferring knowledge from my brain to yours right this second languages come in many shapes and sizes they can be spoken or written and are made up of different components like sentences words and characters that vary across cultures for instance english has 26 letters and chinese has tensofthousands of characters so far a lot of the problems we’ve been solving with ai and machine learning technologies have involved processing images but the most common way that most of us interact with computers is through language we type questions into search engines we talk to our smartphones to set alarms and sometimes we even get a little help with our spanish homework from google translate so today we’re going to explore the field of natural language processing intro natural language processing or nlp mainly explores two big ideas first there’s natural language understanding or how we get meaning out of combinations of letters these are ai that filter your spam emails figure out if that amazon search for “apple” was grocery or computer shopping or instruct your selfdriving car how to get to a friend’s house and second there’s natural language generation or how to generate language from knowledge these are ai that perform translations summarize documents or chat with you the key to both problems is understanding the meaning of a word which is tricky because words have no meaning on their own we assign meaning to symbols to make things even harder in many cases language can be ambiguous and the meaning of a word depends on the context it’s used in if i tell you to meet me at the bank without any context i could mean the river bank or the place where i’m grabbing some cash if i say “this fridge is great” that’s a totally different meaning from “this fridge was great it lasted a whole week before breaking” so how did we learn to attach meaning to sounds how do we know great enthusiastic means something different from great sarcastic well even though there’s nothing inherent in the word “cat” that tells us it’s soft purrs and chases mice… when we were kids someone probably told us “this is a cat” or a gato māo billee qut when we’re solving a natural language processing problem whether it’s natural language understanding or natural language generation we have to think about how our ai is going to learn the meaning of words and understand our potential mistakes sometimes we can compare words by looking at the letters they share this works well if a word has morphology take the root word “swim” for examplewe can modify it with rules so if someone’s doing it right now they’re swimming or the person doing the action is the swimmer drinking drinker thinking thinker … you get the idea but we can’t use morphology for all words like how knowing that a van is a vehicle doesn’t let us know that a vandal smashed in a car window many words that are really similar like cat and car are completely unrelated and on the other hand cat and felidae the word for the scientific family of cats mean very similar things and only share one letter one common way to guess that words have similar meaning is using distributional semantics or seeing which words appear in the same sentences a lot this is one of many cases where nlp relies on insights from the field of linguistics as the linguist john firth once said “you shall know a word by the company it keeps” but to make computers understand distributional semantics we have to express the concept in math one simple technique is to use count vectors a count vector is the number of times a word appears in the same article or sentence as other common words if two words show up in the same sentence they probably have pretty similar meanings so let’s say we asked an algorithm to compare three words car cat and felidae using count vectors to guess which ones have similar meaning we could download the beginning of the wikipedia pages for each word to see which other words show up here’s what we got and a lot of the top words are all the same the and of in these are all function words or stop words which help define the structure of language and help convey precise meaning like how “an apple” means any apple but “the apple” specifies one in particular but because they change the meaning of another word they don’t have much meaning by themselves so we’ll remove them for now and simplify plurals and conjugations let’s try it again based on this it looks like cat and felidae mean almost the same thing because they both show up with lots of the same words in their wikipedia articles and neither of them mean the same thing as car but this is also a really simplified example one of the problems with count vectors is that we have to store a lot of data to compare a bunch of words using counts like this we’d need a massive list of every word we’ve ever seen in the same sentence and that’s unmanageable so we’d like to learn a representation for words that captures all the same relationships and similarities as count vectors but is much more compact in the unsupervised learning episode we talked about how to compare images by building representations of those images we needed a model that could build internal representations and that could generate predictions and we can do the same thing for words this is called an encoderdecoder model the encoder tells us what we should think and remember about what we just read and the decoder uses that thought to decide what we want to say or do we’re going to start with a simple version of this framework let’s create a little game of fill in the blank to see what basic pieces we need to train an unsupervised learning model this is a simple task called language modeling if i have the sentence i’m kinda hungry i think i’d like some chocolate   what are the most likely words that can go in that spot and how might we train a model to encode the sentence and decode a guess for the blank in this example i can guess the answer might be “cake” or “milk” but probably not something like “potatoes” because i’ve never heard of “chocolate potatoes” so they probably don’t exist definitely don’t exist that should not be a thing the group of words that can fill in that blank is an unsupervised cluster that an ai could use so for this sentence our encoder might only need to focus on the word chocolate so the decoder has a cluster of “chocolate food words” to pull from to fill in the blank now let’s try a harder example dianna a friend of mine from san diego who really loves physics is having a birthday party next week so i want to find a present for  when i read this sentence my brain identifies and remembers two things first that we’re talking about dianna from 27 words ago and second that my friend dianna uses the pronoun “her” that means we want our encoder to build a representation that captures all these pieces of information from the sentence so the decoder can choose the right word for the blank and if we keep the sentence going dianna a friend of mine from san diego who really loves physics is having a birthday party next week so i want to find a present for her that has to do with   now i can remember that dianna likes physics from earlier in the sentence so we’d like our encoder to remember that too so that the decoder can use that information to guess the answer so we can see how the representation the model builds really has to remember key details of what we’ve said or heard and there’s a limit to how much a model can remember professor ray mooney has famously said that we’ll “never fit the whole meaning of a sentence into a single vector” and we still don’t know if we can professor mooney may be right but that doesn’t mean we can’t make something useful so so far we’ve been using words but computers don’t work words quite like this so let’s step away from our high level view of language modeling and try to predict the next word in a sentence anyway with a neural network to do this our data will be lots of sentences we collect from things like someone speaking or text from books then for each word in every sentence we’ll play a game of fillintheblank we’ll train a model to encode up to that blank and then predict the word that should go there and since we have the whole sentence we know the correct answer first we need to define the encoder we need a model that can read in the input which in this case is a sentence to do this we’ll use a type of neural network called a recurrent neural network or rnn rnns have a loop in them that lets them reuse a single hidden layer which gets updated as the model reads one word at a time slowly the model builds up an understanding of the whole sentence including which words came first or last which words are modifying other words and a whole bunch of other grammatical properties that are linked to meaning now we can’t just directly put words inside a network but we also don’t have features we can easily measure and give the model either unlike images we can’t even measure pixel values so we’re going to ask the model to learn the right representation for a word on its own this is where the unsupervised learning comes in to do this we’ll start off by assigning each word a random representation  in this case a random list of numbers called a vector next our encoder will take in each of those representations and combine them into a single shared representation for the whole sentence at this point our representation might be gibberish but in order to train the rnn we need it to make predictions for this particular problem we’ll consider a very simple decoder a single layer network that takes in the sentence representation vector and then outputs a score for every possible word in our vocabulary we can then interpret the highest scored word as our model’s prediction then we can use backpropagation to train the rnn like we’ve done before with neural networks in crash course ai so by training the model on which word to predict next the model learn weights for the encoder rnn and the decoder prediction layer plus the model changes those random representations we gave every word at the beginning specifically if two words mean something similar the model makes their vectors more similar using the vectors to help make a plot we can actually visualize word representations for example earlier we talked about chocolate and physics so let’s look at some word representations that researchers at google trained near “chocolate” we have lots of foods like cocoa and candy by comparison words with similar representations to “physics” are newton and universe this whole process has used unsupervised learning and it’s given us a basic way to learn some pretty interesting linguistic representations and word clusters but taking in part of a sentence and predicting the next word is just the tip of the iceberg for nlp if our model took in english and produced spanish we’d have a translation system or our model could read questions and produce answers like siri or alexa try to do or our model could convert instructions into actions to control a household robot … hey john green bot just kidding you’re your own robot nobody controls you but the representations of words that our model learns for one kind of task might not work for others like for example if we trained johngreenbot based on reading a bunch of cooking recipes he might learn that roses are made of icing and placed on cakes but he won’t learn that cake roses are different from real roses that have thorns and make a pretty bouquet acquiring encoding and using written or spoken knowledge to help people is a huge and exciting task because we use language for so many things every time you type or talk to a computer phone or other gadget nlp is there now that we understand the basics next week we’ll dive in and build a language model together in our second lab see you then thank you to curiositystream for supporting pbs digital studios curiositystream is a subscription streaming service that offers documentaries and non¬fiction titles from a variety of filmmakers including curiositystream originals for example you can stream dream the future in which host sigourney weaver asks the question “what will the future look like” as she examines how new discoveries and research will impact our everyday lives in the year 2050 you can learn more at curiositystreamcomcrashcourse or click the link in the description crash course ai is produced in association with pbs digital studios if you want to help keep crash course free for everyone forever you can join our community on patreon and if you want to learn more about how human brains process language check out this episode of crash course psychology 