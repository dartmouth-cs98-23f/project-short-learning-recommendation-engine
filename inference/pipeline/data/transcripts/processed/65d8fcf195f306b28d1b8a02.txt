this video today were going to learn how to do parallel computing using cuda which is nvidias parallel computing platform and toolkit so let us get right into it music alright so cuda is as i already mentioned nvidias parallel computing platform and it is super important in machine learning because in machine learning a lot of the tasks are actually just linear algebra matrix multiplications matrix operations vector operations things like that and cuda is optimized for exactly that and in this video today i want to show you how it works i want to give you a very simple uh introduction example of a parallelized hello world which is not very useful but you can see how it basically works and then were going to implement something more advanced which is a matrix vector multiplication first in core c and then in cuda to see the performance difference there now to get started what you need is you need to have cuda and the cuda toolkit installed on your system the easiest way to do that is to just follow the instructions here in the documentation so you have instructions for linux you have instructions for windows and probably also for mac you will hopefully find all three of them in the description down below so you can just follow the installation here i think though on linux its quite simple at least if you have the basic nvidia drivers installed and of course you need to have an nvidia gpu for that you cannot do this with an amd gpu but i think if i just run this command here i should be able to see where im running the compiler from yeah nvidia cuda toolkit so this is what you have to install but if you want to make sure everything works correctly just follow the installation instructions here now i need to mention that this video is going to be quite advanced in terms of it requires you to already have some c knowledge it requires you to already no basic programming concept so this is not an introduction video for someone who is never coded in c never coded in c plus plus or anything like that if you only know python you can still watch this to get some inspiration to see how it works but dont expect to to understand everything here because im going to just use concepts or apply concepts like malloc so allocation of memory freeing memory and stuff like that without explaining how it works and why it needs to be done so once you have everything set up were going to go into the directory that were going to be working in today so in my case its this one it says python even though its not going to be python today uh and were going to start with a basic hello world example so cuda is uh basically just c or c plus plus code but its actually cuda code so its uh very similar the language is almost the same but you have some additional functions and data types and stuff like that and were going to start now by creating a file called hello underscore cuda dot c but were not going to create a c file were going to create a cu file a cu file is essentially a cuda file instead of a just core c file and here now were going to start by writing an ordinary hello world program in c but then were going to change it so that it can be parallelized with cuda so were going to start by including include and then were going to include stdio dot h and then were going to define a function hello world and this function will just printf hello world or actually wanted to say hello cuda so lets call this hello cuda backslash n for line break and that is basically the function then we have our main function here which takes some parameters maybe rxc and then also arc b and then we have the return zero at the end and then we call our function hello cuda and that is basically our c program now what we can do now in cuda or maybe let me show you that this actually works and see for those of you who are not so familiar with c uh move this to this and now i can just compile it and the ordinary way to compile a c uh program is to say gcc or whatever compiler you want to use hello cuda and then hello cuda is the output and then i can just run helen cuda so this is an ordinary c program now now lets move it back to a cuda file lets go into it and change the code so that it can be parallelized um and what were going to do here is were going to make this void function here were going to make it a global function so global underscore underscore global underscore underscore void hello cuda and then when we call this function were going to actually specify three angle brackets opening and closing and in here were going to specify the grid shape and the block shape the basic idea of cuda is that you have multiple blocks that are aligned in a grid and then you have in those blocks multiple threads now maybe i can give you again a not so good visualization here im only going to use my mouse so this is not going to look uh too great but you have basically a grid in your gpu you can say uh and in this grid you have different blocks so we can maybe split this up if this is now two by two so we have this two by two grid we have block zero zero we have block uh zero one we have block one zero and we have block one one now those blocks themselves contain threats so these different blocks are again split up here um maybe we should use a different color for this they have a structure themselves so maybe the block itself is also a two by two block every single block is a two by two block and in those blocks we have now the threat 0 0 and then zero one and one zero and one one and we have the same everywhere so zero zero zero one one zero one one in all these blocks we have threats and they can have whatever shape you want now im going to explain why this is useful later on for now its not really useful for now its just something that we need to know and im going to now specify that i want to have uh one block and one thread in this block so this would be one one um and what we can do now is we can compile this but we can compile this with uh the nvidia compiler so with the cuda compiler nvcc hello cuda dot cu and then we can say output hello cuda and now when i run this we will see everything is oh actually i forgot something important to actually see the output we need to call a function which is cuda device synchronize to actually get the output here as well because otherwise everything is happening on the device so we compile again we run again and we can see hello cuda here so thats quite simple now i can also say i want to have two blocks with one thread each this would also work but then you can see what happens is i run this two times and i can then change this again and i can say i want to have basically i can say either two blocks in one thread i can also say one block with two threads the result would be the same in this case i can also say i want to have two blocks and two threads so in this case i would get four times hello cuda thats the basic idea of how this works now why is this important why is this useful we can access inside of this function here certain values and im going to print them here for you so that you can see how this works we can have a block index x which is going to be a number we can have a block index y which is also going to be a number we can have a threat index x which is also going to be a number and we can have a threat index y obviously which is again also going to be a number and lets add a backslash n here we can just get these values by accessing block idx dot x lock idx dot y threat idx dot x and threat idx dot y and those are important because basically what we can do now is we can compile we can run and due to the shape that i provided we only have x values because i said i want to have two blocks and i didnt say i want to have uh two times two blocks with two times two threads i just have two blocks two threads so were only using the xaxis but you can see that every time uh the gpu runs the code it runs it on a certain block and on a certain thread so we have block zero thread one block zero thread zero and so on and so forth which means that if i can say certain blocks of the gpu are responsible for certain blocks of a matrix calculation this is very compatible because a matrix has a certain uh shape a certain grit shape and the gpu has a certain grid shape and then i can just parallelize the operations in this grit shape sort of way so this is what were going to do now with a matrix vector multiplication and were going to start by implementing this matrix vector multiplication in core c so to can follow the thought process if youre not familiar with cuda and then were going to do the same thing in cuda so that we can see what changes and we can see also why it is faster so im going to start here in new file which is going to be matrix underscore vector dot c and this file here what were going to do is were going to import so were going to include music um stdioh by the way excuse my slow typing im still getting used to my new keyboard i change the layout so im typing a little bit slower than usual but we import these two uh libraries here and then we say void matrix vector uh product lets say and what we want to get here as an input is we want to have first of all a flow a float pointer to a matrix a want to have a float pointer to a matrix v to a vector v1 and to a vector v2 and want to have the matrix size which is going to be an integer the basic idea is that were going to have an n by n matrix an nsized vector were going to multiply the two and were going to store the result in an n sized vector vector 2 again im not going to explain basic uh matrix vector multiplication im not going to explain what a pointer is i expect you to either know that or to not care about it if you watch this video um so were going to define this function now and were going to do it in a very simple way were going to have a loop and i from 0 i being less than the matrix size and then i plus plus so were basically just going through the rows so were going through row 0 1 2 3 and so on and in there what we want to do is we want to have we want to keep track of a sum which starts at zero and then were going to go through the columns int j equals zero j less than matrix size we can do this again here because um because we have an n by n matrix so the matrix size is going to be the size of the row or the the amount of rows and the amount of columns and then j plus plus and all we want to do now basically is we want to say sum plus equals and we want to add whatever the value or we want to multiply whatever the value is at the current position so we take i the current row but we take it times the matrix size because we want to go through the um so basically if youre the second row you want to not just go with two you want to go with um because we dont have a multidimensional array here we have a long array we have the flattened version of the matrix basically and because of that we dont want to just go to row two we want to go to row 2 but this means going uh two times the matrix size plus whatever position were at column wise so i times matrix size plus j so basically maybe i can sketch this for those of you who cannot really follow we basically have music um why can i not draw now let me try again okay now it works basically what we have is we have um we have a matrix lets say we have a three by three matrix now and what i have in c is not the three by three matrix i have a nine an array of size nine basically so we have nine positions here whatever and what i want to do is i want to go to i being 0 i being 1 i being 2 but i being 2 basically means going 6 and then also whatever i need so i have to go six positions to basically pass the first two rows thats the reason why we do that so i times matrix size plus j how many uh which column in that row we take this value and we multiply it by vector one and then just uh the position there and then in the end what we do is we say v2 at respective row is going to be equal to the sum that is the result for this particular row this is just a basic matrix multiplication matrix vector multiplication um then we are going to have our main function which is going to take an int arc c character part b actually we are not using that but im just gonna put it there um and then were going to return zero in the end and what we want to do now is we want to just define everything so we want to set everything up we want to have a float pointer a a float pointer uh v1 and v2 then we want to define a matrix size now were going to start with a matrix size of three just so we can see that the calculation is accurate and then were going to go with 40 000 which is a good value i figured uh to show the performance difference between this version and the cuda version so matrix size is going to be three were going to say a the matrix is going to be typecasted here to a float pointer were going to allocate so were going to use malloc to allocate the matrix size times the matrix size because we have an n by n matrix so matrix size times matrix size times the size of whatever a flow is on the system um so we can copy this we can do the same thing with v1 v2 but this time of course uh were going to just use one matrix size because the vector size is going to be the same size as the as the matrix but its not going to be multidimensional so if we have a 10 by 10 matrix were going to have a 10 sized vector thats the basic idea so matrix size times float size and then uh were just going to initialize our uh matrix in our vector with some basic values so were going to say 4 and i equals zero i being less than matrix size and then i plus plus inside of this were going to run a second loop maybe i can just copy this one here but its going to have a j and basically the idea is let me open my paint again uh what we want to do here is we want to if we have the matrix like this i mean just you know lets do it like this 33 its not the most beautiful matrix one a half to zero one two three four and so on whatever the size is and for the vector that were going to use we want to have also zero one two three and so on thats the basic idea of what we want to do here for the initialization you can also just use random initialization but i didnt want to deal with the whole randomness and see now because i have to come up with uh some some seed that is random and stuff like that but were going to just do it like that now were going to say uh matrix a i times matrix size again so we just do the same thing that i explained above so matrix size plus j and the value of this is going to be exactly loads and then uh basically whats inside of the bracket so i times matrix size plus jake so the position is going to be also the value um and then for the vector were going to do a similar thing im going to copy this here im going to paste it here um were gonna just say vector one position i is going to be equal to float type casting of i and that is the same idea then we perform the matrix vector product for this we pass again a b1 v2 matrix size then as a result of that in v2 we will have the result of the multiplication stored and then we can say four int i equals zero i being less than matrix size i plus plus we can just print the result vector so percent point to f backslash n and we want to have uh v2 i and finally we free the resources so free a free v1 maybe two and that should actually be it so gcc matrix vector dash o matrix vector lets see if we get the proper values yes those are the correct results so now we can also go ahead and change the matrix size to 40 000 and i can compile this i can run this but this will take some time i think around eight seconds something like that i hope this doesnt mess up the recording but it shouldnt actually but it takes some time as you can see to do the calculation then we get the output and this is now what were going to also implement in cuda and youre going to see why cuda is so powerful for tasks like that so im going to start here with a new file matrix vectorcu here were going to now just include stdioh were going to define again a global void which is going to be a matrix vector products it takes again a float pointer to a matrix a a float pointer to a vector b1 load pointer to a vector v2 and a matrix size integer and now what we want to do again remember we have the block index we have the threat index we want the individual threads to be responsible for parts of the calculation so that all of them can work at the same time in the gpu and deliver the result faster and for this were going to say okay the row that im currently working at the row that this particular function call is going to be working at will be determined by the block index x times the block dimension of x im going to explain here in a second why we do this plus thread index x and this is now exactly the same thing that we had before remember we had the same when i explained to you why we why we have the flattened array and why we have the matrix and why we go through i times matrix size this is exactly the same thing so we have i times matrix size here so basically a column size plus j this is what we had multiple times in the last code and this is the same pattern now we take the row the block index x which is the row uh or yeah and then we have the block dimension i mean actually were flipping this i think its actually the column it doesnt matter it works anyway because its squared uh but we have basically the block index x the position times how large is that uh dimension this x dimension so in this case the matrix size as you can see and then we have the thread index in that row or column whatever you want to think about here and this is the j that we have before so its the same pattern uh its not different and were going to do the same thing now with the column uh i think it should actually also work when you reverse it i think so but were going to keep it like that because i dont want to mess up my prepared code here since its squared it doesnt actually matter but we have row and column now and we determine by the index of the block that were currently currently at and by the threat index we determine what part of the calculation were going to focus on and to limit this were not going to actually use all of the threads were going to just say if the column that were working at is zero and the row that were working at is less than the matrix size because of course we can also go uh we can also have if we have for example a block size of 10 but we only have a matrix size of eight we can go beyond the boundary and in this case of course we dont want to go beyond the matrix to do calculations so if that is the case were going to start with a float sum equal to zero again and were going to iterate for this particular column that were in were going to iterate through the rows so i equals zero i being less than the matrix size i plus plus and then we want to say sum plus equal so the code is quite similar again we have a rho times matrix size plus i and then i think my naming is kind of confusing im not sure if i shouldnt reverse it im gonna im gonna keep it like that but maybe were going to change it later on v1 i and basically a calculation is the same we focus on one column or one row and we go through the fields of that column row whatever you want to focus on here and we do the multiplications there and at the end we have a sum and this sum is what we set and now here row is definitely correct because we have in the vector rows and here we want to say that this is the sum so the calculation now is basically the same the only difference is that we focus on specific we dont have two loops here we focus on a specific part of the matrix based on the block index and on the threat index so every thread and block is going to have a different responsibility which means that we dont have to do all the work in one thread we can split up the work and different threads will produce different results in this vector in the gpu thats the basic idea and now were going to take that and were going to use that in the main function now this is going to be a little bit trickier than before because there is something um that we need to do here which we didnt have to do before which is we need to communicate uh between the device and the host now the host is just my cpu my basic program running on the on the computer here and the device is the gpu the device is this gpu this parallel computing uh device and we need to transfer information back and forth all the time to get the actual um to to be able to pass values to the gpu and to get the results from the gpu to then display it on the host thats the basic idea so what we need to do is we need to define a float pointer a again but we also need to define a float pointer a underscore gpu and the same is true were going to do that now here in three separate rows v1 and v1 gpu v2 and v2 gpu so we want to have the equivalent one time on the host and one time on the device uh then were going to again define the matrix size this is something we only need once forty thousand actually lets start with three first so that we can see again if this works properly and then were going to say dimension 3 which is a a data type here and were going to define a block shape now the block shape is the shape of the block itself and basically the block shape let me again open up my favorite tool here the block shape is if we have again this is the overall shape so we have maybe two times two two times two blocks this whole thing here this two times two structure will be the grid shape thats what were going to refer as the grid shape uh in that grid shape we have multiple blocks one two three four blocks and inside of those blocks we have maybe a different shape maybe something like this uh four times four grid this here is going to be the block shape so the shape of the block itself um just so that you dont confuse it so the block shape is not the shape of the blocks how they are aligned but the shape of the blocks inside of the blocks i hope this is not too confusing um and this is going to be in our case 32 times 32 2 in other words in each block we have 32 times 32 threats thats what this basically says and then we also have uh the grit shape so how many blocks do we have and this is important we need to calculate this based on the matrix size so we have three scenarios here we have a matrix size that is perfectly compatible with the block size so we have for example a size of 32 and we have a shape of 16 so we have basically uh we need two times two to fill up the space and it it fits perfectly or we have something thats not precisely compatible so maybe we have a matrix size of 30 and a block shape of 16 in this case we would have we would have to use still two by two but we have some remainder and then we have also the case where the division would result in zero and in this case we would have to just force a one so if we have for example a matrix size of 10 and we have a block shape of 16 times 16 in this case we would have one block even though its too much so for this i came up with this calculation here maximum of 10 so to force a minimum of one um and then seal float actually like this so type casting float matrix size divided by float and then block shape dot x um yeah thats thats it so basically were were taking the matrix size dividing it by the block shape to calculate how many blocks we need we seal the number because we always want to have you know you cannot have 56 blocks in this case you would need six blocks and in the case we get a zero out of this division we go with 10 as a minimum value here that we force and we can then basically just copy this and the same thing is done with y for the yaxis all right so this is our grid shape this is our block shape now um and what we do now is we allocate the space again so this is basically the same as before so i can actually copy this from our previous script that we just wrote or that script program that we just wrote so this part is the same we allocate a the 1v2 on the host on the machine and we also fill it up with the values here so were going to copy that this is going to be the exact same thing um yeah just allocating locally and initializing again with zero one two three and so on for the matrix and for the vector now the new thing is we need to also allocate now on the device this is allocating on the host but we need to also allocate space for the gpu pointers here and for that we need to use a function called cuda malloc so cuda melloc will im going to use your avoid pointer pointer and a underscore gpu this is going to allocate the size on or the space on the gpu on the device and the size is the same matrix size times matrix size times size of float and then we can copy this for v1 v2 and we need to remove a matrix size here because those are the vectors but this is how you do that so now we have a space allocated for those variables now what we need to do is we need to take these or actually just a and v1 because this is just an empty result vector but we need to take the matrix and the vector that we want to use for the calculation and we need to copy them to the device so what we need to do after the allocation is we need to say cuda mem copy and we need to store in a gpu a so we need to transfer a to the a gpu and the size that we use for this is matrix size times matrix size times size off float again and we need to pass the keyword here cuda mem copy post to device same is done for v1 all right and thats basically it we take the local matrix and vector from the host and we transmit it to the device to the gpu then what we do is we perform the calculation on the gpu now we have all the data on the gpu we perform the calculation on the gpu by calling matrix vector product and here now remember the angle brackets we passed a grit shape and we pass the block shape and then we call it on a underscore gpu v1 gpu and v2 gpu we didnt transfer v2 but we still have v2 gpu and we still allocated it here so the result is stored there we also need to pass the matrix size and then what we need to do is since the result is now in b2 gpu to get it on our system on the host system we need to transfer it back so we do another cuda mem copy and we transfer from or into v2 from v2 gpu matrix size times size of float and here now we use cuda mem copy device to host and now all we need to do is we need to save 4 and i equals zero i being less than matrix size i plus plus we print 02 f backslash n v2 i and that is basically it what we need to do in the end of course is also free a oh now i copy it what did i do now hopefully i didnt mess up anything free a free v1 v2 and important we also need to do cuda free a gpu and then the same thing with v1 b2 and then in the end of course return zero that is basically it so let me maybe recap again we have this matrix vector product we decide what were going to focus on here um so which threat is going to do which part of the work here we allocate a agpu v1 v1 gpu v2 v2 gpu so one um variable here one pointer on the host system one on the device system then we define a matrix size we define a block shape we calculate based on that the grid shape we allocate on the host we initialize on the host we allocate on the device we transmit to the device we do the calculation we transmit the result to the host and we print it on the host and we free all the resources that we allocated now lets see if that works if we didnt mess up anything so nbcc matrix vectorcu and then matrix vector underscore cuda then point slash matrix vector underscore cuda and the results are correct so let us go ahead now and change this to uh forty thousand and lets compile this run this and there you go so now what we can do is we can time the two different uh versions we can time the matrix vector which is the c version the core c version and we can see that this takes around seven eight seconds something like that 81 seconds and if i go and say matrix vector cuda this one only takes 455 seconds so it is a massive speed up and this is a simple example now i know it might be a little bit confusing to some of you who are not familiar with c or cuda but in the context of cuda programming this is a very very simple example this is not complex but this type of um this type of parallel computing makes machine learning much faster because again as i mentioned machine learning is a lot of linear algebra matrix multiplications matrix operations and by focusing or by mapping the grid structure of the blocks and the threads to the grid structure of the matrix you can do everything in parallel and you can you have this new sort of paradigm of programming and it makes everything much faster and much better and this is now c you can also do this with c plus plus and i think theres also an interface in python maybe i will make a video about that once i get familiar with that but this is how you do cuda programming to speed up um to speed up tasks to speed up programs with parallel computing so thats it for this video today i hope you enjoyed it and hope you learned something if so let me know by hitting a like button and leaving a comment in the comment section down below and of course dont forget to subscribe to this channel and hit the notification bell to not miss a single future video for free other than that thank you much for watching see you next video and bye 