music hi everyone hi uh sorry for this little delay in our live stream uh but yeah you are here five minutes after and im to welcome you to another exciting edition of x bites brought to you uh by our general sponsor ex and as usual i am your host faai community manag manager ks and todays live stream promised to be extraordinary as usual and yeah you have the privilege to have uh with us today uh zamar jamar welcome welcome to the stage uh he is a the developer relations manager at latin nvidia and well be share more insights about that during his presentations and um hes going to speak a little bit about how to accelerate your e applications using nvidia sdk and before we dive into the technical wonders that jire has prepared for us um i want to extend our thanks to him of course uh for taking up the the challenge and contributing for the growth of our u xbes community uh this live stream and just um a quick shout out to our x gek to our sponsor x geks uh just also to um thank x to be our our sponsor x for those that that dont know is a software house of major group that is k group x6 is specialized specialized in clon native technologies and um they try to maintain a profound commitment to community building and this ucation to the community uh is the is is also demonstrated here in this live stream in this monthly dynamic but we also have for me groups uh we also have our blog spots our youtube recordings among other things things and um and if you want to to follow us and to understand a little bit more our communitydriven initiatives i will uh advise to uh just uh click here in our uh link tree to know a little bit more about us so that said and without further ado because you are not here to listen to me but to jamar um lets give the spotline and the stage to jamar please jamar the stage is yours i will quit myself and thank you again thank you so much fabio for the invitation and also tavio uh its a really honor to be here uh good evening for everyone in this case im im talking from brazil uh its still afternoon here uh the idea today is to show you what i used to call the dorsal spine of nvidia the case for uh artificial intelligence uh we have more than 150 sdks more than 500 libraries almost all of them are for free and open source and sometimes its hard to connect the dots and understand how the pieces uh match together and this is uh why ive built this presentation uh i plan to cover uh the most important areas uh we are working right now and our communities are working right now uh so lets start uh talking uh about how nvidia works usually everyone knows nvidia because of our hardware gpus the cpus and the other stuff uh in hardware we produce but uh the way we work over the past more than 15 years is that we really listen to the needs of our customers and based on the need of our customers we develop a full stock for them uh to to produce what they want im going to use merlin as one example uh its one sdk that is uh here on the list uh recommendation engines is something that was develop it and and use it over the past decades by the whole industry but every time anyone need to start a new uh a new recommendation uh project they need to put together a lot of open source software stack then build something in top of it then finally they have a basic engine and they can start working and when talking about hardware acceleration when you combine all those pieces uh its really really hard to optimize everything uh to to get the most of your hardware so what we did we talked to the key customers that use those kind of uh engines and we listen to uh their needs and then we develop this sdk that is open source named merlin this sdk is based on three big uh platforms actually this one is based on ai but we have three big software platforms one for hpc one for artificial intelligence the other one uh for omniverse that ill explain what it is uh on the end this is based on system software that is rtx cuda and physics and they run on top of our hardware depending on the kind of uh workload we are working on this case uh recommendation engine we need the one specific computer architecture or several architecture uh to host this kind of workflow because the hardware needs the communication needs the memory the disk uh the speeds the latency inside the computer are totally different from a graphics application to a ai training application to a computer vision or to recommendation system like this one so sometimes we need to change a lot of those components to be able to deliver the most valuable uh sdk or2 to our uh developers that our end users so this is how weve built more than 150 sdks over the past uh 15 years uh and basically when when full stack of this is done we open everything so uh the hardware you buy from oems like lenovo uh dell hp super micro and others and all the software we give it away to the um to our uh software ecosystems so it could be startups it could be individual developers a software could be uh for free or they could be open source which is the case of the the most important ones okay this is how any viia growth to the size we are today we growth through our ecosystems developers are extremely important for us today in nidia has more software engineers than hardware engineers just to illustrate uh this so without this ecosystems of oems developers startups and software partners uh we wouldnt be able to do that much of innovation over the past uh the kides i will start this talk talking about one uh of uh our fr works that is focused on uh data science specifically and it doesnt matter what you do with artificial intelligence at some point you need to process a huge volume a huge amount of data and this is why we develop rapids rapids is an open source project you can access more information here uh rapids ai and you can find a lot of resources to learn to explore and even to run rapids on cloud providers with free tire accounts to access gp us there and we develop rapids to address those three issues first data science and data manipulation when the volum is huge its time consuming its associated with the costs not only hardware costs but also people costs because sometimes you as a developer put a network to train or put a a machine learning model to train you need to wait like 10 hours 15 hours and then when you realize that i forgot this parameter i could do this adjustment to the data set and here we go again and wait another 15 hours and it is frustrating for everyone uh working on those projects from the the data scientists to the developers and to the customers and people that are there waiting to see the value of what theyre investing so to address those things uh we develop uh rapids and ifia has solution for anything uh related to data size to be accelerated on our hardware and rapid is is focus here f focused here on analytics machine learning streaming and uh visualization we all that work with data science we learn a data science basically using p dat such as p data uh uh library such as pandas psychic learn numai scifi and others they work very fine uh they are very easy to work with basically but they were developed to run on what i used to call generic hardware which means they are meant to run on cpus basically and everything is uh done and built for that kind of uh environment what we did is that we didnt reinvented the wheel we created a set of libraries that has semantic and synthatic compatibility in most of the cases with those existing um python libraries so we have for instance qdf and qao that can do almost the same of bundas qml qph and several others and the cool thing about it is everything is based on apache arrow that is basically a way of arrange data in a columnar way that makes a lot of sense when we are talking about uh highly distributed and parallel uh algorithms which is basically what i do inside the the gpu this is really important because uh if i if i need gpu acceleration on a part of my data science o aai pipeline i used to pay i want big price every time i need to take the data from the run memory and put it inside the run memory uh of the gpu this is the latency that i have basically due to physical constraints on the servers and computers and sometimes if i use uh gpu just for one part of this pipeline i would take more time bringing thata or moving data back and uh uh back and and forth instead of processing so the ideal scenario when using acceleration like a gpu is that you run almost the biggest part of your algorithm or your pipeline uh without leaving the gpu ideally i just bring the data on the beginning of the processing i do the whole pipeline on the gpu and i return it and this is uh what we do with rapids integrated with aach arrow to start with rapids you need just a computer with an nvidia uh gpu can be something like a gforce 10 something will be uh able to to run uh rapids uh and then you will start with the code you already have uh on pi data the first thing to do is to migrate this code to rapids and then youll be able to see uh a very significant speed up on your code and check that its been really really uh distributed over uh all the computer resources we have on the gpu but the real magic happens when we associate rapids with dusk that is another opensource um software and what desk does is basically hell get the rapids code and hell distribute it automatically through multiple gpus on the single node or through multiple nodes with multiple gpus in a cluster so without knowing all the details about synchronization distributing computing and complex things like that you can start from a item code that we already have accelerated using rapids on the gpu and then really accelerating it through clusters and massive parallelism using desk its really really uh easy to do uh and everyone can you know scale uh their processing and to scale this kind of processing its really important because uh typically we start data science uh workloads and projects with a certain amount of data and this amount of data uh typically will grow a lot over the uh the next years or months so assuring that i just need to add more nodes to increase my computing power without need to rewrite all my code its really important to scalability also rapids can integrate with other deep learning tools uh through a apache um arrow that i explained so with that we can really optimize the data sharing inside the gpu memory and we are able to really do uh a very good uh speed up on our pipeline because most of all the pipeline are been running uh inside the gpu so with rapids we leave a scenario where i spend a lot of time just waiting for the code to run and then discovering that i mythed thingss and so on uh to a a space where im really productive and building more i know cases of training that used to take like 15 hours uh reduce it to 15 minutes uh using uh rapids and there are several cases documented all over the internet about it just to give you an idea who is using and adopt rapids today basically 25 of the foron 100 companies are already using rapids we have more than a 100 open source and commercial software integrations with rapids and much much more than 350 contributors on geub those are some logos of companies that are using rapids and we really have a big community around it one of the case that i really would like to highlight is the usage walmart does with hits basically they used to process a lot of data and one of the things they achieved its a 17 percentage point improvement in forecast accuracy on what will sell on each one of the uh north american usa uh stores so with rapids they were able to deliver 100 faster fit engineering and 20 time faster model training within rapids thats con x boost so we are talking here about a huge amount of data and the result of this kind of processing has a big impact economically speaking on companies and uh organizations such as wmart after talking about data science i will start talking about deep learning and the first framework that i would like to present you uh on nvidia its a framework called a tow that is an acon for train adapt and optimize and this is exactly what to does so creating an ai application uh can be hard and complex and basically we start uh getting uh open source models from model zool we have all over internet those models are okay but once i need to run this model on a gpu to get acceleration i need to do a lot of adjustment on the model and i need to do a lot of pre trining on this model to get it really optimized for the this could take a lot of time efforts and investments to do so what i did in nvidia is that we now offer for free more than a 100 combinations of models that could go from generic things such as image classification object detection segmentation they are all based on the key and most important architecture used today for neural networks and we optimized those models uh not only in the structure but also on everything else related to do the best performance possible on a gpu we give those models for free but we also trained some models to achieve specific tax tasks such as people detection gaze detection facial landmarks vehicle classifications uh automatic speak recognition and several others so when you start a project you dont start with a generic model you start with a model that is already optimized for the gpus then you use uh tto kit to train adapt and optimize this model basically you do the transfer learning and train the model based on your existing data set and the good thing of having this model inside tow is that your production model can be generated according to the gpu that we use uh for the inference so imagine that i have a model to recognize people on a people county solution that i would run uh on a shopping mall uh on my customer a i need to run the analytics uh on a embedded device that will be close to the camera because i dont have bandwidth to connect to the cloud so in this case the model needs to be really optimized to run on an embeded device such as jackson but on the other customers they have their own data center inside uh the shopping mall so i can put a server there and then i will use a different gpu on the server so i can optimize for this gpu on my third client uh my third customer they have a very good infrastructure on a cloud provider so i just stream the data the video and process everything uh at the cloud so i can also generate the best optimized model to this environment so using to you assur that you are starting from the best reference model uh train it and optimize it for gpu and some of those models were trained like 100000 hours on clusters of gpu just to illustrate the amount of effort we put on this so we start with a model really optimize it you do the transfer learning and then you can generate the model uh to be consumed by your application uh according to the hardware that youre going to use and as i said edge to cloud those are the the key frameworks that we have for inference dpstream uh riva and triton i will talk about some of those uh in a few minutes to has also uh characteristics uh of doing training optimization so it can runs on a multi node multi gpu node and multi gpu with multiple gpus it works uh it works with automatic mix at precision and on inference it can also uh do the printing which is removing uh nodes from the neural networks that are not being activated by this particular training and also quantization that is basically adjusting uh the weights of the network according to the hardware that we process it when you use those two things in combination we can achieve up to four time speed up for inference which means that we can uh really uh have very very well optimized uh applications one of uh characteristics of to that i really enjoy because i work a lot with computer vision uh is that to can do data augmentation on online so data augmentation on computer vision is basically a technique where i have a very limited data set i cant have more images on this data set so i do something that i ed to joke as the miracle of multiplying images where we just uh apply some uh image algorithms to generate four or five image based on a single one this could be rotating adjusting brightness and several other uh manipulations that we do but when we do this on the traditional way i need to preprocess my data set and i need to have a lot of space because my data set will become really huge what we can do with to is that once the image is on the gpu memory its very very easy to apply those image uh algorithms to do data augmentation and we can do this during training which mean i dont need to expand my existing data sets i just use this feature from to and online it to uh generate other images and here we have some numbers as you can see 100 images experiment with 20x augmentation we really achieve uh a lot of uh accuracy uh on both uh scenarios so if you work with computer vision please take a look on this feature for tow some examples and use cases of to uh its like this one i will start from a ponet model that is a model trained on rgb cameras and but i need on my solution to identify people on infrared cameras so basically what i do i start from the optimized ponet model i will do transfer learning with my annotated uh images from thermal cameras and i will have a very good result and a working uh solution in a few minutes also i can do things much more complicated on this case i need to have a data set where i recognize helmet uh and and people and heads so i dont have a data set with all those things i would start from the same people net i mentioned before ill use a labeled helmet data set and ill generate a data set with annotations of people and face then i combine both things and i have my final uh neural network trained as you can see i can achieve over 80 of aange class precision after 100 a books that is a really short training when were talking about uh deep learning the newest versions of tow has some very interesting tools the first one uh is to help you to do segmentation masks uh just to illustrate when you need to segment images you need basically to paint the pixels that are part of a special class in this case uh i need to paint all the pixels that are part of uh bottles uh but with this auto labeling tool basically what you can do is is that we can turn uh bounding boxes on images on segmentation masks uh the other thing we are also providing a vision transformer models that are revolutionizing everything we do in computer vision uh basically uh vision transformer is the usage of the transformers technology from natural language processing and llms to computer vision this is really interesting its open source and you can apply it on different type of uh platforms hardware we have tools for automl which uh will help you to identify the best uh hyperparameters for optimize your training uh it can be integrated with uh services using rest apis so you can build a whole solution around uh tow and as i mentioned that when you use burning and quantization we can achieve up to 4x increase uh of performance during inference similar from to that we basically use for uh several different types of uh neuron networks we have nemo that is a specifically for training llms large language models just to illustrate the complexity of this kind of processing in 2018 the stateof art model that we have had around 94 million parameters we are talking today in models with much more than 53 billion parameters gpt3 has 175 billion parameters so were talking about huge models and the training of those models is really complex because we need to have data parallelism we need to use thousands of gpus this will be distributed on a cluster with a lot of computing power and nemo is the framework uh developed exactly for those use cases we can do with nemo the data coration distributed training and accelerated inference it has the key feat features to help you to build and to customize your llms large large language models uh projects and we also offer uh nemo guard raos that is open source and guard raos its a very important part basically on chatbots because they will be uh the the boundaries or they will limit the application to answer on a specific topics uh this means uh safety for the users and prevent hallucinations and executing third party calls uh and increase the security uh if someone ask some funny questions to chat gpt ever and you have say hey i cant answer this this because this and that uh its the guard rail that are working i used to say that if i went to chat tpt without guard raos and say hey i have uh you know those those uh drugs in my home that i bought from drugstore nearby uh which ones i need to take to you know do something bad to myself hell be able to answer this but aar said hey we cannot answer and i believe you got the points so memo guard raos is also available open source as nemo uh you can use it uh and also contribute uh to build our large language model uh projects uh hia its one framework that we developed to do three things automatic speak recognition neuro uh machine translation and text to speeech those are three very complex tasks on ai because you need to do a lot of steps on the pipeline of processing we build riva to do exactly that so its much easier to to talk about reva explaining the use cases in this case i have a customer talking to a contact center i have reva listening to the customer converting this to text using nlp and several other technologies to print real time recommendations on the contact center uh agent screen and allowing him and helping him to do the right recommendations to the customers here its another scenario that we you know get used on the on the pandemics and also on several different uh tools that we have of videos online where we can do automatic transcription we can do automatic translation we can do summarization and everything else just bas it on video this is where we uh really can be us it and when we have a a a full round applications is where the customers is interacting totally with computers hes not talking up with humans anymore and but he feels that hes being uh very well treated in this case uh its a project from nidia called it name it tokyo that is a talking pusk where you go there uh on a on on uh a fast food restaurant and you start talk talking to this small robot he will give you suggestions he will recognize you and it will be a very uh natural way of speak just to have you an idea theres a lot of drivethroughs uh today all over the world that are changing uh uh humans on the on the other side of the microphone for technologies like this and its very adaptable and able to use on those kind of environments imagine that you have a lot of noise you have different audio qualities and r can handles uh all this uh he i use uh pretrained models the same way uh that we talking about to you can use tow to retrain those models and if you need to expand the model or do something more complex you can use nemo uh then you can implement all the three key tasks uh using reva and implement uh your end application uh using this uh technology uh just to explain how we evoluted uh or reg growth over the past years uh in 2018 we have like 45 error rates on automatic speak recognition uh two years ago it was uh less than 10 and its still getting reduced and also uh we can have a lot of accuracy when we customize uh the models because if im working with a model that will uh be used to have conversations with uh uh banks customers uh financial action customers this vocabulary will be totally different of the vocabulary of a person asking for a hamburger on a fast food so with riva we can adjust uh the whole model to be very specific to one kind of jargon or dialect or specific set of words this will increase a lot the accuracy so riva is our sdk for doing this triton is what i used to call the swiss army knife of uh inference on ai today uh when we use an ai based application our application is talking to an inference server and if i write my application to one specific inference server lets say py torch tensorflow or any other and tomorrow my ai and and data scientists discover that there is a new model based it on other framework other inference server that is performing best to my solution i will need to write the end user application again it doesnt mean if this end users application its on a cell phone directly talking to an inference server or if this application uh its lets say a java application that is interfaced during the the the interface between all the ai pipelines and the end user applications that can be a lot so uh tron is key in this kind of scenario because you write your application to talk to tron and tron can talk to all other uh uh major uh backends and frameworks for inference so lets say tor flow tensor rt open vino p torch onyx xg boots several others so basically your application is really to tron and then today my data science scientists said hey this is the best model we have have it need to run on things orlow okay tomorrow or the six months they say no now we have a model on pytorch i just change the model on tron configuration i dont need to write the application again uh tron can communicate using http grpc and c uh you can do inference distributing on the different kind of hardware you have uh it support from uh bare metal to virtualization edge to cloud different types of queries uh that are typical from different types of applications real time badge streams and mixed models and it also totally uh develop it focusing on maximized hardware utilization which means reducing the total cost uh of operations of your uh solution um it also has integration with kubernetes uh with its scalable microservices its very easy we provide this uh uh although things uh for for triton uh is integrated with the key practices and also some tools of machine learn so basically with all the tools from cloud provider aure uh aws google and so on uh it support ao live model updates uh dynamic model loading which is really interesting because i just load the models that i will use on a particular transaction and several other tools that will help you to optimize your operation and mlops guys will love it and also uh we have its all open source and customizable uh you can change it uh to adjust your needs uh this is an idea of all the uh frameworks formats and models it can has on on execution backends this list is growing more and more its very very flexible and in a nutshell triton is what put together everyone working with ai applications and also end user applications so for data science we can use any framework uh for people working with uh maintaining those uh models uh working on inference uh the models can be on any repository and it can be deployed on any platform so tron is what really brings you flexibility to work with all those um all those kind of uh ai applications the next framework that i would like to talk uh is isaac because robotics are really growing today we are having robotics uh boosting every industry uh i know that in europe we have cases of automatic delivery even on small stores in small small cities uh and also we have uh this kind of applications on different areas retail logistics manufacturing and so on but to build ai in robotics at scale its really hard because its hard to develop here we are talking about developing the software the ai and the hardware in parallel its hard to test and its really really hard to deploy so this is why we created uh the isac robotics platform that is basically a platform made by a simulation component a key sdk training component and a deployment uh part so to start the development with isaac i start with a simulator where i will import all the projects that i have for for my physical robot i will create a virtual word where this robot will operate i connect it with virtual sensors and actuators and im able to start programming and training all the ai inside this virtual words when im done with the training i use the isaac sdk to develop all this i just need to move the code from the training with isaac sdk to the physical robot so this whole cycle really really speed ups the development and its very useful in situations such as uh one case one very good case that we have with bmw that they need to train robots to do pilet uh movement on a very specific area of the factory they couldnt stop the factory just to train the robot on that area so they use it lighter and photogrametry they created a digital twin of that area they imported the project projects of the robot they going to use from the the the supplier they trained the robot inside this uh digital tn on several several several thousands of hours and when they are okay with the results they just validated on the reward and for the robot that was traded with photogrametry and real 3d images it doesnt matter uh if he is on the virtual play environment or if its on the real world so isaac is very very useful for that uh the other framework that i think its very important for nvidia uh its part of a bigger platform actually nam it metropolis metropolis is a platform for us to build smart spaces and infrastructure uh when we see what is happening with ai today uh real useful ai solutions computer vision solutions for the the the existing word the real world are not developed by one single neuron network but but for the concatenation of the results of processing of a lot of different kinds of networks on the same system it doesnt matter if its smart cities factories public spaces retail anything uh you use you need a platform to really cross information for different kinds of uh neural networks this is one example app developed by one of nvidia partners using the metropolis platform uh here we can see that they analyzing images coming from drones fixed cameras uh and several different types of cameras they are measuring un counting cars people uh bikes and several other kind of things moving in a city uh this two uh that they develop all allow the the operator to configure and to parameterize any kind of alerts uh he really uh wants to have and as you can see here we have hundreds of neuron networks working in parallel and working giving information to a single platform that is able to consolidate everything this is exactly what metropolis is its a platform a framework for smart sensors where you can use and this is being used actually on retail traffic management logistics healthcare manufacturing factories several other applications under the hood what i have are those pretrained models that i i mentioned to you the beginning of my talk uh we also have tto kit that now you already know what it is and deepstream what im going to explain to you by the way everything that im talking here is available through uh one website that is ng gc nvidiacom on ngc we have mo pretrained models we have containers for all nidia sdks and the coolest part we have containers for uh other sd cas such as tensor flow p torch with everything optimized for the gpu so all you need to have uh a tensor flow running with everything optimized for an nidia gpu is to run one of those containers if you are using a cloud provider some of the containers youre going to find inside uh the marketplaces and also youre going to find uh virtual machine images from nvidia uh on those places that can be used to launch the containers and to to put the containers directly from uh ngc computer vision is much more than ai inference because to every frame sorry we need to capture and the code the data we need to preprocess the image and sometimes do the batching then we do inference tracking to not count the same object twice for instance send this metadata extracted to other systems finally made in a composition typically uh developers use a gpu acceleration just here and they do all the other parts using things like open cvv running on cpu and then we got on that situation where i have a lot of data back and forth from the computer run memory to the gpu memory and im not using an optimized pipeline this is exactly what we can do when we use dpstream dpstream can use the best acceleration it possible on each parts of this processing it supports uh ex run from edge to cloud and its very uh flexible and why this is so fast because we just get the data on the beginning of the pipeline and then we process everything almost with zero memory copies so the buffer is the same we just passed to the other part of the processing buffer to the same place on on the gp vi memory and this is the fastest way of processing this kind of frameworks uh of workloads we have also uh the opportunity to get data right from the camera buffer using a a networking uh technology for nvidia called rer max where i can put industrial camera i can extract the the the r stream from this camera directly to the gpu memory increasing even more uh the the performance of my solution uh we can use uh deepstream we can program deepstream using python using c but its also a composer that is a tool where i can i will just drag and drop some blocks and build uh graphically my application uh on the end of this whole process i will generate a container and i can uh deploy it anywhere uh i will use the models that were p trained to to we can also import onx models to tow and optimize them to use this way and if i need to use any other model i can just use tron with uh the right the proper inference server uh and dpstream can communicate uh with titon using a grpc uh the other cool thing is on i detected metadata from the video that im analyzing or the images i can send messages with this metadata to radies uh and qtt kafka and several other uh cloud providers uh i uh iot protocols okay so after i package this whole thing that i develop the whole solution i will have a container i can deploy anywhere for the people that work with computer vision and knows a little about it uh this is basically a set of plugins for g streamer so being a set of plugins it means that i can uh reuse a lot of of my code from project to project sometimes just changing the type of networks if i need to count people on this application and cards on the other one basically what changes is the neuron network i change the people detection to a car detection and the rest of my application the card of my application is already written and what we see in practice and im able to uh work with more than uh almost 700 startups in latin america uh direct and im seeing this in practice the the startups and the companies that adopt those technologies speed up the development time up on 10 times because its much more easy and productive to work uh with all those uh frameworks metropolis allows developers to develop things like uh smart spaces application where i have several different uh technologies and uh neural networks and i can put all together on a single system to end my talk here i will talk about omniverse that is new era of collaboration and and uh and simulation uh virtual wordss are essential for the next area of innovation we truly believe that everything that we will exist on physical world we will exist on a virtual world first because it is the cheapest way to build the thing right for the first time uh we build and use omniverse uh to our own application so we have ac uh that is uh the avatar technology uh we use omniverse for it we have uh digital twins of several cities where we use those digital twins to train uh anyia drive sim technologies because inside the digital twin uh we can generate uh events that we cant in the real world so change the weather or cause collisions traffic and other conditions isaac sim uh that i mentioned before is already integrated with omniverse so i can create digital twins of small stores up to huge places and also we are developing earth 2 that is a hardware and software infrastructure that were going to work with researchers uh and this is basically a digital twin of the planet earth to study uh the climate changes impact and several other things uh omniverse its the tool for two basically scenarios the first one is 3d creation and collaboration the second one is to create industrial uh digital twins and to operate them when were talking about uh 3d design its really complicated because we are talking about of large teams with different skills which means different tools which means different kind of files to integrates to build a pipeline and we need more and more accuracy on uh on the results of 3d compositions so those workflows are rising uh in complexity uh we have a lot of challenges today people are working from different places uh in the earth uh sometimes we are talking about 3d data sets that cant be mov it from a place to the other because of their size and also uh its really important have a single uh search of truth because most of this work is done uh linearly so this person here uh create an initial idea of a character uh and then this one uh make it more detailed this one put some polygonal and implement it this one put all the skeleto and the last one here start doing the animations imagine what happens if this guy get the wrong version of this one and they will just discover it on the end when everythings built together so having a single sour of truth is really important omniverse can be uh has a what we call nucleus that is a central server where where everyone can work uh together using the key technologies from nvidia over the past decades and they can act and interacting real time so everyone can be on the same place virtual space uh each one doing its part of the work and everyone see what everyone else is doing and when we are talking about digital twins we can uh create the digital twin and we can operate this digital twin doing things like like uh robot training digital human training factory planning operation simulations and this is all done basically on real world existing assets lets say uh the the cad uh files for the factory itself or for the robots that are operating there so basically we go from the real world and we create everything on the on the virtual environment here we have an example of two two people working ga an omniverse on this side i have a a person working uh creating an area that is this kitchen on a specific tool this is not omniverse i think its revit and on the left side i have someone working with omniverse create putting texture lighting and so on so as you can see uh every change one one uh professional does to his part its automatically translated to the other one here we have someone working on the architecture of of this lightning stuff and uh someone else doing the lightning actually and illumination and all those things and we have rear here on the right side an image that lets say our art director can be seing and giving feedback on real time this is good this is bad change this change that change the lights change the structure almost in real time without omniverse this is a back and forth uh linear process here we have three people working putting assets on a kitchen for one specific demo and here we have an arch director on a project director just giving feedback in real time so they can see uh the changes or other um people are doing those are the basic uh uh components that is part of omniverse nucleus that is a central repository connect that allows to connect to several other applications kit that are a set of res reference applications basically in python uh to allow you to interact and to use all these simulation technologies and also rtx renders its just possible because of the universal scene description that is a file format and a file framework that is the html for 3d virtual wordss it was created by pixer because pixer has a very complex uh bu data building and and asset building pipeline and they say hey theres must be an easy way for us to integrate all these so uh nvidia and market working this with uh with pixar uh there is now a whole uh open structure and open association to evolute open usd uh today basically on omniverse when you have an image like this uh this image is based on different assets that are like layers and each of those assets were created with a different to so without omniverse without usd it will be really hard to integrate everything on a single experience omniverse is not a tool its a fabric uh with a huge amount of tools ecosystem this slide is outdated because its really hard to keep it updated but it gives you a very good idea uh here are some of the software partners and dearly adopters of omniverse uh omniverse can be automated and extended using python so i used to explain amerse to developers as imagine a python frameworks that can build metaverses and virtual words and v virtual things for you to explore uh theres a lot of open source involved with this you can just uh download omniverse check one of the extensions and start working with them inside omniverse we have physical uh simulation uh using all nidia technologies so we can replicate the real world rules physical rules to the virtual bo uh we also have a handler and you can use external rinders too uh but its very uh uh well developed it to bisa label so when when we have complex workflows we can distribute this to clusters uh realtime photo physical accurates and base it all on open standards this is a short video of the bmws factory of the future i invite you all to go to youtube and check any video on home universe digital twins youll see a lot of uh very good examples of videos and and projects and sometimes its hard to say what is real and what is virtual using this technology to finalize here i would like to leave you uh three key uh links uh to learn more about all the things developer nvidiacom just go there register yourself forums that really works because the framework developers monitor those forums uh we have a lot of uh uh material to learn um and to know more about an evid including some training uh inception program that is a program to engage startups all over the world we have more than 15000 startups all over the world im responsible for almost 700 here in latin america and we are really helping startups to innovate with ai and hardware gpu acceleration and also an evid on dem month i would suggest to everyone on an that you change this to nerdflix uh but idea was not accepted yet but this is basically what it is so on each event that we have v that we talk about in the next slide uh we generate like 400 talks of content everything is uploaded to this portal so if you need to learn anything about nvidia uh tools and software and use cases you just go to nvidia on demand you can log in using your developer account and you are able to access to all the content we already generated uh march 20 uh 18 to to 21 were going to have in in san jose first time presential actually hybrid after five years were going to have the gtc that is the biggest uh technology conference we have on nvidia i really recommend you all to register now uh just scan this qr code you will be on a link where you can register for free for the online uh event but also if you want to attend in person with this q code you can have 25 discount uh on your tickets i would like to thank again uh to to otavio and fabio that invited me to be here with you today thank you for all uh your attention to uh if you want to contact me on social media uh this is the easiest way uh you can find me everywhere from linkoln to twitter uh using this thank you very much i hope this was useful for you thank you thank you jamar yeah it was really uh amazing and thanks a lot for for that i think that the viewers really understood how to accelerate application using nv sdks i think it was an insightful presentation for all devop cl this just just one last insight i know it was a lot of content but in a nutshell what i would like to say is uh 10 years ago when we going to start to use gp acceleration and ai you really need to know a lot of parallelism uh process distribution it was hard okay so we create a lot of abstraction layers that today its really use its really easy for any developer to start uh working with these technologies uh this is not rocket science anymore and i really recommend you guys to explore uh because theres a lot to be viewed yeah yeah yeah i really agree with you and i and i know that some of our engineers are also here so i think that is amazing for them too and yeah and thank you again for for that it was a very lean and illustrative presentation that i liked that bright aspects that you want to highlight and to dive in so thank you for that thank you for your time and for your kindness to be with us today and while we are waiting to see if you have some questions uh from our viewers uh as i told you before in this live stream we have a final interactive segment where each speaker um must answer must answer question from the previous speaker and do a question for next one and the question that our previous speaker brought to you and you need to answer is its not related with tech but would you rather give up social media or eat the same dinner for the rest of your life so i think that i know your answer but because i think that will get social media you know uh my my twitter account is for 2006 yeah okay so uh i was using twitter when we receive sms messages on feature phones so i started almost back there and uh know a lot of changes over the past i dont think it was so oh thank you otavio uh i dont think its uh its the the the same place it was in the past uh but i have a question for the next one uh how geni how gener generative ai change it the way you work oh amazing amazing and and i think it is a very good question because i already know who the next speaker will be so i think that fits like a glove in him so yeah question i will let me just point that and just to to ask the question to our next speaker and i think think there is no questions and i think it is maybe its a good thing because your your presentation was as insightful as it can be it could be so that said uh and thank you thank you for for um your presentation thank you for being with us today and i hope that you also enjoyed it and for the our viewers and for our ex biters not a goodbye see you see you in march in the third edition of this liv stream and and of course john mario are invited to participate in our next live stream not as a speaker uh but as a viewer but maybe next time you can be speaker again so thank you so much thank you one one last thing guys if you if you need to send me any questions please send me on twitter i will not talk that x because i hate this name and also on linkedin okay just send me your questions on linkedin i answer to everyone there thank you so much for the opportunity i really hope to see very very great ai apption come from you guys as far as i remember your social media name omen bite omen bite omen beachen yeah yeah okay let me just uh on bit just to be here for everyone that wants to like this i think om om bit exactly in the comments exactly so if you want to contact uh jar please do so in their social media by omen be so thank you again jamar and see you next time bye bye thank you so much pleasure byee pleasure 