decided to do it on the architecture of nvidia gpus originally i wanted to do it on both amd and nvidia but after id done a couple hours of research i found out that there is just going to be way too much information to cover for both architectures in just a 7 page paper so i just decided to get a little more specific and ended up just deciding to do it on nvidias architecture alright so when i was doing my research i just started on a googled nvidia gpu architecture and a few things came up but the kind of the main thing i first thing was maxwell architecture so i clicked on that and i started looking into maxwell architecture and actually found that its not really an architecture its a micro architecture and at first i wasnt really sure exactly what the difference was so then i had to go and look up all the information on that and then once i found out kind of the differences i realized oh i really did know the difference between an architecture and a micro architecture i just didnt really know that i knew that so basically when you think about the architecture its basically the instruction set usually we just refer to it as is a but the architecture includes things like the registers the data types instruction types like add sub etc things like that kind of basically the things we learned at the first of the semester in like chapter 2 so kind of the way i like to think about it is when i think about the architecture i like to think about programming with mips when youre writing like a function like add in mips that functions basically the architecture and then when we look at the microarchitecture this is basically how the instruction set architecture is actually implemented um this is kind of the plans or like the design of how the architecture will get done what it needs to do on this this includes things like pipelining data paths and branch prediction this is kind of things weve just barely learned about chapter four so i kind of want to give a quick background or history of the different nvidia microarchitectures basically because i had no idea that they changed them i guess when i started my research i just kind of figured that nvidia have an architecture and they just kind of stuck to it and they made like little improvements here and there i didnt really think that they would actually like rename them and change them that much so i just kinda want to give a quick background just on the different ones im kind of when they ended and when the new ones started so the oldest one that i could really find information on was fermi and they do have some architectures that are older than for me but they dont really have very much information on them so fermi and they used in their geforce 400 and 500 series gpus they faded fermi out in 2011 and they replaced it with kepler and tipler is pretty welldocumented i could find a lot of information on it and they use that for their 600 and 700 series gpus they use them in some of their geforce 800 m series gpus and the m is for mobile so they use those for like laptops and the idea for kepler was to improve the energy efficiency over fermi and they use cutler until 2014 and they replaced kepler with maxwell and maxwell is what theyre currently using for maxwell they use that in the later models of the geforce 700 series gpus and they use them in some of the hundred m and in the 900 series the 900 series it was a little bit confusing at first on those also because theres two versions of maxwell theres maxwell one and maxwell  maxwell  was implemented in the geforce 900 series but it was just its basically the same they just made slight improvements and again for maxwell they wanted to keep on improving the energy efficiency but they also made a lot of improvements to the stringing multi processors those are pretty complex and theyre kind of like at the heart of the architecture i guess so ill be going over the streaming multi processors a little bit later but we kind of need a little bit more information on how the gpu works and how the architecture said it before we get to those so ill be going over those here in just a little bit in my previous slide i had talked a little bit about this shimming multi processors and those kind of work together with cuda so i wanted to give a quick rundown of cuda before i went into further detail on the streaming multi processors cuda stands for compute unified device architecture and its basically cuda is a parallel computing platform and a programming model that nvidia created basically it allows the programmer to write their code a little bit differently and when you do that it gives instructions straight to the gpu bypassing the assembly language for cuda you can write it in c c and fortran and when you write cuda for the same language they just kind of refer to it as parallel c so right here you can see the standard c code on the left and then the parallel c code on the right and probably the biggest company that is known for using this i guess would be adobe they use it in a lot of their software and a lot of people that use adobe products will use nvidia gpus because they can use this parallel c code that adobe has written to take advantage of the gpu and it i just makes the program work a lot better with the gpu with cuda you are able to take advantage of all the alus on the graphics card this is a really big advantage because the gpu has many more al use than the cpu does right here you can see an example of how many a user on the cpu on the left compared to the al use of the gpu on the right and with all available use this basically what makes you so good at doing what they do with all of the al use they are able to calculate many more calculations so many more things so cuda it basically lets you take advantage of the al use without said cuda does still have some disadvantages and here are some of the advantages and disadvantages cuda is very good at running parallel algorithms and parallel algorithms are basically algorithms that can be executed one piece at a time all on different processing devices then at the very end they combine them back together again serial algorithms on how to execute sequentially one time through cuda doesnt take advantage of three augur ifs it works really well with parallel algorithms so if youre going to be using cuda you need to keep that in mind so ive talked a little bit about what cuda is now im going to talk about how it actually works cuda uses thousands of threads executing in parallel all these threads are executing the same function and this is whats known as a kernel so right here we can see the thread is the little squiggly line the programmer or the compiler can organize the threads into what are called thread blocks which you can see on the slide are just multiple threads all block together or group together the thread blocks are then organized into grids of multiple thread blocks the thread block is the grouping of threads that are all executing at the same time basically these work together through shared memory a thread block has its own block id for its grid earlier i mentioned the streaming multi processors this is kind of where they come back into the picture streaming multi processors are the part of the gpu that actually runs these kernels alright now we can dig a little bit into the streaming multi processors the streaming multi processors are very important i kind of like to think of them as like the heart of the architecture the streaming multi processors perform all the actual computations they have their own control units execution pipelines caches and registers nvidia refers to the streaming multi processors in a couple different ways in the previous architecture kepler they referred to as a streaming multi processor adds an sm x in maxwell architecture they just refer to them as an sm m and when they compared both architectures together so if theyre comparing maxwell and kepler theyll just refer to the streaming multiprocessor as an sm it was kind of confusing at first but once you figure it out it makes sense so just keep the wording in mind when i talk about these for the rest of the presentation for maxwell the number of streaming multi processors is different depending on what nvidia card you look at currently the geforce gtx 980 is one of the topoftheline gpus the nvidia is making currently so in the next couple examples im going to be going over the architecture and the streaming multi processors of the gtx 980 and then just keep in mind since from talk about the 980 this is a 900 series gpu so this is version 2 of maxwell i dont know that theyre that much different but just keep in mind that since im going over the gtx 980 this will be the second generation maxwell architecture inside the microarchitecture of the gtx 980 there are 4 64bit memory controllers i have them highlighted right here in red with that there are 4g pcs these are bound to the 4 memory controllers gpc is short for graphics processing cluster each of these g pcs has for streaming multi processors inside of them so we have for streaming multi processors per g pc and then there are 4 g pcs so that gives us 16 streaming multi processors this is specifically for the gtx 980 graphics card if you looked at a lower model like the 970 or the 960 those would have less streaming multi processors inside of them which effectively is what makes the gtx 980 more powerful than those cards is the amount of streaming multi processors that is inside of the gpu now if we just focus on a single streaming multi processor we can see all these green cores and nvidia refers to these as cuda cores and inside this remain multi processor its split up into four processing blocks inside each of these blocks is a four by eight grid of cores and if we want to calculate this we can look and that gives us 32 cores per block and since the streaming multi processor is split up into four blocks each with 32 cores that gives us a total of 128 cores for a single streaming multiprocessor and if we go back and look at the big picture we remember there 16 streaming multi processors in total so if each one has a 128 cores that gives us 2048 cores and i thought it was pretty cool to look at this because if we go to nvidias website and look at the specifications for the gtx 980 its listed it is having 2048 cuda cores this to me was pretty cool because um you always hear about how many cuda cores the graphics card has but that never really meant anything to me it was just a number so when i started doing this research i thought was cool that i could look down into the architecture of the gtx 980 look at the streaming multi processors and then see those actual cuda cores and then see how the sharing multi processor split up and then to see how many of them are and then you can kind of draw the lines and make the connection to see oh this is where they get that number from so i thought that was pretty cool because it goes from just being a number to actually knowing what those cuda cores are and where they are in the actual architecture of the gpu so i thought that was pretty cool going back to the streaming multi processor we need to take a look at these warp schedulers the streaming multi processor schedules threads in groups of 32 parallel threads these are what are called warps each streaming multiprocessor contains four warp schedulers each of these warp schedulers can run two instructions per warp every clock the streaming multi processor uses its own resources for scheduling an instruction buffer lets take a little closer look at the instruction buffer i wanted to compare the swimming multi processors between kepler and maxwell for maxwell nvidia gave each warp scheduler its own instruction buffer so for every streaming multiprocess enter in maxwell it has four instruction buffers if you compare this to the streaming multi processor in kepler it still has four warp schedulers but they dont have any instruction buffer according to nvidia and maxwell they gave the streaming multi processor its own instruction buffer for each warp scheduler and this gave the swing multi process theres a huge performance increase lets take a little closer look at the numbers when you compare between maxwell and kepler so as you can see in maxwell they doubled the amount of streaming multi processors compared to kepler when you look at the cuda cores they also increase the cuda cores by 25 percent and basically they increase everything while also decreasing the power consumption and effect made from the maxwell architecture much more efficient looking at these performance increases between maxwell and kepler and one of the major improvements being the fact that nvidia gave this roomy multi processor and maxwell their own resources it kind of brought up a question that i thought of when i was doing the research and i just kind of i kind of wonder why it took nvidia so long to give the sharing multi processors their own resources and with all the research that ive done you can tell that the tsarina multi processors are very important and they have a big job to do they have tons of information going through them it just kind of seems like a natural idea to give them their own resources in all the previous architectures like kepler the streaming multi processors had to share resources with other things so to me that would seem like a huge bottleneck so i wonder why it took so long until maxwell for them to do this and i dont know it could be just a limitation of the technology or just a limitation of the hardware i dont know im not a gpu architect or anything like that and i could just be oversimplifying the problem it could be something theyve been working on for a long time theyve known about it and they just barely have the resources to do it now im not a hundred percent sure but when i was doing my research that was just kind of one of the things that i kind of brought up as a question i guess is i just wonder why it took so long obviously these are very complex and a lot of stuff goes into them and im not to the level where i could build a gpu or anything like that so i have faith that nvidia does the best that they can do and im sure in the future the gps will only get better and better and faster and more efficient and will get even more performance out of them but i really learned a lot doing this research paper at first it was really frustrating because i didnt realize how much that i didnt know so i would look up one thing and the article would be talking about something and then id have to do research on that because i didnt know what they were talking about so it was a lot more research than i had ever planned but i really did learn a lot and it was really is actually kind of fun to see what kind of how nvidia creates these so i really enjoyed doing this research project and i actually learned much more than i had ever even thought that i could have and heres all my credits thank you very much 