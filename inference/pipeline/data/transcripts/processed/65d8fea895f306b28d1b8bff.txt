for your games for most games you dont need   to think too much but if you want some cool  lighting effects or to make a nice looking   3d game or some postprocessing effects or  stylized art you would need at some point   learn the rendering pipelines i cant say its  easy but who said it would be the whole game   development process is very complicated but  this video will try to explain rendering in   the most basic words possible and will gradually  dig in deeper so hopefully this knowledge will   let you understand not only specific aspects of  the chosen game engine but the whole process   used similarly in most of the engines and  frameworks out there so lets overcome the   fear of getting into rendering by diving into the  heavily inspired by opengl rendering pipeline render pipeline is a whole process of making your  visual components visible on a screen but dont   worry modern frameworks and game engines simplify  the process for example defold takes care of   a tremendous amount of stuff that is needed for  each pixel to be displayed on different devices   you can interact with rendering on some very top  layer but also have the possibility to dig in   to achieve really cool things render pipeline  answers 3 crucial questions what to render   where to render and when to render by answering  them one by one you will see the whole picture   in defold what to render is defined by render  predicate  a kind of group of visual components   where to render is defined by the view of the  games world through some kind of projection   like you would see the word through a camera but  on a flat screen and finally when to render is   controlled by the render script so what you  can render simply all visual components but   additionally in defold you can group them you  can use the mentioned render predicates for this   those are kind of filters allowing you to control  what to render on screen visual components like   for example sprites tilemaps and 3d models are  all rendered on screen separately gui nodes are   drawn separately on screen or even some debugging  lines and texts you can create more such   predicates to control what you are drawing for  example you can disable drawing gui at all you   can enable drawing lights or 3d models separately  such grouping give you more control over where   and when you will be rendering different visual  components so imagine render predicates as just   names tags filters in materials for each visual  component material yes each visual component   has a material assigned to it material is a  nice abstraction defining how to render a given   component and thus answering part of the question  where to render how because it defines special   programs also called shaders that define how given  geometry will be rendered so where you would   see components looking at the games world through  camera seeing some kind of view and projecting   this view on your flat screen and then after  rasterization which i will explain later on   where to draw each pixel on screen there are a  lot of concepts in a few statements so they all   need to be explained but before this well focus  more on the last question when to render this   is defined in defold in render script  the heart  of the render pipeline which describes the whole   process of producing frames on screen this is  the one and only script in defold that defines   the whole rendering process take a look at your  gameproject  2 most important things are in   one tab the main collection that will be loaded  at start of your game and the special render file   which contains simply 2 informations the render  script and the list of materials that could be   used in it thats all before we analyze the  render script lets imagine how we produce pixels   on our screens imagine each visual component  in your game as a bunch of triangles even for   2d sprites those could be drawn on two triangles  forming a rectangle its easy to imagine when   looking at polygons in 3d models or a synthwave  retro graphics theres much mathematics involved   behind it because the graphics are calculated in  a few steps each triangle consists of three lines   connecting three points those points are called  vertices each vertex is a point a coordinate   in 3d space so one of the first steps in graphics  pipelines are regarding operating on those   coordinates those vertices so the program making  calculations on those is called a vertex program   or a vertex shader graphics cards are powerful  in such calculations and dont even sweat making   complex matrix operations on millions of points  after all those overwhelming calculations we   have a nicely defined 3d geometry there are few  important but more complicated steps in between   but at first imagine you only get those triangles  somehow and everything is very simple and you   get all this data projected into a flat screen to  finally get you to the point of rasterization and   this  is simply a slicing of all this geometry  into small pieces very well known as pixels   for each of those pixels at the end we need to  assign a color so each diode on your device   could be lit according to this and as whole form a  frame a color of each pixel is described as a mix   of a given amount of red green and blue colors  and additional transparency value graphics cards   perform calculations for each pixel  a tiny  fragment on your screen utilizing a fragment   program also called fragment shader at the  end some additional work could be performed by   the graphics engine which goes through some tests  and blending stages are you with me could we go   deeper into how those 3d triangles are actually  projected on a flat 2d screen there are a bunch   of steps to transform each point in 3d into a  point on the 2d screen so we would go through   them and name the steps and the coordinate  systems that are produced after each of them   first we have a primitive example geometry  lets say a 3d cube which could be described   with 8 points in 3d space when you make such a  cube in for example blender it will be created   in its local coordinate space and all points will  be described in such a coordinate system so each   visual component in 3d games has their own local  space but you want all those objects to be put   into our game world so they need to be included  in some common global coordinate system  the   word space you have a model matrix inbetween  that converts local points on our 3d cube and   each other model to the world coordinate system  we wont analyze the mathematics behind it but   you might note that multiplying data by special  matrices allows us to convert translations and   rotations to different coordinate systems and this  is what this model matrix is for so when you have   all the data in one world space you then watch  the world through the lenses of the camera or the   players virtual eyes what you see is depending  on where the camera is and at what it is pointing   right so its the view of the camera and so  such a view has its own coordinate system   whose origin is commonly located at the middle of  the view to convert the world space coordinates   to view space you perform a calculation with view  matrix so the view matrix is really describing   your ingame camera for example in first person  shooters you can feel how directly you rotate the   camera around and in 2d games you usually cant  rotate the camera but can move it around showing   different pieces of the world but defining only  a view is not enough we cant see the games   camera in three dimensions because we need to  somehow project this view to flat screens we do   this using a special matrix called projection  matrix and this step converts view space to   a socalled clip space it is a projected view  of what the camera sees and its called clip   because all the coordinates that are outside  such a projected view area are clipped and   not shown on screen the remaining coordinates  will end up as fragments visible on screen and   whats funny  when some triangles on the edge  of view are cut in half because of such clipping   the graphics engine automatically creates new  triangles on the edges to actually fit inside   and fill the whole screen finally such a clipped  view is then very quickly converted into another   convenient space which is a screen space with  its origin commonly in a corner of the screen and   this is done with the use of a viewport matrix  you dont need to actually always use the whole   screen making fullscreen games but you can draw  everything in a window that could have different   dimensions  and this is exactly what the viewport  is for you define the dimensions of a rectangular   space on which you will be drawing all your  pixels ok another level of diving in we   barely mentioned how the pipeline projects the 3d  view to a flat screen but you might be aware that   there are different kinds of projections each  projection could be defined by a special 3d shape   a sliced lump or most simple cuboid or even  a cube  called professionally frustum   it has two important sides called near and far  planes that are actually parallel to the screen   and the rest of the sides just connect them  especially very simple kind of projection   such that near and far planes have the same area  is called an orthographic projection its name is   because the lines describing the view  then are orthogonal to the view plane   so the frustum is a simple cuboid so  effectively you dont scale 3d objects to screen   like you think the objects far away from you are  perceptibly smaller than the ones closer to you   but see them in a way that imaginably are far  far infinitely far away from you but you have an   eagle sight orthographic projection is commonly  used in 2d games because its simple very clear   and because you have flat sprites that dont look  good in perspective projections yeah i mentioned   it without explaining but very easily you imagine  that perspective projection is an opposite its   an effect where you see objects far away from you  actually smaller than the ones close to you its   very useful for 3d games because it really  is corresponding to our actual perception   to how our eye see in such projections the lines  connecting the near and far planes are actually   meeting at one point in space and an angle between  them is called field of view fov this angle   for realistic projection like our eyes work is  usually around 45 degrees but its maths and   nothing stops you from making field of view of 60  degrees even like in doom games it will be kind   of deformed but you will let players see more on  a flat screen all those coordinate systems are   by opengl convention righthanded because  well take your right hand and look at its inner   side with all fingers pointing up in the positive  direction of the yaxis let your thumb point to   the right to the positive direction of the xaxis  and bend some of your fingers to the front of you   to point in the positive direction of the zaxis  you might at some point take advantage of such   knowledge so thank me later that really  is a lot of information for one video   so for even more thorough explanations i invite  you to check out learn opengl website where joey   explained in the most affordable and detailed way  how opengl works and i mentioned opengl a few   times already without saying what it is because  its not so simple its not a language its   not an implementation of rendering  its just a  specification a standard describing how functions   should communicate with graphic cards to produce  desired results its generally the graphics card   manufacturers that implement drivers for this  and game engines are usually an abstraction of   using raw opengl functions more thin or less but  in the end they simplify this defold is running   on opengl and therefore its render pipeline  is heavily influenced by opengl workflow   but thanks to offering such an abstraction layer  over opengl  the underlying renderer could be   replaced and for example many engines including  defold also offers support for vulkan and webgl   render pipelines such possibility is crucial to  support different devices because for example   apple devices use metal while html5 builds run  on webgl but you dont care about it  its a job   of defolf you build one rendering pipeline and  could release on many platforms simultaneously and   thats what makes a game engine multiplatform  you will find a similar approach in most of the   engines community is also fiddling with renderers  and for example there is a possibility to make a   game in defold but with raylib renderer for  now please digest the amount of information   provided here and in the next video we will dive  into the render script of defold and by the way   if you want to help creating such videos  leaving  a traditional like a comment and subscribing to   my channel with this ding dong ringing helps  me be encouraged to continue and make more   videos and for even more support  i already run  patreon kofi and soon  github sponsors  sooo   you know  my kind of pig bang for future is  there  have a nice day and see you soon 