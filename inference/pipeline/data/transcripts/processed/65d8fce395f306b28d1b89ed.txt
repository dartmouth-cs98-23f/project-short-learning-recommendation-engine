maybe radical computational medium like biological and theres other ideas so theres a lot of spaces in a6 or domainspecific and then there could be quantum computers and wood so we can think of all those different mediums and types of computation whats the connection between swapping out different hardware systems in the instruction set do you see those as disjoint or they fundamentally coupled yeah so whats so kind of if we go back to the history you know when moores law is in full effect and youre getting twice as many transistors every couple of years you know kind of the challenge for computer designers is how can we take advantage of that how can we turn those transistors into better computers faster typically and so there was an era i guess in the 80s and 90s where computers were doubling performance every 18 months and if you werent around then what would happen is you had your computer and your friends computer which was like a year year and a half newer and it was much faster than your computer and you he he or she could get their work done much faster than your typical achiever so people took their computers perfectly good computers and threw them away to buy a newer computer because the computer one or two years later was so much faster so thats what the world was like in the 80s and 90s well with the slowing down of moores law thats no longer true right hes not now with you know not death side computers with the laptops i only get a new laptop when it breaks right well damn the disk broke or this display broke i got to buy a new computer but before you would throw them away because it just they were just so sluggish compared to the latest computers so thats you know thats a huge change of whats gone on so but yes since this lasted for decades kind of programmers and maybe all of society is used to computers getting faster regularly it we now now believe those of who are in computer design its called computer architecture that the path forward is instead is to add accelerators that only work well for certain applications so since moores law is slowing down we dont think generalpurpose computers are and get a lot faster so the intel process of the world are not going to havent been getting a lot faster theyve been barely improving like a few percent a year it used to be doubling every 18 months and now its doubling every 20 years so it was just shocking so to be able to deliver on what moores law used to do we think whats going to happen what is happening right now is people adding accelerators to their microprocessors that only work well for some domains and by sheer coincidence at the same time that this is happening has been this revolution in artificial intelligence called machine learning so with as im sure your other guests have said you know a i had these two competing schools of thought is that we could figure out artificial intelligence by just writing the rules topdown or that was wrong you had to look at data and infer what the rules are the machine learning and whats happened in the last decade or eight years this machine learning has won and it turns out that machine learning the hardware you built for machine learning is pretty much multiply the matrix multiply is a key feature for the way people machine learning is done so thats a godsend for computer designers we know how to make metrics multiply run really fast so generalpurpose microprocessors are slowing down were adding accelerators for machine learning that fundamentally are doing matrix multiplies much more efficiently than generalpurpose computers have done so we have to come up with a new way to accelerate things the danger of only accelerating one application is how important is that application turns a translate machine learning gets used for all kinds of things so serendipitously we found something to accelerate thats widely applicable and we dont even were in the middle of this revolution of machine learning were not sure what the limits of machine learning are so this has been kind of a godsend if youre going to be able to excel deliver on improved performance as long as people are moving their programs to be embracing more machine learning we know how to give them more performance even as moores law is slowing down and counterintuitively the machine learning mechanism you can say is domainspecific but because its leveraging data its actually could be very broad in terms of in terms of the domains that could be applied in yeah thats exactly right sort of its almost sort of people sometimes talk about the idea of software 20 were almost taking another step up in the abstraction layer in designing machine learning systems because now youre programming in the space of data in the space of hyper parameters its changing fundamentally the nature of programming and so the specialized devices that that accelerate the performance especially neural network based machine learning systems might become the new general yes so the this thing thats interesting point out these are not coral these are not tied together the its enthusiasm about machine learning about creating programs driven from data that we should figure out the answers from data rather than kind of top down which classically the way most programming is done and the way artificial intelligent used to be done thats a movement thats going on at the same time coincidentally and the the first word machine learnings of machines right so thats going to increase the demand for computing because instead of programmers being smart writing those those things down were going to instead use computers to examine a lot of data to kind of create the programs thats the idea and remarkably this gets used for all kinds of things very successfully the image recognition the language translation the game playing and you know he gets into pieces of the software stack like databases and stuff like that were not quite sure how journal purposes but thats going on independent this hardware stuff whats happening on the hardware side is moores law is slowing down right when we need a lot more cycles its failing us its failing us right when we need it because theres going to be a greater in peace a greater increase in computing and then this idea that were going to do socalled domainspecific heres a domain that your greatest fear is youll make this one thing work and thatll help you know 5 of the people in the world well this this looks like its a very general purpose thing so the timing is fortuitous that if we can perhaps if we can keep building hardware that will accelerate machine learning the neural networks thatll beat the timing be right that that neural network revolution will transform your software the so called software 20 and the software the future will be very different from the software the past and just as our microprocessors even though were still going to have that same basic risk instructions to run a big pieces of the software stack like user interfaces and stuff like that we can accelerate the the kind of a small piece thats computationally intensive its not lots of lines of code but there it takes a lot of cycles to run that code that thats going to be the accelerator piece and so this thats what makes us from a computer designers perspective a really interesting decade but hennessy and i talked about in the title of our turing warrant speech is a new golden age we we see this as a very exciting decade much like when we were assistant professors and the wrists stuff was going on that was a very exciting time was where we were changing what was going on we see this happening again tremendous opportunities of people because were fundamentally changing how software is built and how were running it so which layer of the abstraction do you think most of the acceleration might be happening if you look in the next ten years that google is working on a lot of exciting stuff with the tpu sort of theres a closer to the hardware that could be optimizations around the iroc closer to the instruction set that could be optimization at the compiler level it could be even at the high level software stack yeah its got to be i mean if you think about the the old risks this debate it was both it was software hardware it was the compilers improving as well as the architecture improving and that thats likely to be the way things are now with machine learning they theyre using domainspecific languages the languages like tensorflow and pi torch are very popular with the machine learning people that those are the raising the level of abstraction its easier for people to write machine learning in these domainspecific languages like like pi torch in tensorflow so were the most optimization right yeah and so the and so theyll be both the compiler piece and the hardware piece underneath it so as you kind of the fatal flaw for hardware people is to create really great hardware but not have brought along the compilers and what were seeing right now in the marketplace because of this enthusiasm around hardware for machine learning is getting you know probably a billions of dollars invested in startup companies were seeing startup companies go bellyup because they focus on the hardware but didnt bring the software stack along you 