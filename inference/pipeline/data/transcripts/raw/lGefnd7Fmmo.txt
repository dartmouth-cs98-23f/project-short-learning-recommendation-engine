second,duration,transcript
0.0,4.35,all right for my research topic I
2.37,5.939,decided to do it on the architecture of
4.35,8.459,nvidia gpus originally I wanted to do it
8.309,7.051,on both AMD and NVIDIA but after I'd
12.809,4.351,done a couple hours of research I found
15.36,3.929,out that there is just going to be way
17.16,5.279,too much information to cover for both
19.289,4.951,architectures in just a 7 page paper so
22.439,4.59,I just decided to get a little more
24.24,7.02,specific and ended up just deciding to
27.029,6.721,do it on Nvidia's architecture alright
31.26,5.16,so when I was doing my research I just
33.75,4.379,started on a googled NVIDIA GPU
36.42,4.799,architecture and a few things came up
38.129,5.971,but the kind of the main thing I first
41.219,4.051,thing was Maxwell architecture so I
44.1,4.08,clicked on that and I started looking
45.27,4.53,into Maxwell architecture and actually
48.18,3.149,found that it's not really an
49.8,4.2,architecture it's a micro architecture
51.329,4.921,and at first I wasn't really sure
54.0,3.66,exactly what the difference was so then
56.25,3.48,I had to go and look up all the
57.66,4.26,information on that and then once I
59.73,4.14,found out kind of the differences I
61.92,3.66,realized oh I really did know the
63.87,3.63,difference between an architecture and a
65.58,5.399,micro architecture I just didn't really
67.5,5.909,know that I knew that so basically when
70.979,5.28,you think about the architecture it's
73.409,5.911,basically the instruction set usually we
76.259,4.591,just refer to it as is a but the
79.32,6.299,architecture includes things like the
80.85,8.07,registers the data types instruction
85.619,6.991,types like add sub etc things like that
88.92,4.949,kind of basically the things we learned
92.61,5.88,at the first of the semester in like
93.869,6.54,chapter 2 so kind of the way I like to
98.49,3.75,think about it is when I think about the
100.409,4.981,architecture I like to think about
102.24,5.4,programming with MIPS when you're
105.39,4.2,writing like a function like add in MIPS
107.64,4.32,that functions basically the
109.59,4.62,architecture and then when we look at
111.96,5.07,the microarchitecture this is basically
114.21,5.25,how the instruction set architecture is
117.03,4.549,actually implemented um this is kind of
119.46,4.74,the plans or like the design of how the
121.579,5.341,architecture will get done what it needs
124.2,5.309,to do on this this includes things like
126.92,4.51,pipelining data paths and branch
129.509,3.691,prediction this is kind of things we've
131.43,5.73,just barely learned about
133.2,6.509,Chapter four so I kind of want to give a
137.16,4.549,quick background or history of the
139.709,5.25,different Nvidia microarchitectures
141.709,6.101,basically because I had no idea that
144.959,3.901,they changed them I guess when I started
147.81,3.75,my research I just kind of figured that
148.86,3.959,Nvidia have an architecture and they
151.56,2.459,just kind of stuck to it and they made
152.819,3.691,like little improvements here and there
154.019,3.931,I didn't really think that they would
156.51,3.509,actually like rename them and change
157.95,5.13,them that much so I just kinda want to
160.019,4.171,give a quick background just on the
163.08,4.05,different ones I'm kind of when they
164.19,5.12,ended and when the new ones started so
167.13,4.56,the oldest one that I could really find
169.31,4.06,information on was Fermi and they do
171.69,3.81,have some architectures that are older
173.37,4.89,than for me but they don't really have
175.5,5.519,very much information on them so Fermi
178.26,7.02,and they used in their GeForce 400 and
181.019,6.33,500 series GPUs they faded Fermi out in
185.28,6.0,2011 and they replaced it with Kepler
187.349,5.371,and Tipler is pretty well-documented I
191.28,4.35,could find a lot of information on it
192.72,5.16,and they use that for their 600 and 700
195.63,6.389,series GPUs they use them in some of
197.88,5.82,their GeForce 800 M series GPUs and the
202.019,5.161,M is for mobile so they use those for
203.7,6.09,like laptops and the idea for Kepler was
207.18,6.0,to improve the energy efficiency over
209.79,5.61,Fermi and they use Cutler until 2014 and
213.18,3.72,they replaced Kepler with Maxwell and
215.4,4.739,Maxwell is what they're currently using
216.9,5.91,for Maxwell they use that in the later
220.139,4.711,models of the GeForce 700 series GPUs
222.81,6.29,and they use them in some of the hundred
224.85,6.57,M and in the 900 series the 900 series
229.1,4.81,it was a little bit confusing at first
231.42,4.11,on those also because there's two
233.91,4.859,versions of Maxwell there's Maxwell one
235.53,6.359,and Maxwell - Maxwell - was implemented
238.769,4.59,in the GeForce 900 series but it was
241.889,5.311,just it's basically the same they just
243.359,6.391,made slight improvements and again for
247.2,5.28,Maxwell they wanted to keep on improving
249.75,4.29,the energy efficiency but they also made
252.48,3.27,a lot of improvements to the stringing
254.04,5.91,multi processors
255.75,6.57,those are pretty complex and they're
259.95,4.89,kind of like at the heart of the
262.32,3.96,architecture I guess so I'll be going
264.84,2.37,over the streaming multi processors a
266.28,2.789,little bit later
267.21,5.7,but we kind of need a little bit more
269.069,5.1,information on how the GPU works and how
272.91,3.689,the architecture said it before we get
274.169,6.09,to those so I'll be going over those
276.599,5.04,here in just a little bit in my previous
280.259,4.47,slide I had talked a little bit about
281.639,4.981,this shimming multi processors and those
284.729,5.28,kind of work together with CUDA so I
286.62,5.91,wanted to give a quick rundown of CUDA
290.009,5.88,before I went into further detail on the
292.53,5.52,streaming multi processors CUDA stands
295.889,5.911,for compute unified device architecture
298.05,5.7,and it's basically CUDA is a parallel
301.8,5.91,computing platform and a programming
303.75,5.849,model that Nvidia created basically it
307.71,4.62,allows the programmer to write their
309.599,4.141,code a little bit differently and when
312.33,3.51,you do that it gives instructions
313.74,5.489,straight to the GPU bypassing the
315.84,8.1,assembly language for CUDA you can write
319.229,6.391,it in C C++ and Fortran and when you
323.94,4.55,write CUDA for the same language they
325.62,5.28,just kind of refer to it as parallel C
328.49,4.12,so right here you can see the standard C
330.9,5.4,code on the left and then the parallel C
332.61,6.0,code on the right and probably the
336.3,4.679,biggest company that is known for using
338.61,6.66,this I guess would be Adobe they use it
340.979,6.87,in a lot of their software and a lot of
345.27,4.8,people that use Adobe products will use
347.849,4.651,NVIDIA GPUs because they can use this
350.07,7.29,parallel C code that adobe has written
352.5,7.62,to take advantage of the GPU and it I
357.36,7.23,just makes the program work a lot better
360.12,6.599,with the GPU with CUDA you are able to
364.59,4.41,take advantage of all the ALUs on the
366.719,5.19,graphics card this is a really big
369.0,6.06,advantage because the GPU has many more
371.909,5.25,al use than the CPU does right here you
375.06,4.38,can see an example of how many a user on
377.159,6.781,the CPU on the Left compared to the al
379.44,6.0,use of the GPU on the right and with all
383.94,4.86,available use this basically what makes
385.44,4.979,you so good at doing what they do with
388.8,4.5,all of the al use they are able to
390.419,7.351,calculate many more calculations so many
393.3,7.589,more things so CUDA it basically lets
397.77,4.829,you take advantage of the al use without
400.889,5.761,said CUDA does still have some
402.599,7.44,disadvantages and here are some of the
406.65,5.729,advantages and disadvantages CUDA is
410.039,6.321,very good at running parallel algorithms
412.379,6.271,and parallel algorithms are basically
416.36,3.88,algorithms that can be executed one
418.65,3.81,piece at a time all on different
420.24,4.5,processing devices then at the very end
422.46,5.669,they combine them back together again
424.74,7.289,serial algorithms on how to execute
428.129,5.76,sequentially one time through CUDA
432.029,5.491,doesn't take advantage of three augur
433.889,5.101,ifs it works really well with parallel
437.52,4.25,algorithms so if you're going to be
438.99,5.19,using CUDA you need to keep that in mind
441.77,4.269,so I've talked a little bit about what
444.18,3.509,CUDA is now I'm going to talk about how
446.039,3.81,it actually works
447.689,5.22,CUDA uses thousands of threads executing
449.849,5.43,in parallel all these threads are
452.909,5.28,executing the same function and this is
455.279,4.651,what's known as a kernel so right here
458.189,5.371,we can see the thread is the little
459.93,5.639,squiggly line the programmer or the
463.56,4.829,compiler can organize the threads into
465.569,4.261,what are called thread blocks which you
468.389,3.18,can see on the slide are just multiple
469.83,4.649,threads all block together or group
471.569,5.791,together the thread blocks are then
474.479,6.36,organized into grids of multiple thread
477.36,6.0,blocks the thread block is the grouping
480.839,5.221,of threads that are all executing at the
483.36,5.549,same time basically these work together
486.06,6.839,through shared memory a thread block has
488.909,6.24,its own block ID for its grid earlier I
492.899,4.621,mentioned the streaming multi processors
495.149,3.36,this is kind of where they come back
497.52,3.119,into the picture
498.509,4.56,streaming multi processors are the part
500.639,5.52,of the GPU that actually runs these
503.069,6.65,kernels alright now we can dig a little
506.159,6.63,bit into the streaming multi processors
509.719,4.421,the streaming multi processors are very
512.789,3.06,important I kind of like to think of
514.14,3.93,them as like the heart of the
515.849,3.87,architecture the streaming multi
518.07,4.139,processors perform all the actual
519.719,5.61,computations they have their own control
522.209,6.181,units execution pipelines caches and
525.329,4.531,registers Nvidia refers to the streaming
528.39,2.81,multi processors in a couple different
529.86,3.83,ways
531.2,4.05,in the previous architecture Kepler they
533.69,4.95,referred to as a streaming multi
535.25,5.25,processor adds an SM X in Maxwell
538.64,6.21,architecture they just refer to them as
540.5,5.76,an SM m and when they compared both
544.85,3.18,architectures together so if they're
546.26,3.06,comparing Maxwell and Kepler they'll
548.03,4.77,just refer to the streaming
549.32,5.82,multiprocessor as an SM it was kind of
552.8,6.0,confusing at first but once you figure
555.14,6.03,it out it makes sense so just keep the
558.8,6.87,wording in mind when I talk about these
561.17,6.69,for the rest of the presentation for
565.67,4.05,Maxwell the number of streaming multi
567.86,4.74,processors is different depending on
569.72,6.99,what Nvidia card you look at currently
572.6,6.57,the GeForce GTX 980 is one of the
576.71,4.83,top-of-the-line GPUs the Nvidia is
579.17,4.02,making currently so in the next couple
581.54,3.48,examples I'm going to be going over the
583.19,5.37,architecture and the streaming multi
585.02,4.8,processors of the GTX 980 and then just
588.56,4.62,keep in mind since from talk about the
589.82,6.66,980 this is a 900 series GPU so this is
593.18,5.19,version 2 of Maxwell I don't know that
596.48,3.3,they're that much different but just
598.37,3.54,keep in mind that since I'm going over
599.78,5.97,the GTX 980 this will be the second
601.91,5.82,generation Maxwell architecture inside
605.75,4.71,the microarchitecture of the GTX 980
607.73,4.7,there are 4 64-bit memory controllers I
610.46,5.97,have them highlighted right here in red
612.43,6.91,with that there are 4g pcs these are
616.43,5.69,bound to the 4 memory controllers GPC is
619.34,6.27,short for graphics processing cluster
622.12,7.39,each of these G pcs has for streaming
625.61,5.93,multi processors inside of them so we
629.51,5.28,have for streaming multi processors per
631.54,5.32,G PC and then there are 4 G pcs so that
634.79,4.74,gives us 16 streaming multi processors
636.86,5.46,this is specifically for the gtx 980
639.53,6.18,graphics card if you looked at a lower
642.32,4.8,model like the 970 or the 960 those
645.71,2.94,would have less streaming multi
647.12,4.62,processors inside of them which
648.65,5.13,effectively is what makes the GTX 980
651.74,3.63,more powerful than those cards is the
653.78,6.36,amount of streaming multi processors
655.37,6.48,that is inside of the GPU now if we just
660.14,3.12,focus on a single streaming multi
661.85,2.55,processor we can see all these green
663.26,4.16,cores
664.4,6.93,and NVIDIA refers to these as CUDA cores
667.42,5.71,and inside this remain multi processor
671.33,4.41,it's split up into four processing
673.13,7.14,blocks inside each of these blocks is a
675.74,5.88,four by eight grid of cores and if we
680.27,6.48,want to calculate this we can look and
681.62,7.05,that gives us 32 cores per block and
686.75,5.07,since the streaming multi processor is
688.67,5.7,split up into four blocks each with 32
691.82,4.95,cores that gives us a total of 128 cores
694.37,5.58,for a single streaming multiprocessor
696.77,6.33,and if we go back and look at the big
699.95,6.03,picture we remember there 16 streaming
703.1,6.14,multi processors in total so if each one
705.98,7.53,has a 128 cores that gives us 2048 cores
709.24,6.13,and I thought it was pretty cool to look
713.51,3.93,at this because if we go to Nvidia's
715.37,5.28,website and look at the specifications
717.44,7.89,for the gtx 980 it's listed it is having
720.65,7.95,2048 CUDA cores this to me was pretty
725.33,6.99,cool because um you always hear about
728.6,5.67,how many CUDA cores the graphics card
732.32,4.83,has but that never really meant anything
734.27,4.14,to me it was just a number so when I
737.15,3.27,started doing this research I thought
738.41,5.25,was cool that I could look down into the
740.42,5.22,architecture of the GTX 980 look at the
743.66,4.32,streaming multi processors and then see
745.64,4.56,those actual CUDA cores and then see how
747.98,4.41,the sharing multi processor split up and
750.2,4.14,then to see how many of them are and
752.39,5.19,then you can kind of draw the lines and
754.34,5.31,make the connection to see oh this is
757.58,3.6,where they get that number from so I
759.65,3.27,thought that was pretty cool because it
761.18,4.56,goes from just being a number to
762.92,4.59,actually knowing what those CUDA cores
765.74,3.99,are and where they are in the actual
767.51,5.43,architecture of the GPU so I thought
769.73,4.65,that was pretty cool going back to the
772.94,3.66,streaming multi processor we need to
774.38,3.87,take a look at these warp schedulers the
776.6,4.31,streaming multi processor schedules
778.25,5.73,threads in groups of 32 parallel threads
780.91,5.11,these are what are called warps each
783.98,4.74,streaming multiprocessor contains four
786.02,4.83,warp schedulers each of these warp
788.72,5.55,schedulers can run two instructions per
790.85,5.58,warp every clock the streaming multi
794.27,4.04,processor uses its own resources for
796.43,3.96,scheduling an instruction buffer
798.31,4.72,let's take a little closer look at the
800.39,4.56,instruction buffer I wanted to compare
803.03,5.37,the swimming multi processors between
804.95,5.25,Kepler and Maxwell for Maxwell Nvidia
808.4,4.17,gave each warp scheduler its own
810.2,4.47,instruction buffer so for every
812.57,5.07,streaming multi-process enter in Maxwell
814.67,4.47,it has four instruction buffers if you
817.64,4.02,compare this to the streaming multi
819.14,4.35,processor in Kepler it still has four
821.66,3.6,warp schedulers but they don't have any
823.49,3.96,instruction buffer
825.26,4.44,according to Nvidia and Maxwell they
827.45,4.44,gave the streaming multi processor its
829.7,6.09,own instruction buffer for each warp
831.89,5.34,scheduler and this gave the swing multi
835.79,4.08,process there's a huge performance
837.23,6.0,increase let's take a little closer look
839.87,7.88,at the numbers when you compare between
843.23,7.62,Maxwell and Kepler so as you can see in
847.75,4.69,Maxwell they doubled the amount of
850.85,4.2,streaming multi processors compared to
852.44,4.53,Kepler when you look at the CUDA cores
855.05,5.22,they also increase the CUDA cores by 25
856.97,5.7,percent and basically they increase
860.27,4.47,everything while also decreasing the
862.67,3.93,power consumption and effect made from
864.74,4.71,the Maxwell architecture much more
866.6,5.96,efficient looking at these performance
869.45,5.34,increases between Maxwell and Kepler and
872.56,4.24,one of the major improvements being the
874.79,3.21,fact that Nvidia gave this roomy multi
876.8,3.21,processor and Maxwell their own
878.0,3.69,resources it kind of brought up a
880.01,5.73,question that I thought of when I was
881.69,6.21,doing the research and I just kind of I
885.74,3.3,kind of wonder why it took Nvidia so
887.9,4.74,long to give the sharing multi
889.04,6.51,processors their own resources and with
892.64,4.62,all the research that I've done you can
895.55,3.87,tell that the Tsarina multi processors
897.26,4.17,are very important and they have a big
899.42,6.03,job to do they have tons of information
901.43,7.86,going through them it just kind of seems
905.45,6.84,like a natural idea to give them their
909.29,6.21,own resources in all the previous
912.29,5.04,architectures like Kepler the streaming
915.5,5.34,multi processors had to share resources
917.33,8.85,with other things so to me that would
920.84,7.83,seem like a huge bottleneck so I wonder
926.18,4.079,why it took so long until Maxwell for
928.67,4.199,them to do this and
930.259,6.42,I don't know it could be just a
932.869,6.59,limitation of the technology or just a
936.679,7.111,limitation of the hardware I don't know
939.459,5.8,I'm not a GPU architect or anything like
943.79,3.18,that and I could just be oversimplifying
945.259,2.97,the problem it could be something
946.97,3.299,they've been working on for a long time
948.229,3.84,they've known about it and they just
950.269,5.19,barely have the resources to do it now
952.069,6.24,I'm not a hundred percent sure but when
955.459,4.86,I was doing my research that was just
958.309,5.101,kind of one of the things that I kind of
960.319,6.89,brought up as a question I guess is I
963.41,7.709,just wonder why it took so long
967.209,8.79,obviously these are very complex and a
971.119,7.32,lot of stuff goes into them and I'm not
975.999,5.92,to the level where I could build a GPU
978.439,5.4,or anything like that so I have faith
981.919,5.07,that Nvidia does the best that they can
983.839,5.58,do and I'm sure in the future the GPS
986.989,6.96,will only get better and better and
989.419,6.59,faster and more efficient and will get
993.949,4.74,even more performance out of them but I
996.009,5.62,really learned a lot doing this research
998.689,6.24,paper at first it was really frustrating
1001.629,5.73,because I didn't realize how much that I
1004.929,5.1,didn't know so I would look up one thing
1007.359,5.22,and the article would be talking about
1010.029,3.99,something and then I'd have to do
1012.579,3.0,research on that because I didn't know
1014.019,3.69,what they were talking about so it was a
1015.579,5.91,lot more research than I had ever
1017.709,5.73,planned but I really did learn a lot and
1021.489,5.22,it was really is actually kind of fun to
1023.439,7.26,see what kind of how Nvidia creates
1026.709,5.72,these so I really enjoyed doing this
1030.699,4.74,research project and I actually learned
1032.429,5.561,much more than I had ever even thought
1035.439,6.801,that I could have and here's all my
1037.99,4.25,credits thank you very much
