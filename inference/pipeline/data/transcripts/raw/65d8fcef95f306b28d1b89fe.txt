all right this is an introduction to
parallel programming we're going to
discuss the different types of uh
programming paradigms and some different
libraries and and application
programming interfaces that are common
uh and when we're talking about parallel
programming so just to take care of some
terminology first
um
well
we'll talk about a node you can think of
this as a standalone computer
it's similar to like a desktop computer
or even a laptop
this would be comprised of multiple cpus
cores processors possibly even gpus or
many integrated core accelerators
things that you would normally find in a
high performance say desktop computer
even though they don't have that form
factor of a desktop computer they're
typically blades
that are located in a rack and network
together to find
to form what we call a supercomputer
right so a supercomputer is just a
collection
of nodes and each of these nodes is sort
of a standalone desktop computer
again
they don't they don't look like a
desktop computer
but they have all the essential features
that a desktop computer would have
the next thing is you know these are all
sort of synonym synonyms cpu central
processing units socket processor core
i mean technically there's a difference
between a cpu and a socket as you can or
a socket and a core as you could have
multiple cores per socket but for the
purpose of this class it's fine just to
use the same um
you know or use them as synonyms so this
is the singular sort of execution point
in a computer
and finally then we have tasks and a
task would be
a single instance of computational work
so essentially what we're going to do is
often we have multiple tasks that we can
do at the same time
and we're going to then run them on
multiple processors or multiple cores
simultaneously and those chords can
exist on a single node or they can exist
across a network of nodes
on a supercomputer
so there's different types of
parallel architectures
the the sort of
most common
[Music]
is a distributed memory you know most
common form of a supercomputer would be
a distributed memory form factor this is
where
again like i said you have a collection
of nodes each one of these
would represent a node where you have a
cpu and memory and when i say when we're
talking about memory here we're talking
about ram random access memory not not
hard disk space the hard disk space
would be shared amongst the whole
network
but
essentially you'd have a whole network
of individual nodes
that each have cpus and some memory and
then you'd send the tasks off to each of
those and communicate over the network
when needed to share information between
the different computers
the the other
form factor is what's called a shared
memory computer
these were
pretty rare or very expensive until
until very recently with the advent of
general purpose graphics processing
units and
intel mini integrated core chips
which are both a type of shared memory
accelerator you would call them
but
you can have a
more traditional vectorized
cpu type computer that also has shared
memory
again memory being ram and in this case
you'd have
multiple cpus that all have access to
the same ram
and you can use
an advantage of that
or you can you can take advantage of
this to
do different computational tasks on each
cpu without having to communicate
across a network
when needed to share information
between the tasks
more often than not this is the type of
architectures that we're seeing nowadays
in modern supercomputers
that is
basically you have a collection of nodes
where each node then has multiple cpus
spread out across a network or even
[Music]
of the form factor where
you have a network of nodes and each
node has possibly multiple cpus
and multiple say gpus graphics
processing units which can be used to do
numerical computations
or
even
you know it's not listed here but you
could you could count many integrated
core chips
uh in a similar way to graphics
processor units they both plug into the
same sort of pci bus
on on a uh on a computer's heart uh
motherboard
so
these are common programming models
you would call these application
programming interfaces or particular
libraries
and so for shared memory computing you
have several
posix threads which are a posix unix
standard
openmp
openacc which is a very new thing that
will probably one day
encompass openmp as well
if you're working on an intel machine
you can use uh thread building blocks so
these are
application programming interfaces which
would be
a standard or
a standard set of functions that you
would call in a certain way or possibly
instructions for the compiler that you
would use in a certain way
to say unroll for loops or do other
types of things
[Music]
and and a lot of them have have
interfaces in multiple languages so
fortran c c plus plus these types of
things
additionally since we talked about gpus
gp gpus general purpose graphs
processing processing units these are
gpus that are intended to do numerical
computations on and not just for for
graphics processing
there you have cuda which is this is a
standard by nvidia it's an application
programming interface uh that only works
on nvidia machines and they have their
own compilers and such
opencl
is
introduced by apple and was later open
sourced but it's a way to do gp gpu
calculations on any gpu so it's
supported by nvidia but you could also
use it on
you know intel graphics processing units
for example
and again here you see open acc
which is supported by nvidia and pgi
compilers and others and this is a way
to sort of standardize development
between gpus and cpus
it's an effort to do that so that you
could write the same code and have it
exploit multiple threads on a cpu or
the graphics processing units as well
some some really new techniques if you
haven't integrated
an intel mini integrated core chip
you could use a technique called
offloading where essentially you're
going to move
all the data from the cpu onto
the mini-graded core chip
and have the work done there of course
there's some commun
communication costs associated with that
and then even newer there's this idea of
myo the myo is an acronym that stands
for mine yours ours
and uh this idea is that you can
essentially write the code in one way
and
then the computation will be shared
between the cpu
or multiple cpus
that share ram
along with
the integrated the the mini integrated
core chip that's
available on that node as well
for distributed memory there's really
only only one way
uh mpi message passage and message
passing interface
this is the
de facto standard for distributed memory
computing and one nice thing about it is
once you once you learn this you can
actually use it to do computations on
shared memory machines as well
you may not get quite the performance
boost that you would by using some
combination or you know using
the shared memory but the nice thing is
it does work it'll work on distributed
memory machines and a shared memory
machine and most of us now even our
laptops for example have multiple
cores that we could exploit so if you
use mpi you could
you know write code that could run
well on a supercomputer but also
take advantage of the multiple cores in
your
in your laptop without really any extra
work
and so you know again it stands for
message passing interface this is an
application programming interface this
is not a programming language there are
implementations of this
uh in fortran c and c plus plus
and then of course we've already learned
how you can build wrappers
so
in this class we'll actually use a
python wrapper
to the c plus plus
application programming interface of mpi
and
that's called mpi for pi and there'll be
a whole uh additional lecture on on that
and on mpi itself
and again
as i mentioned earlier most modern
machines are hybrids that is the you
know they are most modern supercomputers
are some hybrid and in that they have
both distributed memory nodes
uh as well as on node you have either
some gpu or mini integrated core
acceleration or at least multiprocessors
and so there's an opportunity to do
hybrid parallelization where you use
mpi to essentially send data
between the nodes and then on nodes use
some type of
posix threads or intel intel tbb or
something like that to speed up the
computation even further
so
uh
there's several different ways to kind
of
design parallel programs the only one
i'm going to discuss here is really with
re because it's it's the most common
uh in engineering applications
and that is that we're going to split
the problem data set up so we'll have
some problem data set this could be some
large mesh associated with a finite
element computation or
some large data set associated with that
you want to perform machine learning out
on
and
so then we're going to split that data
up into individual tasks where the work
will be performed individually and then
often of course these
tasks cannot be performed in isolation
when they can that's good and it's
called an embarrassingly parallel
program
but often they can't be performed in
isolation and so then there's
communication amongst the tasks
and that's what we'll use mpi for
so finally i'd like to just end with the
resources uh you know all the figures in
this
talk came from
from this website but it's also just a
good resource
and fairly up-to-date with respect to uh
parallel computing so
if you have access to the live slides
you can you can click on this hyperlink
and take a look at that for more
information
