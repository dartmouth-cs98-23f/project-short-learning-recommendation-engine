second,duration,transcript
2.06,3.069,[Music]
19.31,6.249,[Music]
38.12,6.72,hi everyone hi uh sorry for this little
41.36,6.28,delay in our live stream uh but yeah you
44.84,4.96,are here five minutes after and I'm to
47.64,5.28,welcome you to another exciting edition
49.8,7.279,of X bites brought to you uh by our
52.92,7.159,general sponsor ex and as usual I am
57.079,5.32,your host faai Community manag manager
60.079,5.561,KS and today's live stream Promised to
62.399,5.881,be extraordinary as usual and yeah you
65.64,6.04,have the privilege to have uh with us
68.28,6.36,today uh zamar Jamar welcome welcome to
71.68,5.0,the stage uh he is a the developer
74.64,5.159,relations manager at
76.68,5.119,Latin Nvidia and we'll be share more
79.799,5.241,insights about that during his
81.799,5.481,presentations and um he's going to speak
85.04,6.6,a little bit about how to accelerate
87.28,6.68,your e applications using Nvidia SDK and
91.64,6.04,before we dive into the technical
93.96,6.439,Wonders that jire has prepared for us um
97.68,5.0,I want to extend our thanks to him of
100.399,4.72,course uh for taking up the The
102.68,6.799,Challenge and contributing for the
105.119,7.6,growth of our U xbes Community uh this
109.479,7.24,live stream and just um a quick shout
112.719,7.641,out to our X gek to our sponsor x geks
116.719,8.4,uh just also to um
120.36,7.48,thank X to be our our sponsor X for
125.119,5.64,those that that don't know is a software
127.84,5.8,House of major group that is K Group X6
130.759,6.041,is specialized specialized in Clon
133.64,5.679,native Technologies and um they try to
136.8,4.88,maintain a profound commitment to
139.319,5.761,community building and this ucation to
141.68,5.36,the community uh is the is is also
145.08,4.159,demonstrated here in this live stream in
147.04,5.04,this monthly Dynamic but we also have
149.239,5.161,for me groups uh we also have our blog
152.08,5.6,spots our YouTube recordings among other
154.4,5.04,things things and um and if you want to
157.68,3.639,to follow us and to understand a little
159.44,6.2,bit more our community-driven
161.319,7.761,initiatives I will uh advise to uh just
165.64,6.679,uh click here in our uh link tree to
169.08,5.04,know a little bit more about us so that
172.319,3.481,said and without further Ado because you
174.12,5.199,are not here to listen to me but to
175.8,6.719,Jamar um let's give the spotline and the
179.319,6.361,stage to Jamar please Jamar the stage is
182.519,5.481,yours I will quit myself and thank you
185.68,5.04,again thank you so much Fabio for the
188.0,5.08,invitation and also tavio uh it's a
190.72,4.239,really honor to be here uh good evening
193.08,3.6,for everyone in this case I'm I'm
194.959,4.84,talking from Brazil uh it's still
196.68,5.119,afternoon here uh the idea today is to
199.799,4.881,show you what I used to call the dorsal
201.799,5.36,spine of Nvidia the case for uh
204.68,6.32,artificial intelligence uh we have more
207.159,6.201,than 150 sdks more than 500 libraries
211.0,4.439,almost all of them are for free and open
213.36,4.04,source and sometimes it's hard to
215.439,4.961,connect the dots and understand how the
217.4,5.6,pieces uh match together and this is uh
220.4,5.199,why I've built this presentation uh I
223.0,4.72,plan to cover uh the most important
225.599,5.2,areas uh we are working right now and
227.72,6.32,our communities are working right now uh
230.799,4.681,so let's start uh talking uh about how
234.04,4.32,Nvidia
235.48,5.479,Works usually everyone knows Nvidia
238.36,4.64,because of our Hardware gpus the CPUs
240.959,5.64,and the other stuff uh in Hardware we
243.0,6.439,produce but uh the way we work over the
246.599,4.761,past more than 15 years is that we
249.439,4.401,really listen to the needs of our
251.36,5.48,customers and based on the need of our
253.84,6.04,customers we develop a full stock for
256.84,5.68,them uh to to produce what they want I'm
259.88,6.68,going to use Merlin as one example uh
262.52,6.72,it's one SDK that is uh here on the list
266.56,5.8,uh recommendation engines is something
269.24,4.959,that was develop it and and use it over
272.36,4.6,the past decades by the whole industry
274.199,6.401,but every time anyone need to start a
276.96,6.28,new uh a new recommendation uh project
280.6,4.439,they need to put together a lot of Open
283.24,4.0,Source software stack then build
285.039,4.72,something in top of it then finally they
287.24,5.0,have a basic engine and they can start
289.759,4.841,working and when talking about Hardware
292.24,5.44,acceleration when you combine all those
294.6,5.319,pieces uh it's really really hard to
297.68,4.959,optimize everything uh to to get the
299.919,5.241,most of your Hardware so what we did we
302.639,6.28,talked to the key customers that use
305.16,7.039,those kind of uh engines and we listen
308.919,5.84,to uh their needs and then we develop
312.199,6.0,this SDK that is open source named
314.759,5.241,Merlin this SDK is based on three big uh
318.199,3.72,platforms actually this one is based on
320.0,5.479,AI but we have three big software
321.919,5.321,platforms one for HPC one for artificial
325.479,3.84,intelligence the other one uh for
327.24,5.399,Omniverse that I'll explain what it is
329.319,6.88,uh on the end this is based on system
332.639,6.081,software that is RTX Cuda and physics
336.199,4.921,and they run on top of our Hardware
338.72,4.199,depending on the kind of uh workload we
341.12,4.12,are working on this case uh
342.919,4.361,recommendation engine we need the one
345.24,4.48,specific computer architecture or
347.28,4.199,several architecture uh to host this
349.72,4.64,kind of workflow because the hardware
351.479,5.361,needs the communication needs the memory
354.36,4.679,the disk uh the speeds the latency
356.84,4.96,inside the computer are totally
359.039,5.28,different from a graphics application to
361.8,4.839,a AI training application to a computer
364.319,6.561,vision or to recommendation system like
366.639,6.521,this one so sometimes we need to change
370.88,6.039,a lot of those components to be able to
373.16,8.2,deliver the most valuable uh SDK or2 to
376.919,6.961,our uh developers that our end users so
381.36,6.64,this is how we've built more than 150
383.88,7.159,sdks over the past uh 15 years uh and
388.0,6.96,basically when when full stack of this
391.039,7.761,is done we open everything so uh the
394.96,7.359,hardware you buy from oems like Lenovo
398.8,7.0,uh Dell HP super micro and others and
402.319,6.761,all the software we give it away to the
405.8,5.119,um to our uh software ecosystems so it
409.08,4.399,could be startups it could be individual
410.919,4.201,developers a software could be uh for
413.479,3.641,free or they could be open source which
415.12,5.639,is the case of the the most important
417.12,6.04,ones okay this is how any viia growth to
420.759,5.28,the size we are today we growth through
423.16,5.439,our ecosystems developers are extremely
426.039,4.081,important for us today in Nidia has more
428.599,4.521,software Engineers than Hardware
430.12,5.479,Engineers just to illustrate uh this so
433.12,5.24,without this ecosystems of oems
435.599,6.081,developers startups and software
438.36,6.0,Partners uh we wouldn't be able to do
441.68,5.079,that much of innovation over the past uh
444.36,5.959,the kides I will start this talk talking
446.759,6.961,about one uh of uh our fr works that is
450.319,5.121,focused on uh data science specifically
453.72,4.28,and it doesn't matter what you do with
455.44,4.879,artificial intelligence at some point
458.0,4.759,you need to process a huge volume a huge
460.319,4.84,amount of data and this is why we
462.759,4.481,develop Rapids Rapids is an open source
465.159,5.88,project you can access more information
467.24,6.679,here uh rapids. Ai and you can find a
471.039,5.641,lot of resources to learn to explore and
473.919,5.801,even to run Rapids on cloud providers
476.68,6.079,with free tire accounts to access GP us
479.72,5.72,there and we develop Rapids to address
482.759,4.921,those three issues First Data science
485.44,4.479,and data manipulation when the volum is
487.68,5.0,huge it's time
489.919,5.081,consuming It's associated with the costs
492.68,4.16,not only Hardware costs but also people
495.0,4.599,costs because sometimes you as a
496.84,4.88,developer put a network to train or put
499.599,5.241,a a machine learning model to train you
501.72,5.52,need to wait like 10 hours 15 hours and
504.84,5.44,then when you realize that I forgot this
507.24,5.52,parameter I could do this adjustment to
510.28,5.08,the data set and here we go again and
512.76,5.199,wait another 15 hours and it is
515.36,4.2,frustrating for everyone uh working on
517.959,4.32,those projects from the the data
519.56,4.359,scientists to the developers and to the
522.279,4.0,customers and people that are there
523.919,4.881,waiting to see the value of what they're
526.279,5.921,investing so to address those things uh
528.8,6.039,we develop uh Rapids and ifia has
532.2,5.199,solution for anything uh related to data
534.839,4.801,size to be accelerated on our hardware
537.399,4.0,and Rapid is is focus here f focused
539.64,5.6,here on analytics machine learning
541.399,6.081,streaming and uh visualization we all
545.24,5.599,that work with data science we learn a
547.48,7.12,data science basically using p dat such
550.839,6.921,as P data uh uh Library such as pandas
554.6,5.479,psychic learn numai sci-fi and others
557.76,4.639,they work very fine uh they are very
560.079,5.161,easy to work with basically but they
562.399,5.201,were developed to run on what I used to
565.24,6.159,call generic Hardware which means they
567.6,6.88,are meant to run on CPUs basically and
571.399,4.88,everything is uh done and built for that
574.48,4.16,kind of uh
576.279,4.68,environment what we did is that we
578.64,5.0,didn't reinvented the wheel we created a
580.959,5.361,set of libraries that has semantic and
583.64,6.6,synthatic compatibility in most of the
586.32,7.28,cases with those existing um python
590.24,6.12,libraries so we have for instance qdf
593.6,6.88,and qao that can do almost the same of
596.36,6.28,bundas qml qph and several others and
600.48,4.76,the cool thing about it is everything is
602.64,6.16,based on Apache arrow that is basically
605.24,5.2,a way of arrange data in a columnar way
608.8,4.279,that makes a lot of sense when we are
610.44,4.839,talking about uh highly distributed and
613.079,5.401,parallel uh algorithms which is
615.279,6.521,basically what I do inside the the GPU
618.48,5.96,this is really important because uh if I
621.8,6.76,if I need GPU acceleration on a part of
624.44,7.28,my data science o aai pipeline I used to
628.56,6.32,pay I want big price every time I need
631.72,6.559,to take the data from the Run memory and
634.88,6.44,put it inside the Run memory uh of the
638.279,5.36,GPU this is the latency that I have
641.32,6.04,basically due to physical constraints on
643.639,6.681,the servers and computers and sometimes
647.36,6.12,if I use uh GPU just for one part of
650.32,6.079,this pipeline I would take more time
653.48,5.68,bringing thata or moving data back and
656.399,5.401,uh uh back and and forth instead of
659.16,5.799,processing so the ideal scenario when
661.8,6.24,using acceleration like a GPU is that
664.959,5.721,you run almost the biggest part of your
668.04,6.12,algorithm or your pipeline uh without
670.68,5.399,leaving the GPU ideally I just bring the
674.16,4.799,data on the beginning of the processing
676.079,5.44,I do the whole Pipeline on the GPU and I
678.959,6.081,return it and this is uh what we do with
681.519,5.32,Rapids integrated with aach arrow to
685.04,4.479,start with Rapids you need just a
686.839,6.361,computer with an Nvidia
689.519,7.281,uh GPU can be something like a gForce 10
693.2,6.079,something will be uh able to to run uh
696.8,6.279,Rapids uh and then you will start with
699.279,6.081,the code you already have uh on Pi data
703.079,4.76,the first thing to do is to migrate this
705.36,5.64,code to Rapids and then you'll be able
707.839,5.68,to see uh a very significant speed up on
711.0,5.2,your code and check that it's been
713.519,4.241,really really uh distributed over uh all
716.2,4.639,the Computer Resources we have on the
717.76,5.879,GPU but the real magic happens when we
720.839,6.24,associate Rapids with dusk that is
723.639,5.601,another open-source um software and what
727.079,4.641,desk does is basically he'll get the
729.24,5.0,Rapids code and he'll distribute it
731.72,4.799,automatically through multiple gpus on
734.24,6.2,the single node or through multiple
736.519,6.361,nodes with multiple gpus in a cluster so
740.44,4.839,without knowing all the details about
742.88,5.04,synchronization Distributing Computing
745.279,5.041,and complex things like that you can
747.92,5.919,start from a item code that we already
750.32,6.0,have accelerated using Rapids on the GPU
753.839,4.961,and then really accelerating it through
756.32,6.079,clusters and massive parallelism using
758.8,6.64,desk it's really really uh easy to do uh
762.399,4.8,and everyone can you know scale uh their
765.44,4.16,processing and to scale this kind of
767.199,4.88,processing it's really important because
769.6,4.4,uh typically we start data science uh
772.079,4.76,workloads and projects with a certain
774.0,5.24,amount of data and this amount of data
776.839,5.44,uh typically will grow a lot over the uh
779.24,5.719,the next years or months so assuring
782.279,4.881,that I just need to add more nodes to
784.959,4.761,increase my computing power without need
787.16,3.799,to rewrite all my code it's really
789.72,3.96,important to
790.959,4.88,scalability also Rapids can integrate
793.68,5.68,with other deep learning tools uh
795.839,7.201,through a Apache um arrow that I
799.36,6.279,explained so with that we can really
803.04,6.52,optimize the data sharing inside the GPU
805.639,6.88,memory and we are able to really do uh a
809.56,5.079,very good uh speed up on our pipeline
812.519,5.081,because most of all the pipeline are
814.639,6.081,been running uh inside the GPU so with
817.6,5.239,Rapids we leave a scenario where I spend
820.72,4.359,a lot of time just waiting for the code
822.839,5.44,to run and then discovering that I
825.079,5.521,mythed thingss and so on uh to a a space
828.279,5.48,where I'm really productive and building
830.6,7.12,more I know cases of training that used
833.759,7.0,to take like 15 hours uh reduce it to 15
837.72,5.72,minutes uh using uh Rapids and there are
840.759,5.08,several cases documented all over the
843.44,5.28,Internet about it just to give you an
845.839,6.68,idea who is using and adopt Rapids today
848.72,6.2,basically 25% of the foron 100 companies
852.519,4.641,are already using Rapids we have more
854.92,5.08,than a 100 open source and Commercial
857.16,5.4,software Integrations with Rapids and
860.0,5.279,much much more than 350 contributors on
862.56,5.24,geub those are some logos of companies
865.279,4.12,that are using Rapids and we really have
867.8,4.2,a big community
869.399,4.321,around it one of the case that I really
872.0,5.0,would like to highlight is the usage
873.72,6.679,Walmart does with hits basically they
877.0,6.12,used to process a lot of data and one of
880.399,5.041,the things they achieved it's a 1.7
883.12,5.159,percentage Point Improvement in forecast
885.44,6.6,accuracy on what will sell on each one
888.279,6.641,of the uh North American USA uh stores
892.04,6.12,so with Rapids they were able to deliver
894.92,5.32,100 faster fit engineering and 20 time
898.16,4.72,faster model training within Rapids
900.24,5.44,that's Con X boost so we are talking
902.88,5.8,here about a huge amount of data and the
905.68,5.68,result of this kind of processing has a
908.68,7.279,big impact economically speaking on
911.36,7.24,companies and uh organizations such as
915.959,4.521,wmart after talking about data science I
918.6,3.56,will start talking about deep learning
920.48,4.68,and the first framework that I would
922.16,5.84,like to present you uh on Nvidia it's a
925.16,5.599,framework called a tow that is an acon
928.0,6.68,for train adapt and optimize and this is
930.759,6.241,exactly what to does so creating an AI
934.68,5.639,application uh can be hard and complex
937.0,6.079,and basically we start uh getting uh
940.319,4.88,open source models from model Zool we
943.079,4.801,have all over internet those models are
945.199,5.76,okay but once I need to run this model
947.88,6.04,on a GPU to get acceleration I need to
950.959,5.041,do a lot of adjustment on the model and
953.92,4.2,I need to do a lot of pre trining on
956.0,3.399,this model to get it really optimized
958.12,3.399,for the
959.399,4.88,this could take a lot of time efforts
961.519,6.201,and Investments to do so what I did in
964.279,6.321,Nvidia is that we now offer for free
967.72,5.28,more than a 100 combinations of models
970.6,5.599,that could go from generic things such
973.0,5.8,as image classification object detection
976.199,5.481,segmentation they are all based on the
978.8,6.159,key and most important architecture used
981.68,6.0,today for neural networks and we
984.959,4.761,optimized those models uh not only in
987.68,5.2,the structure but also on everything
989.72,5.72,else related to do the best performance
992.88,5.84,possible on a GPU we give those models
995.44,6.319,for free but we also trained some models
998.72,5.799,to achieve specific tax tasks such as
1001.759,5.481,people detection gaze detection facial
1004.519,5.641,landmarks vehicle classifications uh
1007.24,5.719,automatic speak recognition and several
1010.16,5.52,others so when you start a project you
1012.959,4.481,don't start with a generic model you
1015.68,5.839,start with a model that is already
1017.44,7.72,optimized for the gpus then you use uh
1021.519,5.4,tto kit to train adapt and optimize this
1025.16,4.08,model basically you do the transfer
1026.919,5.16,learning and train the model based on
1029.24,6.0,your existing data set and the good
1032.079,5.561,thing of having this model inside tow is
1035.24,5.4,that your production model can be
1037.64,5.84,generated according to the GPU that we
1040.64,5.84,use uh for the inference so imagine that
1043.48,5.48,I have a model to recognize people on a
1046.48,5.36,people County solution that I would run
1048.96,6.76,uh on a shopping mall uh on my customer
1051.84,6.079,a I need to run the analytics uh on a
1055.72,4.64,embedded device that will be close to
1057.919,5.361,the camera because I don't have
1060.36,5.439,bandwidth to connect to the cloud so in
1063.28,5.16,this case the model needs to be really
1065.799,5.76,optimized to run on an embeded device
1068.44,4.68,such as Jackson but on the other
1071.559,3.961,customers they have their own data
1073.12,4.52,center inside uh the shopping mall so I
1075.52,4.279,can put a server there and then I will
1077.64,4.96,use a different GPU on the server so I
1079.799,6.201,can optimize for this GPU on my third
1082.6,5.36,client uh my Third customer they have a
1086.0,5.36,very good infrastructure on a cloud
1087.96,6.0,provider so I just stream the data the
1091.36,6.199,video and process everything uh at the
1093.96,6.68,cloud so I can also generate the best
1097.559,6.201,optimized model to this environment so
1100.64,5.68,using to you assur that you are starting
1103.76,5.039,from the best reference model uh train
1106.32,5.479,it and optimize it for GPU and some of
1108.799,5.801,"those models were trained like 100,000"
1111.799,5.201,hours on clusters of GPU just to
1114.6,4.8,illustrate the amount of effort we put
1117.0,4.64,on this so we start with a model really
1119.4,4.24,optimize it you do the transfer learning
1121.64,4.36,and then you can generate the model uh
1123.64,3.84,to be consumed by your application uh
1126.0,4.96,according to the hardware that you're
1127.48,6.12,going to use and as I said edge to Cloud
1130.96,5.52,those are the the key Frameworks that we
1133.6,5.4,have for inference dpstream uh Riva and
1136.48,4.24,Triton I will talk about some of those
1139.0,5.48,uh in a few
1140.72,6.28,minutes to has also uh characteristics
1144.48,4.92,uh of doing training optimization so it
1147.0,5.2,can runs on a multi node multi GPU node
1149.4,5.08,and multi GPU with multiple gpus it
1152.2,5.359,works uh it works with automatic mix at
1154.48,6.84,precision and on inference it can also
1157.559,6.0,uh do the printing which is removing uh
1161.32,4.32,nodes from the neural networks that are
1163.559,4.841,not being activated by this particular
1165.64,5.399,training and also quantization that is
1168.4,4.72,basically adjusting uh the weights of
1171.039,4.441,the network according to the hardware
1173.12,4.72,that we process it when you use those
1175.48,5.8,two things in combination we can achieve
1177.84,6.64,up to four time speed up for inference
1181.28,5.399,which means that we can uh really uh
1184.48,6.199,have very very well
1186.679,5.801,optimized uh applications one of uh
1190.679,3.761,characteristics of to that I really
1192.48,4.88,enjoy because I work a lot with computer
1194.44,5.32,vision uh is that to can do data
1197.36,4.4,augmentation on online so data
1199.76,4.12,augmentation on computer vision is
1201.76,5.36,basically a technique where I have a
1203.88,5.64,very limited data set I can't have more
1207.12,4.2,images on this data set so I do
1209.52,4.88,something that I Ed to joke as the
1211.32,6.8,miracle of multiplying images where we
1214.4,6.08,just uh apply some uh image algorithms
1218.12,4.72,to generate four or five image based on
1220.48,4.52,a single one this could be rotating
1222.84,4.719,adjusting brightness and several other
1225.0,4.88,uh manipulations that we do but when we
1227.559,4.681,do this on the traditional way I need to
1229.88,5.08,preprocess my data set and I need to
1232.24,5.4,have a lot of space because my data set
1234.96,5.199,will become really huge what we can do
1237.64,6.24,with to is that once the image is on the
1240.159,6.481,GPU memory it's very very easy to apply
1243.88,5.159,those image uh algorithms to do data
1246.64,4.8,augmentation and we can do this during
1249.039,6.681,training which mean I don't need to
1251.44,7.599,expand my existing data sets I just use
1255.72,5.6,this feature from to and online it to uh
1259.039,5.161,generate other images and here we have
1261.32,5.88,some numbers as you can see 100 images
1264.2,8.12,experiment with 20x augmentation we
1267.2,7.719,really achieve uh a lot of uh accuracy
1272.32,4.8,uh on both uh scenarios so if you work
1274.919,5.321,with computer vision please take a look
1277.12,5.96,on this feature for tow some examples
1280.24,5.439,and use cases of to uh it's like this
1283.08,5.64,one I will start from a ponet model that
1285.679,4.961,is a model trained on RGB cameras
1288.72,5.4,and but I need on my solution to
1290.64,5.919,identify people on infrared cameras so
1294.12,5.24,basically what I do I start from the
1296.559,5.401,optimized ponet model I will do transfer
1299.36,5.88,learning with my annotated uh images
1301.96,5.52,from thermal cameras and I will have a
1305.24,5.36,very good result and a working uh
1307.48,6.199,solution in a few
1310.6,4.84,minutes also I can do things much more
1313.679,5.24,complicated on this case I need to have
1315.44,5.88,a data set where I recognize helmet
1318.919,4.481,uh and and people and heads so I don't
1321.32,3.88,have a data set with all those things I
1323.4,4.399,would start from the same people net I
1325.2,5.719,mentioned before I'll use a labeled
1327.799,6.441,helmet data set and I'll generate a data
1330.919,5.841,set with annotations of people and face
1334.24,5.559,then I combine both things and I have my
1336.76,6.84,final uh neural network trained as you
1339.799,6.721,can see I can achieve over 80% of aange
1343.6,4.92,class Precision after 100 a books that
1346.52,5.0,is a really short training when we're
1348.52,5.639,talking about uh deep learning the
1351.52,6.36,newest versions of tow has some very
1354.159,7.0,interesting tools the first one uh is to
1357.88,4.799,help you to do segmentation masks uh
1361.159,3.801,just to illustrate when you need to
1362.679,5.0,segment images you need basically to
1364.96,5.56,paint the pixels that are part of a
1367.679,5.561,special class in this case uh I need to
1370.52,5.12,paint all the pixels that are part of uh
1373.24,4.88,bottles uh but with this Auto labeling
1375.64,6.039,tool basically what you can do is is
1378.12,6.559,that we can turn uh bounding boxes on
1381.679,5.281,images on segmentation masks uh the
1384.679,4.521,other thing we are also providing a
1386.96,4.04,vision Transformer models that are
1389.2,4.719,revolutionizing everything we do in
1391.0,5.08,computer vision uh basically uh Vision
1393.919,4.921,Transformer is the usage of the
1396.08,5.36,Transformers technology from natural
1398.84,4.959,language processing and llms to computer
1401.44,4.2,vision this is really interesting it's
1403.799,4.961,open source and you can apply it on
1405.64,6.24,different type of uh platforms Hardware
1408.76,6.08,we have tools for automl which uh will
1411.88,5.159,help you to identify the best uh
1414.84,5.56,hyperparameters for optimize your
1417.039,6.561,training uh it can be integrated with uh
1420.4,6.04,services using rest apis so you can
1423.6,4.64,build a whole solution around uh Tow and
1426.44,4.96,as I mentioned that when you use burning
1428.24,7.24,and quantization we can achieve up to 4X
1431.4,6.68,increase uh of performance during
1435.48,6.0,inference similar from to that we
1438.08,6.76,basically use for uh several different
1441.48,6.6,types of uh neuron networks we have Nemo
1444.84,5.88,that is a specifically for training llms
1448.08,4.36,large language models just to illustrate
1450.72,5.64,the complexity of this kind of
1452.44,6.32,processing in 2018 the stateof art model
1456.36,4.84,that we have had around 94 million
1458.76,6.12,parameters we are talking today in
1461.2,8.4,models with much more than 53 billion
1464.88,7.039,parameters gpt3 has 175 billion
1469.6,4.6,parameters so we're talking about huge
1471.919,4.561,models and the training of those models
1474.2,5.079,is really complex because we need to
1476.48,5.6,have data parallelism we need to use
1479.279,5.52,thousands of gpus this will be
1482.08,4.68,distributed on a cluster with a lot of
1484.799,5.521,computing power and Nemo is the
1486.76,6.2,framework uh developed exactly for those
1490.32,4.8,use cases we can do with Nemo the data
1492.96,4.56,coration distributed training and
1495.12,4.48,accelerated inference it has the key
1497.52,4.96,feat features to help you to build and
1499.6,6.24,to customize your llms large large
1502.48,5.76,language models uh projects and we also
1505.84,5.0,offer uh Nemo guard raos that is open
1508.24,5.76,source and guard raos it's a very
1510.84,5.559,important part basically on chatbots
1514.0,4.44,because they will be uh the the
1516.399,5.241,boundaries or they will limit the
1518.44,6.68,application to answer on a specific
1521.64,6.36,topics uh this means uh safety for the
1525.12,5.36,users and prevent hallucinations and
1528.0,6.36,executing third party calls uh and
1530.48,7.24,increase the security uh if someone ask
1534.36,6.24,some funny questions to chat GPT ever
1537.72,6.16,and you have say hey I can't answer this
1540.6,5.24,this because this and that uh it's the
1543.88,4.6,guard rail that are working I used to
1545.84,5.439,say that if I went to chat TPT without
1548.48,6.52,guard raos and say hey I have uh you
1551.279,6.441,know those those uh drugs in my home
1555.0,4.679,that I bought from drugstore nearby uh
1557.72,4.319,which ones I need to take to you know do
1559.679,5.48,something bad to myself he'll be able to
1562.039,5.921,answer this but AAR said hey we cannot
1565.159,5.88,answer and I believe you got the points
1567.96,5.88,so memo guard raos is also available
1571.039,5.161,open source as Nemo uh you can use it uh
1573.84,4.8,and also contribute uh to build our
1576.2,5.839,large language model uh
1578.64,5.8,projects uh hia it's one framework that
1582.039,5.161,we developed to do three things
1584.44,6.0,automatic speak recognition neuro uh
1587.2,7.079,machine translation and text to speeech
1590.44,6.359,those are three very complex tasks on AI
1594.279,5.52,because you need to do a lot of steps on
1596.799,5.801,the pipeline of processing we build Riva
1599.799,5.201,to do exactly that so it's much easier
1602.6,4.679,to to talk about Reva explaining the use
1605.0,5.279,cases in this case I have a customer
1607.279,5.12,talking to a contact center I have Reva
1610.279,5.4,listening to the customer converting
1612.399,5.561,this to text using NLP and several other
1615.679,5.12,Technologies to print real time
1617.96,5.12,recommendations on the contact center uh
1620.799,4.041,agent screen and allowing him and
1623.08,4.4,helping him to do the right
1624.84,5.079,recommendations to the customers here
1627.48,4.559,it's another scenario that we you know
1629.919,5.921,get used on the on the pandemics and
1632.039,6.36,also on several different uh tools that
1635.84,4.959,we have of videos online where we can do
1638.399,4.801,automatic transcription we can do
1640.799,4.401,automatic translation we can do
1643.2,5.04,summarization and everything else just
1645.2,6.839,Bas it on video this is where we uh
1648.24,7.0,really can be us it and when we have a a
1652.039,6.401,a full round applications is where the
1655.24,5.08,customers is interacting totally with
1658.44,4.64,computers he's not talking up with
1660.32,5.599,humans anymore and but he feels that
1663.08,5.24,he's being uh very well treated in this
1665.919,5.36,case uh it's a project from Nidia called
1668.32,7.0,it name it Tokyo that is a talking pusk
1671.279,6.24,where you go there uh on a on on uh a
1675.32,4.32,fast food restaurant and you start talk
1677.519,4.52,talking to this small robot he will give
1679.64,5.36,you suggestions he will recognize you
1682.039,5.681,and it will be a very uh natural way of
1685.0,5.559,speak just to have you an idea there's a
1687.72,6.36,lot of drive-throughs uh today all over
1690.559,5.36,the world that are changing uh uh humans
1694.08,4.079,on the on the other side of the
1695.919,5.201,microphone for Technologies like this
1698.159,4.801,and it's very adaptable and able to use
1701.12,4.159,on those kind of environments imagine
1702.96,5.04,that you have a lot of noise you have
1705.279,4.721,different audio qualities and r can
1708.0,6.039,handles uh all
1710.0,6.84,this uh he I use uh pretrained models
1714.039,5.48,the same way uh that we talking about to
1716.84,4.439,you can use tow to retrain those models
1719.519,4.16,and if you need to expand the model or
1721.279,5.361,do something more complex you can use
1723.679,5.681,Nemo uh then you can Implement all the
1726.64,5.759,three key tasks uh using Reva and
1729.36,6.24,Implement uh your end application uh
1732.399,5.88,using this uh technology uh just to
1735.6,5.88,explain how we evoluted uh or reg growth
1738.279,7.801,over the past years uh in 2018 we have
1741.48,7.64,like 45% error rates on automatic speak
1746.08,5.52,recognition uh two years ago it was uh
1749.12,5.439,less than 10 and it's still getting
1751.6,6.36,reduced and also uh we can have a lot of
1754.559,6.201,accuracy when we customize uh the models
1757.96,5.559,because if I'm working with a model that
1760.76,6.799,will uh be used to have conversations
1763.519,6.921,with uh uh Banks customers uh Financial
1767.559,5.081,action customers this vocabulary will be
1770.44,4.92,totally different of the vocabulary of a
1772.64,6.72,person asking for a hamburger on a fast
1775.36,7.48,food so with Riva we can adjust uh the
1779.36,7.4,whole model to be very specific to one
1782.84,6.6,kind of jargon or dialect or specific
1786.76,6.08,set of words this will increase a lot
1789.44,4.479,the accuracy so Riva is our SDK for
1792.84,4.52,doing
1793.919,7.161,this Triton is what I used to call the
1797.36,7.439,Swiss army knife of uh inference on AI
1801.08,7.36,today uh when we use an AI based
1804.799,8.281,application our application is talking
1808.44,7.16,to an inference server and if I write my
1813.08,5.76,application to one specific inference
1815.6,7.72,server let's say py torch tensorflow or
1818.84,7.16,any other and tomorrow my Ai and and
1823.32,5.32,data scientists discover that there is a
1826.0,4.6,new model based it on other framework
1828.64,4.759,other inference server that is
1830.6,5.52,performing best to my solution I will
1833.399,5.321,need to write the end user application
1836.12,4.6,again it doesn't mean if this end users
1838.72,4.36,application it's on a cell phone
1840.72,5.24,directly talking to an inference server
1843.08,5.76,or if this application uh it's let's say
1845.96,5.88,a Java application that is interfaced
1848.84,5.64,during the the the interface between all
1851.84,5.839,the AI pipelines and the end user
1854.48,5.84,applications that can be a lot so uh
1857.679,4.561,Tron is key in this kind of scenario
1860.32,5.599,because you write your application to
1862.24,7.72,talk to Tron and Tron can talk to all
1865.919,7.081,other uh uh major uh backends and
1869.96,6.599,Frameworks for inference so let's say
1873.0,5.799,Tor flow tensor RT open Vino P torch
1876.559,4.641,Onyx XG boots several others so
1878.799,5.441,basically your application is really to
1881.2,5.479,Tron and then today my data science
1884.24,4.399,scientists said hey this is the best
1886.679,5.161,model we have have it need to run on
1888.639,5.0,things orlow okay tomorrow or the six
1891.84,4.64,months they say no now we have a model
1893.639,5.121,on pytorch I just change the model on
1896.48,4.559,Tron configuration I don't need to write
1898.76,6.36,the application again uh Tron can
1901.039,6.401,communicate using HTTP grpc and C++ uh
1905.12,4.559,you can do inference Distributing on the
1907.44,5.359,different kind of Hardware you have uh
1909.679,5.96,it support from uh bare metal to
1912.799,5.041,virtualization Edge to cloud different
1915.639,4.52,types of queries uh that are typical
1917.84,5.799,from different types of applications
1920.159,6.681,real time badge streams and mixed models
1923.639,5.481,and it also totally uh develop it
1926.84,4.6,focusing on maximized Hardware
1929.12,6.279,utilization which means reducing the
1931.44,8.199,total cost uh of operations of your uh
1935.399,6.24,solution um it also has integration with
1939.639,4.28,kubernetes uh with its scalable
1941.639,6.16,microservices it's very easy we provide
1943.919,7.041,this uh uh although things uh for for
1947.799,5.84,triton uh is integrated with the key
1950.96,4.52,practices and also some tools of machine
1953.639,5.681,learn so basically with all the tools
1955.48,7.079,from cloud provider aure uh AWS Google
1959.32,5.88,and so on uh it support AO live model
1962.559,4.281,updates uh Dynamic model loading which
1965.2,4.4,is really interesting because I just
1966.84,4.839,load the models that I will use on a
1969.6,3.76,particular transaction and several other
1971.679,4.24,tools that will help you to optimize
1973.36,5.76,your operation and mlops guys will love
1975.919,4.321,it and also uh we have it's all open
1979.12,3.88,source and
1980.24,6.12,customizable uh you can change it uh to
1983.0,6.519,adjust your needs uh this is an idea of
1986.36,5.64,all the uh Frameworks formats and models
1989.519,5.16,it can has on on execution backends this
1992.0,6.12,list is growing more and more it's very
1994.679,6.161,very flexible and in a nutshell Triton
1998.12,5.519,is what put together everyone working
2000.84,5.679,with AI applications and also end user
2003.639,6.16,applications so for data science we can
2006.519,7.961,use any framework uh for people working
2009.799,7.201,with uh maintaining those uh models uh
2014.48,5.079,working on inference uh the models can
2017.0,6.44,be on any repository and it can be
2019.559,6.6,deployed on any platform so Tron is what
2023.44,6.479,really brings you flexibility to work
2026.159,6.0,with all those um all those kind of uh
2029.919,4.441,AI
2032.159,5.52,applications the next framework that I
2034.36,6.319,would like to talk uh is Isaac because
2037.679,5.281,Robotics are really growing today we are
2040.679,4.801,having robotics uh boosting every
2042.96,5.6,industry uh I know that in Europe we
2045.48,7.04,have cases of automatic delivery even on
2048.56,5.92,small stores in small small cities uh
2052.52,4.04,and also we have uh this kind of
2054.48,5.32,applications on different areas retail
2056.56,5.88,Logistics manufacturing and so on but to
2059.8,5.039,build AI in robotics at scale it's
2062.44,4.52,really hard because it's hard to develop
2064.839,4.08,here we are talking about developing the
2066.96,5.08,software the AI and the hardware in
2068.919,5.801,parallel it's hard to test and it's
2072.04,5.639,really really hard to deploy so this is
2074.72,5.399,why we created uh the ISAC robotics
2077.679,6.121,platform that is basically a platform
2080.119,6.48,made by a simulation component a key SDK
2083.8,6.599,training component and a deployment uh
2086.599,7.28,part so to start the development with
2090.399,6.24,Isaac I start with a simulator where I
2093.879,5.801,will import all the projects that I have
2096.639,5.44,for for my physical robot I will create
2099.68,5.159,a virtual word where this robot will
2102.079,6.0,operate I connect it with virtual
2104.839,5.801,sensors and actuators and I'm able to
2108.079,5.201,start programming and training all the
2110.64,6.6,AI inside this virtual
2113.28,7.28,words when I'm done with the training I
2117.24,5.96,use the Isaac SDK to develop all this I
2120.56,5.6,just need to move the code from the
2123.2,6.68,training with Isaac SDK to the physical
2126.16,6.16,robot so this whole cycle really really
2129.88,5.719,speed UPS the development and it's very
2132.32,6.12,useful in situations such as uh one case
2135.599,4.881,one very good case that we have with BMW
2138.44,4.6,that they need to train robots to do
2140.48,5.28,pilet uh movement on a very specific
2143.04,4.92,area of the factory they couldn't stop
2145.76,5.079,the factory just to train the robot on
2147.96,5.6,that area so they use it lighter and
2150.839,5.681,photogrametry they created a digital
2153.56,4.92,twin of that area they imported the
2156.52,5.24,project projects of the robot they going
2158.48,6.839,to use from the the the supplier they
2161.76,7.12,trained the robot inside this uh digital
2165.319,6.76,TN on several several several thousands
2168.88,5.12,of hours and when they are okay with the
2172.079,4.801,results they just validated on the
2174.0,6.24,reward and for the robot that was traded
2176.88,5.479,with photogrametry and real 3D images it
2180.24,4.72,doesn't matter uh if he is on the
2182.359,5.48,virtual play environment or if it's on
2184.96,4.76,the real world so Isaac is very very
2187.839,5.24,useful for
2189.72,5.68,that uh the other framework that I think
2193.079,4.24,it's very important for NVIDIA uh it's
2195.4,4.88,part of a bigger platform actually Nam
2197.319,5.361,it Metropolis Metropolis is a platform
2200.28,4.52,for us to build smart spaces and
2202.68,5.84,infrastructure uh when we see what is
2204.8,5.68,happening with AI today uh real useful
2208.52,4.64,AI Solutions computer vision solutions
2210.48,5.2,for the the the existing word the real
2213.16,4.32,world are not developed by one single
2215.68,4.24,neuron network but but for the
2217.48,5.599,concatenation of the results of
2219.92,5.439,processing of a lot of different kinds
2223.079,4.401,of networks on the same system it
2225.359,5.48,doesn't matter if it's smart cities
2227.48,7.08,factories public spaces retail anything
2230.839,6.401,uh you use you need a platform to really
2234.56,5.44,cross information for different kinds of
2237.24,5.48,uh neural networks this is one example
2240.0,5.72,app developed by one of Nvidia Partners
2242.72,5.28,using the Metropolis platform uh here we
2245.72,5.76,can see that they analyzing images
2248.0,6.119,coming from drones fixed cameras uh and
2251.48,6.16,several different types of cameras they
2254.119,5.801,are measuring un Counting Cars people uh
2257.64,5.28,bikes and several other kind of things
2259.92,6.84,moving in a city uh this two uh that
2262.92,6.56,they develop all allow the the operator
2266.76,5.68,to configure and to parameterize any
2269.48,6.2,kind of alerts uh he really uh wants to
2272.44,5.48,have and as you can see here we have
2275.68,5.72,hundreds of neuron networks working in
2277.92,5.36,parallel and working giving information
2281.4,4.56,to a single platform that is able to
2283.28,6.16,consolidate everything this is exactly
2285.96,5.96,what Metropolis is it's a platform a
2289.44,5.04,framework for smart sensors where you
2291.92,5.04,can use and this is being used actually
2294.48,4.68,on retail traffic management Logistics
2296.96,4.6,Healthcare manufacturing factories
2299.16,5.0,several other applications under the
2301.56,5.24,hood what I have are those pretrained
2304.16,5.36,models that I I mentioned to you the
2306.8,4.92,beginning of my talk uh we also have tto
2309.52,4.16,kit that now you already know what it is
2311.72,4.44,and deepstream what I'm going to explain
2313.68,5.88,to you by the way everything that I'm
2316.16,6.0,talking here is available through uh one
2319.56,8.279,website that is NG
2322.16,8.4,gc. nvidia.com on NGC we have Mo
2327.839,6.28,pre-trained models we have containers
2330.56,7.039,for all Nidia sdks and the coolest part
2334.119,6.561,we have containers for uh other SD Cas
2337.599,6.24,such as tensor flow P torch with
2340.68,5.679,everything optimized for the GPU so all
2343.839,5.081,you need to have uh a tensor flow
2346.359,5.24,running with everything optimized for an
2348.92,4.8,Nidia GPU is to run one of those
2351.599,3.881,containers if you are using a cloud
2353.72,4.76,provider some of the containers you're
2355.48,5.08,going to find inside uh the marketplaces
2358.48,4.879,and also you're going to find uh virtual
2360.56,4.84,machine images from Nvidia uh on those
2363.359,4.48,places that can be used to launch the
2365.4,4.88,containers and to to put the containers
2367.839,6.48,directly from uh
2370.28,6.88,NGC computer vision is much more than AI
2374.319,5.601,inference because to every
2377.16,4.6,frame sorry we need to capture and the
2379.92,5.0,code the
2381.76,5.28,data we need to preprocess the image and
2384.92,4.96,sometimes do the batching then we do
2387.04,5.64,inference tracking to not count the same
2389.88,5.08,object twice for instance send this
2392.68,5.2,metadata extracted to other systems
2394.96,6.52,finally made in a composition
2397.88,6.28,typically uh developers use a GPU
2401.48,4.879,acceleration just here and they do all
2404.16,5.72,the other parts using things like open
2406.359,6.041,CVV running on CPU and then we got on
2409.88,5.439,that situation where I have a lot of
2412.4,6.04,data back and forth from the computer
2415.319,5.481,run memory to the GPU memory and I'm not
2418.44,4.8,using an optimized pipeline this is
2420.8,6.2,exactly what we can do when we use
2423.24,6.839,dpstream dpstream can use the best
2427.0,6.68,acceleration it possible on each parts
2430.079,7.321,of this processing it supports uh ex run
2433.68,7.96,from Edge to cloud and it's very uh
2437.4,6.88,flexible and why this is so fast because
2441.64,5.0,we just get the data on the beginning of
2444.28,4.68,the pipeline and then we process
2446.64,5.08,everything almost with zero memory
2448.96,4.28,copies so the buffer is the same we just
2451.72,3.639,passed to the other part of the
2453.24,4.879,processing buffer to the same place on
2455.359,4.96,on the GP VI memory and this is the
2458.119,5.601,fastest way of processing this kind of
2460.319,6.52,Frameworks uh of workloads we have also
2463.72,5.8,uh the opportunity to get data right
2466.839,5.0,from the camera buffer using a a
2469.52,4.559,networking uh technology for NVIDIA
2471.839,5.48,called rer Max where I can put
2474.079,6.201,industrial camera I can extract the the
2477.319,6.921,the r stream from this camera directly
2480.28,8.48,to the GPU memory increasing even more
2484.24,6.96,uh the the performance of my solution uh
2488.76,5.88,we can use uh deepstream we can program
2491.2,5.8,deepstream using python using C++ but
2494.64,4.6,it's also a composer that is a tool
2497.0,5.88,where I can I will just drag and drop
2499.24,7.839,some blocks and build uh graphically my
2502.88,6.68,application uh on the end of this whole
2507.079,5.801,process I will generate a container and
2509.56,6.759,I can uh deploy it anywhere uh I will
2512.88,6.439,use the models that were P trained to to
2516.319,5.681,we can also import onx models to tow and
2519.319,5.241,optimize them to use this way and if I
2522.0,6.28,need to use any other model I can just
2524.56,6.4,use Tron with uh the right the proper
2528.28,5.48,inference server uh and dpstream can
2530.96,6.2,communicate uh with titon using a
2533.76,5.92,grpc uh the other cool thing is on I
2537.16,5.12,detected metadata from the video that
2539.68,6.28,I'm analyzing or the images I can send
2542.28,7.36,messages with this metadata to radies uh
2545.96,9.24,and qtt Kafka and several other uh Cloud
2549.64,7.919,providers uh I uh iot protocols okay so
2555.2,4.52,after I package this whole thing that I
2557.559,4.76,develop the whole solution I will have a
2559.72,4.359,container I can deploy anywhere for the
2562.319,5.441,people that work with computer vision
2564.079,6.161,and knows a little about it uh this is
2567.76,5.92,basically a set of plugins for G
2570.24,6.319,streamer so being a set of plugins it
2573.68,5.12,means that I can uh reuse a lot of of my
2576.559,4.76,code from Project to project sometimes
2578.8,4.519,just changing the type of networks if I
2581.319,4.201,need to count people on this application
2583.319,4.881,and cards on the other one basically
2585.52,4.88,what changes is the neuron Network I
2588.2,4.399,change the people detection to a car
2590.4,4.52,detection and the rest of my application
2592.599,5.48,the card of my application is already
2594.92,6.76,written and what we see in practice and
2598.079,7.561,I'm able to uh work with more than uh
2601.68,6.159,almost 700 startups in Latin America uh
2605.64,4.0,Direct and I'm seeing this in practice
2607.839,4.361,the the startups and the companies that
2609.64,5.439,adopt those Technologies speed up the
2612.2,4.919,development time up on 10 times because
2615.079,5.681,it's much more easy and productive to
2617.119,6.921,work uh with all those uh
2620.76,6.359,Frameworks Metropolis allows developers
2624.04,4.96,to develop things like uh smart spaces
2627.119,5.081,application where I have several
2629.0,5.839,different uh Technologies and uh neural
2632.2,5.76,networks and I can put all together on a
2634.839,5.801,single system to end my talk here I will
2637.96,7.84,talk about Omniverse that is New Era of
2640.64,7.52,collaboration and and uh and simulation
2645.8,4.6,uh virtual wordss are essential for the
2648.16,4.6,next area of innovation we truly believe
2650.4,4.52,that everything that we will exist on
2652.76,5.079,physical world we will exist on a
2654.92,6.0,virtual world first because it is the
2657.839,6.601,cheapest way to build the thing right
2660.92,6.6,for the first time uh we build and use
2664.44,6.919,Omniverse uh to our own application so
2667.52,7.2,we have AC uh that is uh the Avatar
2671.359,6.72,technology uh we use Omniverse for it we
2674.72,6.44,have uh digital twins of several cities
2678.079,6.441,where we use those digital twins to
2681.16,5.84,train uh anyia Drive Sim Technologies
2684.52,5.039,because inside the digital twin uh we
2687.0,5.68,can generate uh events that we can't in
2689.559,5.481,the real world so change the weather or
2692.68,5.52,cause collisions traffic and other
2695.04,5.279,conditions Isaac Sim uh that I mentioned
2698.2,5.2,before is already integrated with
2700.319,6.641,Omniverse so I can create digital twins
2703.4,6.4,of small stores up to huge places and
2706.96,4.84,also we are developing Earth 2 that is a
2709.8,3.88,hardware and software infrastructure
2711.8,4.08,that we're going to work with
2713.68,4.6,researchers uh and this is basically a
2715.88,5.64,digital twin of the planet Earth to
2718.28,5.079,study uh the climate changes impact and
2721.52,5.2,several other
2723.359,5.76,things uh Omniverse it's the tool for
2726.72,5.32,two basically scenarios the first one is
2729.119,5.96,3D creation and collaboration the second
2732.04,5.799,one is to create industrial uh digital
2735.079,5.401,Twins and to operate them when we're
2737.839,4.601,talking about uh 3D design it's really
2740.48,5.04,complicated because we are talking about
2742.44,5.639,of large teams with different skills
2745.52,4.76,which means different tools which means
2748.079,4.801,different kind of files to integrates to
2750.28,6.88,build a pipeline and we need more and
2752.88,6.64,more accuracy on uh on the results of 3D
2757.16,5.919,compositions so those workflows are
2759.52,5.68,rising uh in complexity uh we have a lot
2763.079,5.52,of challenges today people are working
2765.2,5.919,from different places uh in the earth uh
2768.599,5.041,sometimes we are talking about 3D data
2771.119,5.0,sets that can't be mov it from a place
2773.64,5.199,to the other because of their size and
2776.119,5.761,also uh it's really important have a
2778.839,6.601,single uh search of truth because most
2781.88,6.32,of this work is done uh linearly so this
2785.44,6.0,person here uh create an initial idea of
2788.2,5.04,a character uh and then this one uh make
2791.44,4.32,it more detailed this one put some
2793.24,4.48,polygonal and implement it this one put
2795.76,4.4,all the skeleto and the last one here
2797.72,5.04,start doing the animations imagine what
2800.16,5.36,happens if this guy get the wrong
2802.76,4.96,version of this one and they will just
2805.52,4.48,Discover it on the end when everything's
2807.72,5.52,built together so having a single sour
2810.0,5.96,of Truth is really important Omniverse
2813.24,5.0,can be uh has a what we call nucleus
2815.96,6.08,that is a central server where where
2818.24,6.079,everyone can work uh together using the
2822.04,4.6,key Technologies from Nvidia over the
2824.319,5.0,past decades and they can act and
2826.64,5.84,interacting real time so everyone can be
2829.319,5.481,on the same place virtual space uh each
2832.48,4.839,one doing its part of the work and
2834.8,4.36,everyone see what everyone else is doing
2837.319,5.441,and when we are talking about digital
2839.16,5.72,twins we can uh create the digital twin
2842.76,5.0,and we can operate this digital twin
2844.88,5.28,doing things like like uh robot training
2847.76,5.16,digital human training Factory planning
2850.16,5.959,operation simulations and this is all
2852.92,7.56,done basically on real world existing
2856.119,7.041,assets let's say uh the the cad uh files
2860.48,5.68,for the factory itself or for the robots
2863.16,5.439,that are operating there so basically we
2866.16,5.159,go from The Real World and we create
2868.599,5.281,everything on the on the virtual
2871.319,5.841,environment here we have an example of
2873.88,6.479,two two people working GA an Omniverse
2877.16,5.6,on this side I have a a person working
2880.359,4.801,uh creating an area that is this kitchen
2882.76,5.72,on a specific tool this is not Omniverse
2885.16,5.56,I think it's Revit and on the left side
2888.48,5.16,I have someone working with Omniverse
2890.72,6.8,create putting texture lighting and so
2893.64,6.679,on so as you can see uh every change one
2897.52,4.599,one uh professional does to his part
2900.319,4.0,it's automatically translated to the
2902.119,5.321,other one here we have someone working
2904.319,7.161,on the architecture of of this lightning
2907.44,6.2,stuff and uh someone else doing the
2911.48,4.96,lightning actually and illumination and
2913.64,5.64,all those things and we have rear here
2916.44,5.399,on the right side an image that let's
2919.28,4.36,say our art director can be seing and
2921.839,4.28,giving feedback on real time this is
2923.64,4.719,good this is bad change this change that
2926.119,4.96,change the lights change the structure
2928.359,5.641,almost in real time without Omniverse
2931.079,5.601,this is a back and forth uh linear
2934.0,5.72,process here we have three people
2936.68,6.24,working putting assets on a kitchen for
2939.72,5.839,one specific demo and here we have an
2942.92,5.159,arch director on a project director just
2945.559,6.401,giving feedback in real time so they can
2948.079,8.681,see uh the changes or other um people
2951.96,7.28,are doing those are the basic uh uh
2956.76,4.96,components that is part of Omniverse
2959.24,4.16,nucleus that is a central repository
2961.72,4.04,connect that allows to connect to
2963.4,4.56,several other applications kit that are
2965.76,5.079,a set of res reference applications
2967.96,5.599,basically in Python uh to allow you to
2970.839,6.0,interact and to use all these simulation
2973.559,5.28,Technologies and also RTX renders it's
2976.839,4.72,just possible because of the universal
2978.839,5.48,scene description that is a file format
2981.559,5.681,and a file framework that is the HTML
2984.319,6.201,for 3D virtual wordss it was created by
2987.24,5.24,pixer because pixer has a very complex
2990.52,4.079,uh bu data building and and asset
2992.48,4.4,building Pipeline and they say hey
2994.599,5.161,there's must be an easy way for us to
2996.88,6.0,integrate all these so uh Nvidia and
2999.76,5.72,Market working this with uh with Pixar
3002.88,4.84,uh there is now a whole uh open
3005.48,6.56,structure and open Association to
3007.72,6.52,evolute open USD uh today basically on
3012.04,5.6,Omniverse when you have an image like
3014.24,7.44,this uh this image is based on different
3017.64,5.8,assets that are like layers and each of
3021.68,4.439,those assets were created with a
3023.44,5.28,different to so without Omniverse
3026.119,4.921,without USD it will be really hard to
3028.72,4.96,integrate everything on a single
3031.04,6.6,experience Omniverse is not a tool it's
3033.68,6.879,a fabric uh with a huge amount of tools
3037.64,5.24,ecosystem this slide is outdated because
3040.559,5.201,it's really hard to keep it updated but
3042.88,5.0,it gives you a very good idea uh here
3045.76,4.88,are some of the software partners and
3047.88,5.28,dearly adopters of Omniverse uh
3050.64,5.64,Omniverse can be automated and extended
3053.16,5.08,using python so I used to explain amerse
3056.28,4.88,to developers as imagine a python
3058.24,5.72,Frameworks that can build metaverses and
3061.16,5.56,virtual words and V virtual things for
3063.96,5.32,you to explore uh there's a lot of Open
3066.72,4.52,Source involved with this you can just
3069.28,5.279,uh download Omniverse check one of the
3071.24,6.319,extensions and start working with them
3074.559,5.321,inside Omniverse we have physical uh
3077.559,5.201,simulation uh using all Nidia
3079.88,5.04,Technologies so we can replicate the
3082.76,5.4,real world rules physical rules to the
3084.92,5.76,virtual bo uh we also have a Handler and
3088.16,6.199,you can use external rinders too uh but
3090.68,6.8,it's very uh uh well developed it to
3094.359,5.24,bisa label so when when we have complex
3097.48,4.96,workflows we can distribute this to
3099.599,5.361,clusters uh realtime photo physical
3102.44,5.8,accurates and base it all on Open
3104.96,6.44,Standards this is a short video of the
3108.24,6.92,BMWs Factory of the future I invite you
3111.4,6.36,all to go to YouTube and check any video
3115.16,5.8,on home Universe digital twins you'll
3117.76,6.2,see a lot of uh very good examples of
3120.96,6.2,videos and and projects and sometimes
3123.96,6.159,it's hard to say what is real and what
3127.16,5.439,is virtual using this technology to
3130.119,5.761,finalize here I would like to leave you
3132.599,5.561,uh three key uh links uh to learn more
3135.88,4.36,about all the things developer.
3138.16,4.6,nvidia.com just go there register
3140.24,5.2,yourself forums that really works
3142.76,5.799,because the framework developers monitor
3145.44,6.48,those forums uh we have a lot of uh uh
3148.559,6.081,material to learn um and to know more
3151.92,4.6,about an evid including some training uh
3154.64,3.719,Inception program that is a program to
3156.52,4.52,engage startups all over the world we
3158.359,5.48,"have more than 15,000 startups all over"
3161.04,5.24,the world I'm responsible for almost 700
3163.839,5.561,here in Latin America and we are really
3166.28,5.96,helping startups to innovate with AI and
3169.4,5.12,Hardware GPU acceleration and also an
3172.24,4.119,evid on Dem month I would suggest to
3174.52,4.72,everyone on an that you change this to
3176.359,5.881,nerdflix uh but idea was not accepted
3179.24,5.16,yet but this is basically what it is so
3182.24,4.599,on each event that we have V that we
3184.4,5.84,talk about in the next slide uh we
3186.839,7.321,generate like 400 talks of content
3190.24,6.04,everything is uploaded to this portal so
3194.16,5.56,if you need to learn anything about
3196.28,6.24,Nvidia uh tools and software and use
3199.72,4.8,cases you just go to Nvidia on demand
3202.52,4.0,you can log in using your developer
3204.52,4.799,account and you are able to access to
3206.52,6.36,all the content we already generated uh
3209.319,5.721,March 20 uh 18 to to 21 we're going to
3212.88,4.199,have in in San Jose first time
3215.04,4.519,presential actually hybrid after five
3217.079,5.0,years we're going to have the GTC that
3219.559,5.081,is the biggest uh technology conference
3222.079,5.841,we have on Nvidia I really recommend you
3224.64,5.84,all to register now uh just scan this QR
3227.92,5.56,code you will be on a link where you can
3230.48,5.4,register for free for the online uh
3233.48,5.56,event but also if you want to attend in
3235.88,6.08,person with this Q code you can have
3239.04,6.079,25% discount uh on your
3241.96,5.8,tickets I would like to thank again uh
3245.119,4.921,to to otavio and Fabio that invited me
3247.76,5.319,to be here with you today thank you for
3250.04,6.079,all uh your attention to uh if you want
3253.079,5.48,to contact me on social media uh this is
3256.119,4.96,the easiest way uh you can find me
3258.559,5.24,everywhere from linkoln to Twitter uh
3261.079,6.0,using this thank you very much I hope
3263.799,3.28,this was useful for you
3267.88,5.56,thank you thank you Jamar yeah it was
3270.04,5.799,really uh amazing and thanks a lot for
3273.44,5.32,for that I think that the viewers really
3275.839,5.96,understood how to accelerate application
3278.76,5.12,using NV sdks I think it was an
3281.799,5.56,insightful presentation for
3283.88,3.479,all devop
3287.799,4.32,CL
3289.599,4.321,this just Just One Last Insight I know
3292.119,4.401,it was a lot of content but in a
3293.92,4.28,nutshell what I would like to say is uh
3296.52,4.76,10 years ago when we going to start to
3298.2,6.48,use GP acceleration and AI you really
3301.28,7.2,need to know a lot of parallelism uh
3304.68,7.32,process distribution it was hard okay so
3308.48,6.079,we create a lot of abstraction layers
3312.0,5.2,that today it's really use it's really
3314.559,4.641,easy for any developer to start uh
3317.2,4.52,working with these Technologies uh this
3319.2,5.44,is not rocket science anymore and I
3321.72,5.599,really recommend you guys to explore uh
3324.64,5.04,because there's a lot to be viewed yeah
3327.319,4.161,yeah yeah I really agree with you and I
3329.68,4.28,and I know that some of our Engineers
3331.48,4.839,are also here so I think that is amazing
3333.96,4.359,for them too and yeah and thank you
3336.319,3.76,again for for that it was a very lean
3338.319,4.681,and illustrative presentation that I
3340.079,4.641,liked that bright aspects that you want
3343.0,3.2,to highlight and to dive in so thank you
3344.72,4.48,for that thank you for your time and for
3346.2,4.8,your kindness to be with us today and
3349.2,5.24,while we are waiting to see if you have
3351.0,5.359,some questions uh from our viewers uh as
3354.44,4.72,I told you before in this live stream we
3356.359,6.121,have a final interactive segment where
3359.16,5.199,each speaker um must answer must answer
3362.48,4.119,question from the previous speaker and
3364.359,4.121,do a question for next one and the
3366.599,4.921,question that our previous speaker
3368.48,6.24,brought to you and you need to answer is
3371.52,5.279,it's not related with tech but would you
3374.72,4.76,rather give up social media or eat the
3376.799,4.721,same dinner for the rest of your life so
3379.48,4.44,I think that I know your answer but
3381.52,6.519,because I think
3383.92,6.72,that will get social media you know uh
3388.039,6.641,my my Twitter account is for
3390.64,7.6,2006 yeah okay so uh I was using Twitter
3394.68,7.72,when we receive SMS messages on feature
3398.24,7.28,phones so I started almost back there
3402.4,5.56,and uh know a lot of changes over the
3405.52,5.92,past I don't think it was so oh thank
3407.96,6.639,you otavio uh I don't think it's uh it's
3411.44,4.159,the the the same place it was in the
3414.599,2.641,past
3415.599,6.76,uh but I have a question for the next
3417.24,7.4,one uh how geni how gener generative AI
3422.359,5.081,change it the way you
3424.64,4.36,work oh amazing amazing and and I think
3427.44,3.879,it is a very good question because I
3429.0,5.52,already know who the next speaker will
3431.319,6.28,be so I think that fits like a glove in
3434.52,5.48,him so yeah question I will let me just
3437.599,5.841,point that and just
3440.0,5.64,to to ask the question to our next
3443.44,4.639,speaker and I think think there is no
3445.64,4.159,questions and I think it is maybe it's a
3448.079,5.0,good thing because your your
3449.799,7.04,presentation was as insightful as it can
3453.079,6.441,be it could be so that said uh and thank
3456.839,5.041,you thank you for for um your
3459.52,5.24,presentation thank you for being with us
3461.88,6.959,today and I hope that you also enjoyed
3464.76,7.319,it and for the our viewers and for our
3468.839,5.441,ex biters not a goodbye see you see you
3472.079,3.681,in March in the Third Edition of this
3474.28,3.64,Liv stream and and of course John Mario
3475.76,5.2,are invited to participate in our next
3477.92,5.119,live stream not as a speaker uh but as a
3480.96,3.72,viewer but maybe next time you can be
3483.039,4.201,speaker again so thank you so much thank
3484.68,4.679,you one one last thing guys if you if
3487.24,4.879,you need to send me any questions please
3489.359,5.44,send me on Twitter I will not talk that
3492.119,5.0,X because I hate this name and also on
3494.799,4.201,LinkedIn okay just send me your
3497.119,3.841,questions on LinkedIn I answer to
3499.0,4.2,everyone there thank you so much for the
3500.96,6.119,opportunity I really hope to see very
3503.2,6.119,very great AI apption come from you guys
3507.079,6.601,as far as I remember your social media
3509.319,4.361,name Omen bite Omen bite Omen
3514.0,8.079,beachen yeah yeah okay let me just uh on
3519.559,3.56,bit just to be here for everyone that
3522.079,4.881,wants
3523.119,3.841,to like this I
3527.079,8.361,think om om bit exactly in the comments
3531.2,6.24,exactly so if you want to contact uh jar
3535.44,4.119,please do so in their social media by
3537.44,4.56,Omen be so thank you again Jamar and see
3539.559,5.76,you next time bye bye thank you so much
3542.0,3.319,pleasure byee
3545.799,3.0,pleasure
