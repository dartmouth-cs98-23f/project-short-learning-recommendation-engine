second,duration,transcript
0.12,4.5,today I'm going to talk to you about
2.399,4.801,Quantum Computing applications in
4.62,5.64,machine learning this is a very exciting
7.2,4.74,area of quantum Computing research and
10.26,4.02,lots of classical machine learning
11.94,5.099,developers are understandably excited
14.28,4.32,about the potential applications within
17.039,4.261,their own field
18.6,4.86,so to get started let's talk about a
21.3,4.559,classical machine learning problem that
23.46,3.659,is one that's very common linear
25.859,4.561,classification
27.119,5.221,so if we start with two sets of data
30.42,4.56,that we want to classify into two
32.34,4.739,separate categories let's draw them here
34.98,7.919,we're just going to have
37.079,8.521,Three Dots and three crosses all on a
42.899,5.461,single linear plane here if we arrange
45.6,4.74,if the data is arranged like this it can
48.36,4.14,be pretty easy to classify this into two
50.34,4.44,discrete groups we can draw a single
52.5,4.379,line in the middle here and now we've
54.78,5.22,classified them
56.879,4.741,but this can be a lot harder if our data
60.0,3.96,is more complex
61.62,3.24,for example if our data is arranged like
63.96,3.78,this
64.86,5.46,perhaps with the crosses in the middle
67.74,3.84,now there isn't a single line that we
70.32,4.5,can draw
71.58,6.539,um on this plane to classify the data
74.82,5.82,into two discrete groups
78.119,4.561,so in order to solve this problem and
80.64,6.54,classify this data what we need to do is
82.68,6.96,we need to map this data into a higher
87.18,5.6,dimensional space which we're going to
89.64,3.14,call a feature space
96.24,6.019,then if we've mapped the data for
99.36,2.899,example like this
103.439,5.161,we can now see because we've mapped this
105.659,5.701,data into a high dimensional space there
108.6,3.72,is now a much easier way to classify
111.36,4.56,this
112.32,6.299,so how do we do this step of uh
115.92,4.979,transferring our data mapping it into a
118.619,6.921,higher dimensional feature space to do
120.899,4.641,this we can use kernel functions
128.28,5.64,kernel functions work by taking some
131.7,5.039,underlying features of the original data
133.92,4.2,set and using that to map those data
136.739,2.701,points into this High dimensional
138.12,4.259,feature space
139.44,5.34,kernel functions are incredibly powerful
142.379,4.681,and Incredibly versatile but they do
144.78,3.9,face problems sometimes they just give
147.06,4.5,poor results
148.68,5.699,um and also the compute runtime can
151.56,4.14,explode as the complexity of the data
154.379,3.36,sets increase
155.7,4.259,if you're a if you're an experienced
157.739,4.321,machine learning developer perhaps
159.959,3.601,you've seen this already if you're
162.06,3.78,dealing with data that has very strong
163.56,4.92,correlations or perhaps if you're
165.84,5.46,dealing with time series forecasting
168.48,4.86,where the data is very complex and at a
171.3,4.62,high frequency
173.34,3.899,but quantum computers have the potential
175.92,5.22,to
177.239,6.0,um provide an advantage in this space
181.14,3.84,they can be useful because quantum
183.239,5.22,computers
184.98,5.58,can access much uh more complex and
188.459,5.541,higher dimensional feature spaces than
190.56,3.44,their classical counterparts can
194.099,4.86,and they can do this because
196.5,4.62,quantum computers can we can encode our
198.959,5.161,data into Quantum circuits and the
201.12,5.399,resulting kernel functions could be very
204.12,4.8,difficult or even impossible to
206.519,4.621,replicate on a classical machine as well
208.92,3.84,as this those kind of functions also can
211.14,4.26,perform better
212.76,4.92,uh in 2021
215.4,5.339,IBM researchers
217.68,5.94,um actually proved that Quantum kernels
220.739,6.0,can provide an exponential speed up over
223.62,8.399,their classical counterparts for certain
226.739,5.28,uh classes of classification problems
235.56,4.739,um as well as this there is a lot of
237.9,6.119,research going into improving Quantum
240.299,6.36,kernels with structured data and kernel
244.019,5.761,alignment so as you can see this field
246.659,7.021,is incredibly exciting there's a lot of
249.78,7.76,research going on in this space
253.68,3.86,um and you can use kiss kit runtime
265.199,4.981,to easily build a Quantum machine
267.419,6.601,learning algorithms with built-in tools
270.18,7.56,such as the sampler primitive which
274.02,6.3,Primitives are unique to uh the IBM's
277.74,5.7,kisket runtime these are essentially
280.32,6.659,predefined programs that help us to
283.44,6.06,optimize workflows and execute them
286.979,4.561,efficiently on Quantum systems
289.5,4.74,let's take for example our linear
291.54,6.36,classification problem let's say we have
294.24,6.06,our data and we've encoded it into a
297.9,6.799,Quantum circuit
300.3,4.399,we can then use the sampler primitive
308.28,7.74,to obtain quasi-probabilities indicating
312.12,6.6,the relationships between the the
316.02,5.94,different data points and these
318.72,5.84,relationships can constitute our kernel
321.96,2.6,Matrix
324.78,5.639,and that kernel Matrix can then be
327.44,5.64,evaluated and used in even a classical
330.419,6.361,support Vector machine
333.08,5.5,to predict new classification labels
336.78,3.78,so if you're ready to get started
338.58,4.74,learning more about Quantum machine
340.56,4.02,learning you can check out the links in
343.32,4.14,the description for more information
344.58,4.679,about kisket runtime as well as a
347.46,4.62,Quantum machine learning course that's
349.259,4.861,available on the kiskit textbook I hope
352.08,4.82,you've enjoyed this content thank you
354.12,2.78,very much for watching
