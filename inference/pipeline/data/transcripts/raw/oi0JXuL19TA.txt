second,duration,transcript
0.18,3.58,"Thanks to Curiosity Stream for supporting
PBS Digital Studios."
3.76,4.26,"Hey, I’m Jabril and welcome to Crash Course AI!"
8.02,2.8,"Language is one of the most impressive things
humans do."
10.82,4.38,"It’s how I’m transferring knowledge from
my brain to yours right this second!"
15.2,4.47,"Languages come in many shapes and sizes, they
can be spoken or written, and are made up"
19.67,6.82,"of different components like sentences, words,
and characters that vary across cultures."
26.49,6.74,"For instance, English has 26 letters and Chinese
has tens-of-thousands of characters."
33.23,3.83,"So far, a lot of the problems we’ve been
solving with AI and machine learning technologies"
37.06,5.97,"have involved processing images, but the most
common way that most of us interact with computers"
43.03,1.23,is through language.
44.26,4.85,"We type questions into search engines, we
talk to our smartphones to set alarms, and"
49.11,4.449,"sometimes we even get a little help with our
Spanish homework from Google Translate."
53.56,4.92,"So today, we’re going to explore the field
of Natural Language Processing."
58.48,8.9,INTRO
67.38,5.74,"Natural Language Processing, or NLP, mainly
explores two big ideas."
73.12,4.92,"First, there’s Natural Language Understanding,
or how we get meaning out of combinations"
78.04,1.23,of letters.
79.27,4.25,"These are AI that filter your spam emails,
figure out if that Amazon search for “apple”"
83.52,4.88,"was grocery or computer shopping, or instruct
your self-driving car how to get to a friend’s"
88.4,1.0,house.
89.4,5.41,"And second, there’s Natural Language Generation,
or how to generate language from knowledge."
94.81,4.629,"These are AI that perform translations, summarize
documents, or chat with you."
99.439,5.29,"The key to both problems is understanding
the meaning of a word, which is tricky because"
104.729,2.221,words have no meaning on their own.
106.95,2.349,We assign meaning to symbols.
109.299,5.421,"To make things even harder, in many cases,
language can be ambiguous and the meaning"
114.72,3.079,"of a word depends on the context it’s used
in"
117.799,5.14,"If I tell you to meet me at the bank, without
any context, I could mean the river bank or"
122.939,1.991,the place where I’m grabbing some cash.
124.93,5.29,"If I say “This fridge is great!”, that’s
a totally different meaning from “This fridge"
130.22,3.859,"was *great*, it lasted a whole week before
breaking.”"
134.079,3.351,"So, how did we learn to attach meaning to
sounds?"
137.43,4.669,"How do we know great [enthusiastic] means
something different from great [sarcastic]?"
142.099,3.9,"Well, even though there’s nothing inherent
in the word “cat” that tells us it’s"
145.999,5.151,"soft, purrs, and chases mice… when we were
kids, someone probably told us “this is"
151.15,1.169,a cat.”
152.32,3.46,"Or a gato, māo, billee, qut."
155.78,4.459,"When we’re solving a natural language processing
problem, whether it’s natural language understanding"
160.239,4.73,"or natural language generation, we have to
think about how our AI is going to learn the"
164.969,3.691,"meaning of words and understand our potential
mistakes."
168.66,4.139,"Sometimes we can compare words by looking
at the letters they share."
172.799,2.92,This works well if a word has morphology.
175.719,5.24,"Take the root word “swim” for example.We
can modify it with rules so if someone’s"
180.959,6.161,"doing it right now, they’re swimming, or
the person doing the action is the swimmer."
187.12,4.0,"Drinking, drinker, thinking, thinker, … you
get the idea."
191.12,5.949,"But we can’t use morphology for all words,
like how knowing that a van is a vehicle doesn’t"
197.069,2.831,"let us know that a vandal smashed in a car
window."
199.9,4.899,"Many words that are really similar, like cat
and car, are completely unrelated."
204.8,5.409,"And on the other hand, cat and Felidae (the word for the scientific family of cats) mean very"
210.209,2.81,similar things and only share one letter!
213.019,5.22,"One common way to guess that words have similar
meaning is using distributional semantics,"
218.239,3.191,"or seeing which words appear in the same sentences
a lot."
221.43,5.869,"This is one of many cases where NLP relies
on insights from the field of linguistics."
227.299,5.17,"As the linguist John Firth once said, “You
shall know a word by the company it keeps.”"
232.469,4.8,"But to make computers understand distributional
semantics, we have to express the concept"
237.269,1.0,in math.
238.269,3.021,One simple technique is to use count vectors.
241.29,5.099,"A count vector is the number of times a word
appears in the same article or sentence as"
246.389,1.721,other common words.
248.11,4.699,"If two words show up in the same sentence,
they probably have pretty similar meanings."
252.809,6.18,"So let’s say we asked an algorithm to compare
three words, car, cat, and Felidae, using"
258.989,2.87,"count vectors to guess which ones have similar
meaning."
261.859,4.381,"We could download the beginning of the Wikipedia
pages for each word to see which /other/ words"
266.24,1.0,show up.
267.24,1.06,Here’s what we got:
268.3,6.619,"And a lot of the top words are all the same:
the, and, of, in."
274.919,5.5,"These are all function words or stop words,
which help define the structure of language,"
280.419,2.47,and help convey precise meaning.
282.889,5.451,"Like how “an apple” means any apple, but
“the apple” specifies one in particular."
288.34,5.299,"But, because they change the meaning of another
word, they don’t have much meaning by themselves,"
293.639,4.78,"so we’ll remove them for now, and simplify
plurals and conjugations."
298.419,1.75,Let’s try it again:
300.169,4.821,"Based on this, it looks like cat and Felidae
mean almost the same thing, because they both"
304.99,3.66,"show up with lots of the same words in their
Wikipedia articles!"
308.65,3.109,"And neither of them mean the same thing as
car."
311.759,3.451,But this is also a really simplified example.
315.21,4.139,"One of the problems with count vectors is
that we have to store a LOT of data."
319.349,4.53,"To compare a bunch of words using counts like
this, we’d need a massive list of every"
323.88,4.7,"word we’ve ever seen in the same sentence,
and that’s unmanageable."
328.58,4.639,"So, we’d like to learn a representation
for words that captures all the same relationships"
333.219,4.57,"and similarities as count vectors but is much
more compact."
337.789,4.821,"In the unsupervised learning episode, we talked
about how to compare images by building representations"
342.61,1.68,of those images.
344.29,5.129,"We needed a model that could build internal
representations and that could generate predictions."
349.419,2.691,And we can do the same thing for words.
352.11,5.33,"This is called an encoder-decoder model: the
encoder tells us what we should think and"
357.44,2.0,remember about what we just read...
359.44,4.909,"and the decoder uses that thought to decide
what we want to say or do."
364.349,3.021,"We’re going to start with a simple version
of this framework."
367.37,4.079,"Let’s create a little game of fill in the
blank to see what basic pieces we need to"
371.449,2.661,train an unsupervised learning model.
374.11,3.059,This is a simple task called language modeling.
377.169,1.12,If I have the sentence:
378.289,4.12,"I’m kinda hungry, I think I’d like some
chocolate _____ ."
382.409,3.32,"What are the most likely words that can go
in that spot?"
385.729,4.581,"And how might we train a model to encode the
sentence and decode a guess for the blank?"
390.31,5.099,"In this example, I can guess the answer might
be “cake” or “milk” but probably not"
395.409,4.85,"something like “potatoes,” because I’ve
never heard of “chocolate potatoes” so"
400.259,2.581,they probably don’t exist.
402.84,2.609,Definitely don’t exist.
405.449,1.171,That should not be a thing.
406.62,5.36,The group of words that can fill in that blank is an unsupervised cluster that an AI could use.
411.98,4.96,"So for this sentence, our encoder might only
need to focus on the word chocolate so the"
416.94,5.59,"decoder has a cluster of “chocolate food
words” to pull from to fill in the blank."
422.53,2.37,Now let’s try a harder example:
424.9,4.519,"Dianna, a friend of mine from San Diego who
really loves physics, is having a birthday"
429.419,4.381,"party next week, so I want to find a present
for ____."
433.8,5.06,"When I read this sentence, my brain identifies
and remembers two things: First, that we’re"
438.86,2.709,talking about Dianna from 27 words ago!
441.569,3.761,"And second, that my friend Dianna uses the
pronoun “her.”"
445.33,4.28,"That means we want our encoder to build a
representation that captures all these pieces"
449.61,5.55,"of information from the sentence, so the decoder
can choose the right word for the blank."
455.16,1.98,And if we keep the sentence going:
457.15,4.41,"Dianna, a friend of mine from San Diego who
really loves physics, is having a birthday"
461.56,5.129,"party next week, so I want to find a present
for her that has to do with _____ ."
466.689,4.13,"Now, I can remember that Dianna likes physics
from earlier in the sentence."
470.819,4.791,"So we’d like our encoder to remember that
too, so that the decoder can use that information"
475.61,1.359,to guess the answer.
476.969,4.35,"So we can see how the representation the model
builds really has to remember key details"
481.32,2.24,of what we’ve said or heard.
483.56,2.8,"And there’s a limit to how much a model
can remember."
486.36,3.88,"Professor Ray Mooney has famously said that
we’ll “never fit the whole meaning of"
490.24,4.929,"a sentence into a single vector” and we
still don’t know if we can."
495.169,4.671,"Professor Mooney may be right, but that doesn’t
mean we can’t make something useful."
499.84,3.09,So so far we’ve been using words.
502.93,3.289,"But computers don’t work words quite
like this."
506.219,3.81,"So let’s step away from our high level view
of language modeling and try to predict the"
510.029,3.491,"next word in a sentence anyway with a neural
network."
513.52,5.26,"To do this, our data will be lots of sentences
we collect from things like someone speaking"
518.789,1.771,or text from books.
520.56,4.18,"Then, for each word in every sentence, we’ll
play a game of fill-in-the-blank."
524.74,4.52,"We’ll train a model to encode up to that
blank and then predict the word that should"
529.26,1.1,go there.
530.36,3.61,"And since we have the whole sentence, we know
the correct answer."
533.97,2.739,"First, we need to define the encoder."
536.709,4.711,"We need a model that can read in the input,
which in this case is a sentence."
541.42,6.2,"To do this, we’ll use a type of neural network
called a Recurrent Neural Network or RNN."
547.62,4.89,"RNNs have a loop in them that lets them reuse
a single hidden layer, which gets updated"
552.51,2.579,as the model reads one word at a time.
555.089,4.381,"Slowly, the model builds up an understanding
of the whole sentence, including which words"
559.47,4.76,"came first or last, which words are modifying
other words, and a whole bunch of other grammatical"
564.23,2.09,properties that are linked to meaning.
566.32,3.62,"Now, we can’t just directly put words inside
a network."
569.94,3.99,"But we also don’t have features we can easily
measure and give the model either."
573.93,3.969,"Unlike images, we can’t even measure pixel
values."
577.899,4.011,"So we’re going to ask the model to learn
the right representation for a word on its"
581.91,3.29,"own (this is where the unsupervised learning
comes in)."
585.2,4.64,"To do this, we’ll start off by assigning
each word a random representation -- in this"
589.84,3.609,case a random list of numbers called a vector.
593.449,4.651,"Next, our encoder will take in each of those
representations and combine them into a single"
598.1,3.229,/shared/ representation for the whole sentence.
601.329,4.831,"At this point, our representation might be
gibberish, but in order to train the RNN,"
606.16,1.9,we need it to make predictions.
608.06,5.449,"For this particular problem, we’ll consider
a very simple decoder, a single layer network"
613.509,4.52,"that takes in the sentence representation
vector, and then outputs a score for every"
618.029,2.541,possible word in our vocabulary.
620.57,3.89,"We can then interpret the highest scored word
as our model’s prediction."
624.46,4.73,"Then, we can use backpropagation to train
the RNN, like we’ve done before with neural"
629.19,1.72,networks in Crash Course AI.
630.91,4.549,"So by training the model on which word to
predict next, the model learn weights for"
635.459,3.151,"the encoder RNN and the decoder prediction
layer."
638.61,4.9,"Plus, the model changes those random representations
we gave every word at the beginning."
643.51,4.7,"Specifically, if two words mean something
similar, the model makes their vectors more"
648.21,1.0,similar.
649.21,4.66,"Using the vectors to help make a plot, we
can actually visualize word representations."
653.87,5.19,"For example, earlier we talked about chocolate
and physics, so let’s look at some word"
659.06,2.94,"representations that researchers at Google
trained."
662.0,4.16,"Near “chocolate,” we have lots of foods
like cocoa and candy:"
666.16,5.42,"By comparison, words with similar representations
to “physics” are newton and universe."
671.58,4.93,"This whole process has used unsupervised learning,
and it’s given us a basic way to learn some"
676.51,4.48,"pretty interesting linguistic representations
and word clusters."
680.99,4.709,"But taking in part of a sentence and predicting
the next word is just the tip of the iceberg"
685.699,1.091,for NLP.
686.79,4.75,"If our model took in English and produced
Spanish, we’d have a translation system."
691.54,5.77,"Or our model could read questions and produce answers, like Siri or Alexa try to do."
697.31,5.49,"Or our model could convert instructions into
actions to control a household robot … Hey"
702.8,5.77,John Green Bot?
708.57,2.11,Just kidding you’re your own robot.
710.68,1.349,Nobody controls you.
712.029,4.571,"But the representations of words that our
model learns for one kind of task might not"
716.6,1.27,work for others.
717.87,5.4,"Like, for example, if we trained John-Green-bot
based on reading a bunch of cooking recipes,"
723.27,4.0,"he might learn that roses are made of icing
and placed on cakes."
727.27,4.12,"But he won’t learn that cake roses are different
from real roses that have thorns and make"
731.39,1.259,a pretty bouquet.
732.649,4.791,"Acquiring, encoding, and using written or
spoken knowledge to help people is a huge"
737.44,4.37,"and exciting task, because we use language
for so many things!"
741.81,5.06,"Every time you type or talk to a computer,
phone or other gadget, NLP is there."
746.87,4.29,"Now that we understand the basics, next week
we’ll dive in and build a language model"
751.16,4.4,together in our second lab! See you then.
755.56,3.94,Thank you to CuriosityStream for supporting PBS Digital Studios.
759.5,4.88,"CuriosityStream is a subscription streaming
service that offers documentaries and non¬fiction"
764.38,5.18,"titles from a variety of filmmakers, including
CuriosityStream originals."
769.56,5.149,"For example, you can stream Dream the Future
in which host Sigourney Weaver asks the question,"
774.709,4.661,"“What will the future look like?” as she
examines how new discoveries and research"
779.37,3.399,"will impact our everyday lives in the year
2050."
782.769,3.861,You can learn more at curiositystream.com/crashcourse
786.63,2.06,Or click the link in the description.
788.69,3.7,"Crash Course Ai is produced in association
with PBS Digital Studios!"
792.39,4.639,"If you want to help keep Crash Course free
for everyone, forever, you can join our community"
797.029,1.711,on Patreon.
798.74,4.26,"And if you want to learn more about how human
brains process language, check out this episode"
803.0,2.54,of Crash Course Psychology.
