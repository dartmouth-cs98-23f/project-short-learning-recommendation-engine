second,duration,transcript
0.08,4.4,machine learning teach a computer how to
2.48,4.08,perform a task without explicitly
4.48,4.72,programming it to perform said task
6.56,4.159,instead feed data into an algorithm to
9.2,3.92,gradually improve outcomes with
10.719,4.96,experience similar to how organic life
13.12,4.96,learns the term was coined in 1959 by
15.679,4.241,arthur samuel at ibm who was developing
18.08,3.52,artificial intelligence that could play
19.92,3.519,checkers half a century later and
21.6,3.679,predictive models are embedded in many
23.439,4.0,of the products we use every day which
25.279,4.24,perform two fundamental jobs one is to
27.439,3.92,classify data like is there another car
29.519,4.241,on the road or does this patient have
31.359,4.161,cancer the other is to make predictions
33.76,3.84,about future outcomes like will the
35.52,3.76,stock go up or which youtube video do
37.6,4.0,you want to watch next the first step in
39.28,4.4,the process is to acquire and clean up
41.6,3.76,data lots and lots of data the better
43.68,3.84,the data represents the problem the
45.36,3.92,better the results garbage in garbage
47.52,3.679,out the data needs to have some kind of
49.28,3.599,signal to be valuable to the algorithm
51.199,3.601,for making predictions and data
52.879,4.32,scientists perform a job called feature
54.8,4.16,engineering to transform raw data into
57.199,3.921,features that better represent the
58.96,4.239,underlying problem the next step is to
61.12,4.399,separate the data into a training set
63.199,4.321,and testing set the training data is fed
65.519,4.161,into an algorithm to build a model then
67.52,4.639,the testing data is used to validate the
69.68,4.32,accuracy or error of the model the next
72.159,3.921,step is to choose an algorithm which
74.0,4.159,might be a simple statistical model like
76.08,3.76,linear or logistic regression or a
78.159,3.521,decision tree that assigns different
79.84,4.0,weights to features in the data or you
81.68,3.92,might get fancy with a convolutional
83.84,3.76,neural network which is an algorithm
85.6,3.92,that also assigns weights to features
87.6,3.12,but also takes the input data and
89.52,2.88,creates additional features
90.72,3.68,automatically and that's extremely
92.4,4.16,useful for data sets that contain things
94.4,4.079,like images or natural language where
96.56,3.84,manual feature engineering is virtually
98.479,3.761,impossible every one of these algorithms
100.4,3.999,learns to get better by comparing its
102.24,4.159,predictions to an error function if it's
104.399,4.481,a classification problem like is this
106.399,4.561,animal a cat or a dog the error function
108.88,3.84,might be accuracy if it's a regression
110.96,3.68,problem like how much will a loaf of
112.72,4.16,bread cost next year then it might be
114.64,4.24,mean absolute error python is the
116.88,4.96,language of choice among data scientists
118.88,4.48,but r and julia are also popular options
121.84,2.879,and there are many supporting frameworks
123.36,2.96,out there to make the process
124.719,3.441,approachable the end result of the
126.32,3.84,machine learning process is a model
128.16,3.92,which is just a file that takes some
130.16,3.84,input data in the same shape that it was
132.08,3.84,trained on then spits out a prediction
134.0,3.52,that tries to minimize the error that it
135.92,3.679,was optimized for it can then be
137.52,4.0,embedded on an actual device or deployed
139.599,3.681,to the cloud to build a real world
141.52,4.0,product this has been machine learning
143.28,3.92,in 100 seconds like and subscribe if you
145.52,3.04,want to see more short videos like this
147.2,2.96,and leave a comment if you want to see
148.56,3.36,more machine learning content on this
150.16,5.64,channel thanks for watching and i will
151.92,3.88,see you in the next one
