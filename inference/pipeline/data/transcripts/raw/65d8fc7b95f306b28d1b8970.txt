 Hello friends, welcome to Gate Smashers.
 In this video we are going to discuss
 introduction to dynamic programming.
 Dynamic programming we use to solve optimization problems.
 Optimization problem means where I have to find optimal answer.
 Optimal means either I have to find 
maximum answer or minimum answer.
 Maximum means let's say if we are finding
 profit then I will find maximum profit.
 Let's say if I am finding cost then I will find minimum cost.
 So this is actually optimization problem.
 Now optimization problem must be coming in your mind that we have solved this with greedy method.
 Dynamic programming is also doing the same work 
but there is a lot of difference in both the approaches.
 If we talk about greedy method, what we 
did there is let's say this is my source.
 Source is connected to destination and 
this is intermediate nodes in between.
 I have directed graph, let's say this is my directed 
graph and this is my source and destination.
 Now these are intermediate nodes, let's say I
 have to reach destination from source.
 Now here obviously what I will use, I want to find minimum cost.
 What is the optimal answer? You reach 
destination from source with minimum cost.
 What does greedy method do? Let's 
say this is distance, this is 10, 22.
 Now what will greedy method do? By default
 it will choose this arrow or this edge only.
 Reason for that is at the stage you 
are standing, at that stage, the minimum one,
 means if we talk from cost point of view, \
who is giving the best answer? S to A.
 So what will it do? It will close both these paths.
 Means it will not even look at this, which one? Greedy method.
 Because what we are doing is which one is the best out of these three? This one.
So we are going above this. Now is it necessary 
that it will always give right answer? No.
 Let's say the cost of the next path A to B is 100
 and this cost is let's say 10, 20,
 take anything, 5, 3. Obviously here my cost is 2, I chose this first.
 Now next cost A to B is 100. So obviously 
what will be my wrong answer here?
 Reason for that is at the stage you are standing, 
you have found the minimum solution here.
 This is the approach of greedy approach. 
But what does dynamic programming do?
 Dynamic programming does not take 
such decisions. It will follow all the path,
 it will follow all the sequence of decisions 
first, it will traverse all those sequence of decisions
 then it will reach a final solution. So here 
you can say that dynamic programming
 always give the optimal answer. Yes. Greedy always give the optimal answer may or may not be true.
 As I told you in the example here, what is 
the method of greedy here? It is failing.
 But dynamic programming will not fail 
here because it will check all the paths
 then it will give the final answer here. 
So the first point is here, greedy
 and dynamic programming actually divides
 the problem into series of overlapping subproblems.
 Means we are dividing a big problem into small subproblems
 then we are combining the solutions of my subproblems.
 Like we do in divide and conquer. What we did in divide and conquer?
 Like there is an example of merge sort. So what we do in merge sort?
 We divide the array which we have into two parts, then we divide it into two parts.
 In this way we convert it into subproblems. Then we combine these two,
 means we combine all the problems and convert it into a final answer again.
 But if we talk about dynamic programming here, 
then the first approach is also the same.
 Means what is the first feature of these two features? Optimal substructure.
 What does optimal substructure mean? That your problem is divisible.
 Means your problem is divisible in parts in this way, all the problems are being made.
 You solve all those problems and combine them and convert them into final answer.
 But the main difference is this from divide 
and conquer dynamics. That is overlapping subproblems.
 What does it mean by subproblems? What 
does it mean by overlapping subproblems?
 That all the problems here are repeated but
 in divide and conquer, non-overlapping will come here.
 Because it will not be repeated there. 
Let's say if I take 5, 3, 2, 1, 10, 20.
 If I take it like this, let's say here 
5, 3, 2 came, here 1, 10, 20 came.
 Here I have 5 and 3 came. Now when I combine
 5 and 3, I obviously sorted these two.
 What do we do in merge sort? 
We sorted these two, 3 and 5.
 Now see 3 and 5 are not being repeated 
anywhere else in the whole program.
 Means all these problems are not being repeated 
anywhere. You solved it, use it, that's it.
 But in dynamic programming, what are the 
problems? All the problems are repeated.
 What do you do with those repetitions? You store them 
so that you do not solve all the problems again and again.
 Once you have solved it, store it. Next time
 if the same subproblem comes again,
 you pick up data from the direct table. Means pick up 
data from the table where you have stored the result.
 How? Let's say if we take the example of Fibonacci series.
 Now how does the program work in Fibonacci series? Let's say we have f of n.
 What do we do with f of n? F of n-1 plus f of n-2, we convert it.
 Means if my series is 0, 1, 2, 3, 4, 5. So what is Fibonacci series?
 Fibonacci series is 0 on 0, 1 on 1. Next 
1 plus 0, 1. 1 plus 1, 2. 2 plus 1, 3. 3 plus 2, 5.
 5 plus 3, 8. 8 plus 5, 13. It moves forward in this way.
 Means here my actual recurrence relation 
comes that f of n is equal to f of n-1 plus n-2.
 When will my f of n come? When n value is 1. 
If my n value is 0, then my f of n will come 0.
 Otherwise my recurrence relation becomes this. Now if I solve f of 4 here.
 If we solve f of 4, then what will happen? f of 3, f of 2. See this is what we did.
 First of all optimal substructure means we 
are dividing a problem. So are we able to divide?
 Yes, we are able to divide. What will we do next? 
f of 2, f of 1. What will we do with this?
 f of 1, f of 0. So see the problem is dividing
 in your substructure in all the problems.
 But the second point here is overlapping subproblem. 
What does overlapping subproblem mean?
 When we solve this, see leave level, here I can write f of 1, f of 0.
 This f of 1 is already a small subproblem. 
This is also small. Now see here when I solve f of 1.
 How much answer did f of 1 give? 1. When n value is 1, then the answer will be 1.
 When n value is 0, then the answer will be 0. What will 
be 1 plus 0? 1. So see what you do with f of 2, store it.
 Now next time f of 2 is coming somewhere, see f of 2 is being repeated here.
 f of 1 is being repeated, f of 0 is being repeated. So 
you don't need to solve all these problems again and again.
 What you do is store the data in a table. 
Means we have stored f of 2, f of 1, f of 0,
 f of 3 so that when the same subproblem will 
be repeated again. I have only written f of 4.
 If you try to write f of 10 or f of 11, then you will 
see that a lot of subproblems are being repeated.
 That is called the overlapping subproblems.
 So what does dynamic programming do here?
 We will store these subproblems, we will store their results so that we don't solve them again and again.
 Because if we solve them again and again, then 
its time complexity order of 2 raised to power n.
 Means exponential time complexity will 
be created here. But if you store it,
 let's say you have stored f of 2, f of 3, f of 4, 
that is how many functions will be opened?
 f of 4, f of 3, f of 2, 1 and 0. You don't need to
 solve this repetition again and again.
 So this is actually the foundation of dynamic 
programming. We will use this in multiple problems.
 Like matrix chain multiplication, multi-stage graph, 
we have a traveling salesman problem,
 longest common subsequence, sum of subset 
problem. We use all these problems in them.
 Even all pair shortest path, 0-1 knapsack. 
What do we use in all these? Dynamic programming.
Thank You.
