second,duration,transcript
2.08,4.4,before we get started
3.679,3.841,i wanted to begin uh just with one of my
6.48,4.64,favorite
7.52,4.079,examples of practical analytics and how
11.12,2.639,it
11.599,4.561,influences our everyday life i've spent
13.759,3.841,a lot of time traveling for my work and
16.16,2.72,i've really grown to appreciate the
17.6,3.04,simple meaningful
18.88,3.36,actionable predictive analytics that
20.64,3.44,uber provides
22.24,3.199,if you don't know what uber is it's a
24.08,1.92,ride-sharing system and it's kind of
25.439,2.881,like
26.0,3.119,being able to call a taxi from your
28.32,2.32,phone
29.119,3.521,except it's more than that because when
30.64,3.599,you make a request from uber
32.64,4.64,they know your gps location of your
34.239,6.401,device they also know the gps location
37.28,5.2,of drivers who are around you and uber
40.64,4.079,uses your gps location
42.48,3.2,and the gps location of the drivers
44.719,2.801,around you
45.68,4.24,to estimate how long they think it'll
47.52,2.4,take to get
52.8,5.12,now available as an apple watch
56.239,3.441,application so what you're seeing here
57.92,3.68,is a picture of my watch yesterday
59.68,3.359,and at that time uber said it would take
61.6,2.639,five minutes to get a driver to me and
63.039,3.521,it was all based on
64.239,4.481,predictive analytics of my location the
66.56,3.919,location of the drivers around me
68.72,4.48,the routes between me and those drivers
70.479,5.921,the travel time uh the time of day
73.2,5.44,um and and perhaps past uh travel
76.4,3.759,times of those routes and all of that
78.64,2.159,information is kind of put together into
80.159,2.801,a simple
80.799,4.561,this simple number five minutes and this
82.96,4.479,is great because it's super actionable
85.36,3.68,if i am happy with that number i go
87.439,2.401,ahead and hit that request and a driver
89.04,3.039,shows up
89.84,3.76,in about five minutes if i'm not happy
92.079,3.201,with that number maybe it's too big
93.6,3.6,maybe i just cancel my request and call
95.28,3.839,a cab and i've done that before
97.2,4.16,and it's just a great example of how
99.119,3.28,uber is taking all of this data about me
101.36,3.039,and the drivers
102.399,3.68,compiling into a very simple actionable
104.399,4.641,number that's delivered right
106.079,4.481,right to my watch and it's not just uber
109.04,3.2,we live in a world where predictive
110.56,4.64,analytics is pervasive
112.24,5.199,so when you log into amazon or netflix
115.2,3.84,these this is what amazon and netflix
117.439,3.28,thinks that i want to watch based on the
119.04,3.679,the viewing patterns of the account
120.719,3.44,that i use and it's interesting i think
122.719,4.72,it's pretty obvious probably to
124.159,4.88,many people that's not necessarily me
127.439,4.401,who's watching these videos
129.039,3.521,um what what's going on here is amazon
131.84,3.2,is taking the
132.56,3.759,the buying and viewing patterns of me
135.04,3.44,and comparing them with
136.319,4.0,users who have similar viewing patterns
138.48,2.96,and making suggestions based on those
140.319,4.481,patterns
141.44,5.04,and what's happened what
144.8,3.439,happens here is my kids log in on the
146.48,2.56,weekends and they watch all sorts of
148.239,3.441,cartoons
149.04,4.32,and amazon and netflix both think i have
151.68,4.32,a strong interest in watching a
153.36,4.48,ragtag group of cartoon puppies solve
156.0,3.2,problems that's what paw patrol is
157.84,3.119,but the truth is i'm not really
159.2,2.08,interested in these predictive analytics
160.959,2.241,and
161.28,3.76,it's an interesting thing to think about
163.2,3.28,is what are the assumptions that we make
165.04,3.36,about the underlying data
166.48,3.6,as we use predictive analytics and i
168.4,3.119,think we'll ponder some of these
170.08,2.64,questions about what it means for health
171.519,3.761,care a little bit later in the
172.72,2.56,presentation
177.12,3.039,we're going to uh have another quick
178.8,3.2,poll question so
180.159,3.921,how important are predictive analytics
182.0,5.12,for the future of health care
184.08,5.84,not at all important low importance
187.12,3.44,neutral moderately important extremely
189.92,2.959,important
190.56,3.52,or unsure or not applicable hey eric
192.879,3.041,well we're having everyone respond to
194.08,3.439,those i'd like to apologize to everybody
195.92,2.959,the audio we had a couple of audio
197.519,2.481,issues we think we've got those sorted
198.879,2.72,out
200.0,4.4,we said we this is new software for us
201.599,4.401,so we appreciate your patience with us
204.4,5.119,all right let's go ahead and share the
206.0,3.519,results of our poll
210.239,5.601,but we're showing 75 extremely important
214.48,2.64,yes that's why they joined the webinar
215.84,2.0,today i was gonna say it sounds like a
217.12,2.88,little bit of uh
217.84,4.0,selection bias but it is i think it is
220.0,4.959,important and we'll talk about
221.84,6.479,how we can lower the barrier to uh doing
224.959,3.36,predictive analytics in healthcare
231.599,3.28,all right we're just getting back to uh
235.12,3.759,share that screen all right as we move
238.48,2.319,into
238.879,3.681,healthcare let's so first of all let's
240.799,3.44,just
242.56,3.2,high level predictive analytics is about
244.239,2.401,using pattern recognition just like we
245.76,2.72,talked about
246.64,3.599,with the amazon and netflix examples
248.48,3.6,there's patterns in that data that
250.239,4.161,they're using to predict future events
252.08,3.439,we can apply that to healthcare but it's
254.4,3.2,really important
255.519,3.521,that predicting something to understand
257.6,2.159,that predicting something is not good
259.04,3.12,enough
259.759,3.521,you must have the data to act and
262.16,2.8,intervene
263.28,4.479,and especially in healthcare the
264.96,5.519,organizational wherewithal to intervene
267.759,3.121,it it's one thing to predict what videos
270.479,2.72,i might
270.88,3.28,want to view next or what things i might
273.199,2.0,want to buy next
274.16,2.64,it's a different thing to start
275.199,3.041,recommending care based on predictive
276.8,3.76,analytics so in healthcare
278.24,3.84,the stakes are higher but the rewards
280.56,3.199,are potentially much greater and it's
282.08,2.48,important for an organizational to buy
283.759,4.481,into that
284.56,5.6,to that risk balance and also to
288.24,3.76,ensure that the analytics are
290.16,3.36,incorporated in an appropriate way into
292.0,3.199,the very complex operations of a
293.52,4.64,healthcare organization so
295.199,2.961,definitely a different game
304.0,4.8,at this point i wanted to talk just
307.36,4.399,outlay some definitions and we have a
308.8,4.88,few uh definition slides here
311.759,3.521,um in the presentation and i just wanted
313.68,3.2,to kind of clarify these as we as we
315.28,3.12,move along because i'll be using some
316.88,2.879,jargon in the presentation and i think
318.4,3.04,it's good to get everybody on the same
319.759,4.88,page about what that means
321.44,4.96,so we often hear machine learning and
324.639,3.201,predictive analytics in the same breath
326.4,4.079,and sometimes even mentioned
327.84,4.48,synonymously so machine learning
330.479,3.761,explores the study and construction of
332.32,3.52,algorithms that can learn from and make
334.24,4.48,predictions on data
335.84,4.72,then within the field of analytics using
338.72,2.4,machine learning is a method used to
340.56,1.919,devise
341.12,3.44,models that lend themselves to
342.479,3.601,prediction this is predictive analytics
344.56,4.0,so the way that i like to think about it
346.08,3.92,is that machine learning is a technique
348.56,2.88,that's used in predictive analytics
350.0,2.72,there are other other ways to do
351.44,3.36,predictive analytics but
352.72,3.039,machine learning is by far the most
354.8,3.6,pervasive
355.759,4.0,popular and growing method right now for
358.4,4.639,predictive analytics that's why you
359.759,3.28,often hear them mentioned in the same
364.84,3.24,breath
366.08,3.04,predictive analytics isn't completely
368.08,4.48,new to health care
369.12,5.919,so when going all the way back to 1987
372.56,3.6,the charleston index is actually a
375.039,3.521,predictive algorithm
376.16,5.44,it's designed to predict the mortality
378.56,5.359,of a patient with multiple comorbidities
381.6,4.08,and the charleston index was done by a
383.919,4.4,group that took data from
385.68,4.639,numerous patients classified their
388.319,3.44,conditions into comorbid conditions
390.319,3.361,and they used a fairly narrow set of
391.759,3.28,data so it's fairly easy to get set of
393.68,2.88,administrative data
395.039,2.88,then they counted those comorbid
396.56,2.639,conditions and ranked them based on
397.919,3.28,their severity
399.199,3.521,and they they combined that combined
401.199,2.961,comorbidity score
402.72,3.68,with other information about the
404.16,4.159,patients such as their age to develop a
406.4,3.76,relative risk that that patient
408.319,4.401,was going to die in the next 10 years so
410.16,4.08,it is a predictor of mortality
412.72,3.199,and it's actually gained widespread
414.24,4.959,popularity we hear a lot about
415.919,3.84,charleston index still today the lace
419.199,3.12,index
419.759,5.28,is another example of predictive
422.319,4.88,analytics and lace is meant to predict
425.039,4.401,readmissions and the lace group took
427.199,4.081,data from all across the country
429.44,3.36,and developed a model that predicts
431.28,5.039,readmissions based on
432.8,6.08,length of stay acuity
436.319,4.401,comorbidities and er utilization that's
438.88,5.68,what lace stands for
440.72,4.4,and the um the group basically took data
444.56,2.16,from
445.12,3.68,a large number of different
446.72,4.16,organizations contributing their data
448.8,3.04,and they develop this model that uses
450.88,3.68,those uh
451.84,3.759,inputs to determine the patient's risk
454.56,3.12,of readmission
455.599,3.521,and it also has gained widespread
457.68,2.48,popularity and we hear of a lot of
459.12,3.519,organizations
460.16,4.4,that are implementing waste the
462.639,4.161,interesting thing about these models
464.56,3.039,is that while they're very good because
466.8,3.44,they allow
467.599,4.32,organizations without a deep machine
470.24,2.48,learning capability to do predictive
471.919,3.12,analytics
472.72,3.36,and to do it on a smaller easier to get
475.039,2.56,set of data
476.08,3.2,there are problems with these models and
477.599,3.521,and we're showing here
479.28,4.08,two issues that have come up with these
481.12,4.56,models so and these are just two of many
483.36,3.119,so what you see in the top headline is
485.68,2.56,the top
486.479,4.16,citation is that patient they were using
488.24,3.92,lace to predict readmissions for chf
490.639,2.081,patients congestive heart failure
492.16,2.0,patients
492.72,3.199,and the next one they were trying to use
494.16,4.64,lace to predict
495.919,3.761,readmissions for older uk populations
498.8,2.64,and what
499.68,3.6,what they found at both of the
501.44,3.439,conclusions it said that lace was not a
503.28,3.84,good predictor for both of these
504.879,3.6,specific populations and part of the
507.12,3.199,reason for that is that
508.479,3.44,when the lace model was created they
510.319,3.041,were using data from
511.919,3.12,all different kinds of patients from all
513.36,2.4,across the country and we know for
515.039,3.041,example that
515.76,4.0,the factors that drive an appendectomy
518.08,2.399,readmission are quite different than the
519.76,3.04,factors
520.479,3.92,that drive a congestive heart failure
522.8,2.88,re-emission in the lace model all of
524.399,3.12,those are mixed together
525.68,3.44,and as soon as you start looking in
527.519,3.521,specifically and using lace to try to
529.12,4.159,predict a specific population
531.04,3.6,you lose your predictive value so these
533.279,2.56,general models while they're helpful to
534.64,3.199,get people started
535.839,3.201,they lose their predictive value as we
537.839,2.881,start to look more and more into
539.04,3.2,specific populations and anybody who's
540.72,3.36,working in healthcare today
542.24,4.4,knows that that's we're doing a lot of
544.08,4.24,that we're looking at looking into
546.64,3.92,how do we care for this specific
548.32,5.36,population so those models don't hold up
550.56,3.12,so well for that use case
554.64,4.72,so what's happened since these models
556.32,3.04,came out in 2010
559.839,3.761,first we talked about on the last slide
562.08,3.199,the limitations on those models while
563.6,3.28,they're good to get started they
565.279,4.641,they lack in their ability to predict
566.88,5.44,specific populations
569.92,3.44,next data availability has has grown a
572.32,3.199,lot since 2010
573.36,3.68,we've been lucky enough to be a part of
575.519,3.601,organizations that are investing
577.04,4.56,in data warehouses after the big
579.12,5.2,investment in electronic health records
581.6,3.679,a lot more data became available and the
584.32,2.56,premise of the
585.279,3.761,index models is that they're they're
586.88,4.0,using a narrow set of data
589.04,4.479,but now organizations just have access
590.88,4.32,to much deeper repositories of data
593.519,3.041,again we've been lucky enough to be a
595.2,4.72,part of that and see that
596.56,6.08,see that play out there's also
599.92,4.24,more advanced analytics capabilities so
602.64,3.439,the basic
604.16,3.359,understanding of how to use data to
606.079,5.681,improve business process
607.519,4.241,and to improve care has been
611.92,5.039,taking has taken a larger part of our
614.88,5.12,national focus as well
616.959,4.721,organizations are routinely using data
620.0,3.2,to improve health care and they're
621.68,2.32,starting to ask that next level of
623.2,2.72,questions so we
624.0,3.04,typically start with retrospective
625.92,2.64,analytics so what
627.04,3.359,where have we gone wrong in the past and
628.56,3.6,how can we fix that moving forward
630.399,3.68,now organizations are asking more mature
632.16,3.76,questions about help me get ahead of my
634.079,4.961,problems and predictive analytics are
635.92,5.2,of course a big part of that
639.04,3.359,and finally we we have much better
641.12,3.52,machine learning tools
642.399,4.481,since then so even in that relatively
644.64,5.92,short amount of time there's been a a
646.88,7.04,huge explosion of open source tools of
650.56,4.0,online education that help to spread
653.92,2.56,this
654.56,4.0,machine learning and how to do machine
656.48,4.799,learning and so those better tools
658.56,3.92,are also part of this increased interest
661.279,4.481,in machine learning
662.48,3.28,based predictive analytics
667.839,3.12,so we're going to ask another poll
669.279,3.361,question
670.959,3.041,what's the biggest barrier to
672.64,4.319,implementing predictive
674.0,4.0,analytics we're lacking the right people
676.959,2.721,or skills
678.0,4.24,we don't have the right data or
679.68,4.399,technical tools and infrastructure
682.24,3.039,we don't have the executive support or
684.079,3.521,budget
685.279,3.281,past efforts have failed to show results
687.6,3.919,other or
688.56,5.04,unsure or not applicable right we'll
691.519,3.201,give some time for folks to respond to
693.6,2.56,the poll
694.72,3.04,and we'd like to remind everyone we have
696.16,2.72,had a few questions
697.76,2.8,about the slides we'd like to remind
698.88,2.24,everyone that we will be sending out an
700.56,2.64,email
701.12,5.04,after the event with links to the
703.2,5.68,recorded on-demand webinar as well as
706.16,3.44,the slides as well so let's go ahead and
708.88,4.959,look
709.6,4.239,at our results
713.92,5.039,okay it looks like organizations the top
716.639,4.801,two responses are people or skills
718.959,3.521,and the right data or technical tools
721.44,3.199,and hopefully those
722.48,3.68,those points are well addressed uh
724.639,4.081,within the rest of the presentation
726.16,3.04,and executive support or budget is also
728.72,3.04,a very
729.2,3.439,a very big factor and will hopefully
731.76,2.879,have some
732.639,3.361,information that can help convince
734.639,3.841,executives that this is a good thing to
736.0,2.48,do as well
740.8,3.039,so the main message of all of this
742.639,2.081,presentation is that predictive
743.839,4.56,analytics
744.72,5.119,is easy it's at least easier and part of
748.399,3.12,part of that's due to the explosion of
749.839,3.361,tools
751.519,3.281,but what organizations are truly
753.2,4.079,struggling with is
754.8,3.2,making predictive analytics routine
757.279,2.161,pervasive
758.0,3.279,and actionable and that's what we want
759.44,3.44,to talk about today is how do we take
761.279,3.12,predictive analytics and make it
762.88,5.04,something that's easier to do
764.399,3.521,and routine for an organization
770.16,3.2,the typical current state of predictive
771.839,5.321,analytics is still
773.36,5.84,um not necessarily optimized for
777.16,3.72,operationalization
779.2,3.6,what happens is you've got data
780.88,3.44,scientists and they may have access and
782.8,2.56,they have read access to a data
784.32,3.04,repository
785.36,3.84,and the first thing that they do when
787.36,2.96,they
789.2,2.72,the first thing that they do when they
790.32,2.56,have a predictive model that they want
791.92,3.039,to develop
792.88,3.36,is they write a really big query against
794.959,2.88,that data source because they need they
796.24,3.039,need to get all of the data points they
797.839,2.961,think they're going to need
799.279,2.8,to make a prediction and they know
800.8,3.039,they're going to have to manipulate that
802.079,2.32,data so they write this really big sql
803.839,1.921,query
804.399,3.041,then they bring it into their tool of
805.76,4.4,choice it could be excel
807.44,4.16,it could be sas it could be r but the
810.16,2.479,idea is that they get all that data into
811.6,2.0,their tool
812.639,3.2,because that's where they feel
813.6,3.919,comfortable manipulating data
815.839,3.601,and then they do that data manipulation
817.519,3.601,to get that data in a state that's ready
819.44,3.519,to be used in a predictive model
821.12,3.279,and again they're using a tool outside
822.959,3.601,of their analytics environment to do
824.399,4.641,this
826.56,3.839,then they apply the tools and algorithms
829.04,4.32,so examples sas
830.399,5.041,week r and python all of these tools are
833.36,3.599,tools that are available to take those
835.44,3.28,take that data and turn it into a
836.959,3.201,predictive model
838.72,3.28,and then once they've developed a
840.16,2.32,predictive model there's a big question
842.0,2.56,mark
842.48,3.68,uh two questions usually number one is
844.56,2.8,how do we move this into production
846.16,3.359,and then number two is how do we
847.36,4.24,actually get it to improve care
849.519,3.921,how do we get it to actually enhance a
851.6,3.52,decision so that's a big
853.44,3.199,oftentimes a big question and we've seen
855.12,3.2,that a bunch of times where
856.639,4.161,a good predictive model is developed but
858.32,4.72,it's never really deployed
860.8,3.279,and the point today we'd like to just
863.04,2.64,talk about three
864.079,3.041,recommendations and the way we'll be
865.68,2.24,structuring the rest of the presentation
867.12,4.399,is about
867.92,4.88,these three recommendations number one
871.519,2.641,fully leverage your analytics
872.8,3.36,environment we'll talk about what that
874.16,4.16,means but in a nutshell
876.16,3.6,don't do a lot of data manipulation
878.32,2.959,outside of your analytics environment
879.76,4.8,because then it becomes a silo
881.279,5.761,and very difficult to reuse
884.56,4.719,standardized tools and methods and use
887.04,3.84,pred and create production quality code
889.279,3.841,that you feel comfortable putting into
890.88,3.84,production if you develop a good model
893.12,3.12,the logical next step is going to be to
894.72,4.479,put in production so having
896.24,4.399,really good code to to do that is very
899.199,2.401,important and also to have standard
900.639,3.521,methods um
901.6,2.56,with your team
905.279,3.201,and this one is last but it should
907.12,2.159,really be first because it's the most
908.48,2.479,important
909.279,3.601,of all of these points is to deploy your
910.959,4.0,models with a strategy for intervention
912.88,3.92,make sure you know who's going to use
914.959,3.68,the data to change what they're doing or
916.8,4.08,to help make a decision
918.639,3.281,and and how that's going to be presented
920.88,2.8,with them that's
921.92,3.44,the the most important point of all this
923.68,3.12,and we'll talk about what that
925.36,4.24,what that looks like a little bit later
926.8,2.8,in the presentation
930.56,5.04,so when let's talk about fully
932.079,5.841,leveraging your analytics environment
935.6,5.2,here's another piece of jargon in in
937.92,5.52,predictive analytics
940.8,4.08,a feature is simply an input parameter
943.44,2.56,just think of it as an input to one of
944.88,3.36,your data models
946.0,3.519,and in machine learning we just we call
948.24,2.159,it a feature so when you hear me use the
949.519,2.56,word feature
950.399,3.201,just think of these are the inputs to
952.079,2.56,the model that i'm trying to generate a
953.6,6.56,prediction from
954.639,5.521,and this definition is from wikipedia
962.48,3.599,and let's think about what an analytics
964.32,3.36,environment is an analytics environment
966.079,3.76,or data warehouse
967.68,3.68,you can think of as almost like a chock
969.839,3.281,full of features you've got a bunch of
971.36,3.279,data there but it's not always just
973.12,3.04,sitting there in raw format
974.639,4.081,you've got things like clinical
976.16,4.96,registries you have comorbidity models
978.72,4.64,you have calculations on readmissions
981.12,3.839,and length of stay and other calculated
983.36,5.12,fields so all of these make
984.959,6.0,great feature inputs to models but
988.48,3.76,it's really important to understand that
990.959,5.041,read-only access
992.24,5.36,is not enough data scientists and the
996.0,2.399,folks who are generating predictive
997.6,2.4,models
998.399,3.44,need to be able to create their own
1000.0,4.959,features in the analytics environment
1001.839,5.281,we'll make a strong case for that here
1004.959,4.481,to illustrate the point we're going to
1007.12,3.76,explore a poly pharmacy feature and this
1009.44,3.519,is a feature that we developed
1010.88,4.56,as an input to one of our models our
1012.959,3.601,data scientist was developing a model
1015.44,3.44,one of our data scientists was
1016.56,3.36,developing a model for predicting
1018.88,4.72,complications
1019.92,5.44,in diabetic patients and
1023.6,3.04,if you don't know what polypharmacy is
1025.36,3.76,the new york times here
1026.64,3.36,represents it as the ever-mounting pile
1029.12,2.16,of pills
1030.0,3.199,quite simply put it's a number of
1031.28,3.919,medications that a patient is on
1033.199,3.281,at any given point in time and there's
1035.199,2.161,well there's good examples in the
1036.48,3.599,literature
1037.36,4.24,of polypharmacy being a good predictor
1040.079,3.281,of specific outcomes
1041.6,3.839,so this data scientist wanted to use a
1043.36,5.12,polypharmacy feature
1045.439,4.881,in his model and when he looked at the
1048.48,3.199,medication data though it was a little
1050.32,3.68,bit messy and i'm sure
1051.679,3.841,given the number of data architects and
1054.0,2.96,analysts we have on the call
1055.52,3.44,this shouldn't come as a surprise that
1056.96,2.56,there's messy data underneath the hood
1058.96,2.64,in the
1059.52,3.68,data warehouse environment and what what
1061.6,4.4,you see on the left-hand side
1063.2,3.839,is the medica table of medications and
1066.0,3.52,for every patient
1067.039,4.0,medication pair there's a start date and
1069.52,3.36,an end date the date that a patient
1071.039,4.961,started a specific medication
1072.88,3.12,and the date they ended up
1076.48,4.319,as you can see on the far right hand
1080.4,2.399,side
1080.799,4.0,that there's several nulls so the end
1082.799,3.201,date is not known in cases and it's
1084.799,3.12,actually missing data
1086.0,3.44,that can actually be very damaging to a
1087.919,2.88,predictive model so how do we clean that
1089.44,3.04,up we've got to understand
1090.799,3.201,what's going on to create those null
1092.48,4.96,values in some cases
1094.0,5.2,the patient died before the end date
1097.44,3.2,and in other cases the patient took a
1099.2,4.8,one-time dose
1100.64,3.36,where they were not um
1104.48,3.68,the m date wasn't put in because there
1105.84,4.16,was a single dose of the medication
1108.16,3.12,and finally there's yet another case
1110.0,2.64,where the patient just hasn't reached
1111.28,2.32,the end date yet they're still on the
1112.64,2.72,medication
1113.6,4.319,so understanding all of those business
1115.36,5.679,rules helped our data scientists
1117.919,4.401,to fill in appropriate missing end dates
1121.039,2.321,and create what you see on the right
1122.32,3.76,hand side here
1123.36,3.52,which is what an input to a predictive
1126.08,3.52,model
1126.88,5.039,looks like it's very clean and it gives
1129.6,4.319,us that polypharmacy count so for every
1131.919,4.481,patient encounter for any point in time
1133.919,3.841,we can easily tell how many medications
1136.4,4.48,a patient was on
1137.76,6.159,and this is an example of what we call
1140.88,5.52,feature engineering
1143.919,3.841,feature engineering is the process of
1146.4,3.12,transforming raw data
1147.76,3.52,into features that better represent the
1149.52,2.48,underlying problem to the predictive
1151.28,3.36,models
1152.0,4.72,resulting in improved model accuracy and
1154.64,4.48,feature engineering in our opinion
1156.72,3.36,is one of the most challenging and
1159.12,4.64,interesting parts
1160.08,6.64,of developing predictive models
1163.76,3.279,it's also recognized by by folks out
1166.72,2.319,there
1167.039,4.321,on on the internet that much of the
1169.039,4.801,success of machine learning is actually
1171.36,4.08,success in engineering features that a
1173.84,4.079,learner can understand
1175.44,3.92,so feature engineering is an absolutely
1177.919,4.241,critical part
1179.36,3.28,to data science and predictive analytics
1182.16,4.24,we
1182.64,3.76,we can't underscore that point enough
1186.64,3.039,other examples of feature engineering
1188.32,2.719,and i'm sure the data architects and
1189.679,2.321,data analysts on the phone will
1191.039,2.401,recognize how
1192.0,3.12,some of these things sound really simple
1193.44,3.2,but they're actually a little bit more
1195.12,2.4,complicated to put together than you
1196.64,3.039,might think
1197.52,3.84,so the number of er visits in the last
1199.679,3.36,year fairly simple
1201.36,3.6,the number of line days that a patient
1203.039,4.481,is on sometimes the underlying data
1204.96,4.16,presents a challenge in calculating that
1207.52,3.12,the number and types of comorbid
1209.12,2.88,conditions how do you classify those
1210.64,4.48,comorbid conditions
1212.0,5.84,almost any input to a predictive model
1215.12,4.799,will need to be engineered in some way
1217.84,3.36,and the ability for data scientists to
1219.919,4.321,engineer features
1221.2,4.64,is critical to the success of predictive
1224.24,4.08,analytics and a machine learning
1225.84,2.48,strategy
1229.28,3.519,and remember the point of this section
1231.12,3.039,is to fully leverage your analytics
1232.799,2.561,environment and one of the main reasons
1234.159,3.041,why we say that
1235.36,3.199,is because the analytics environment is
1237.2,5.04,the best place
1238.559,5.12,to engineer features the data scientists
1242.24,2.88,have to have to be able to promote
1243.679,2.401,efficient reuse of the engineered
1245.12,3.039,features that's one
1246.08,4.32,great example so that if we go back to
1248.159,4.081,that polypharmacy example
1250.4,3.6,that polypharmacy table is now sitting
1252.24,3.2,in the data warehouse and available for
1254.0,3.44,other models to use
1255.44,3.76,so by using the analytics environment to
1257.44,2.479,do our feature engineering and not doing
1259.2,3.04,it in a
1259.919,4.081,siloed tool we're promoting reuse of all
1262.24,4.799,that great work
1264.0,5.52,secondly the data warehouse has standard
1267.039,3.681,tools to operationalize and run these on
1269.52,3.68,a nightly basis
1270.72,3.28,we call it etl or extract transform and
1273.2,2.8,load
1274.0,3.52,and that those tools are very valuable
1276.0,2.72,in productionalizing that code it
1277.52,5.039,becomes much easier
1278.72,7.28,to productionalize than in a script in a
1282.559,3.441,in one of the machine learning languages
1289.44,4.719,so going back to our three key
1292.64,2.96,recommendations remember the first was
1294.159,2.961,to fully leverage the analytics
1295.6,4.8,environment
1297.12,8.4,and the next is to standardize tools and
1300.4,8.08,methods using production quality code
1305.52,4.48,as you start to put forth a data science
1308.48,2.48,machine learning predictive analytics
1310.0,2.96,strategy
1310.96,3.52,you need lots of smart people to do this
1312.96,2.8,this shouldn't be a surprise
1314.48,3.36,and the two roles that i want to talk
1315.76,2.96,today are they're similar but different
1317.84,3.44,roles
1318.72,3.76,so the data scientist formulates
1321.28,3.279,hypotheses about
1322.48,3.52,features driving a predictive model the
1324.559,3.041,data scientist is the one
1326.0,4.08,who's talking to clinicians and trying
1327.6,4.88,to understand the underlying causes
1330.08,3.36,of uh what what is trying to be
1332.48,2.72,predicted
1333.44,4.0,the data scientist is doing what we call
1335.2,4.08,experiments and trying various models
1337.44,3.359,to determine the best approach for
1339.28,3.519,prediction
1340.799,3.281,and the data scientist is assessing the
1342.799,3.441,model output
1344.08,4.16,and looking at the accuracy and trying
1346.24,5.04,to decide on what the best
1348.24,5.04,approach is the machine learning
1351.28,3.2,engineer like i said it's a similar but
1353.28,3.12,different so the machine learning
1354.48,3.28,engineer has to have a lot of knowledge
1356.4,3.92,of data science
1357.76,3.68,but one of the the challenging things is
1360.32,3.2,to find somebody who has
1361.44,3.119,a knowledge of data science and a
1363.52,3.039,knowledge of
1364.559,3.441,software engineering best practices
1366.559,2.161,because remember we're talking about
1368.0,2.96,generating
1368.72,4.319,production quality code and one of the
1370.96,4.64,biggest impacts we've had on our group
1373.039,4.161,was when we hired levi who's got a great
1375.6,4.0,machine learning engineer
1377.2,3.04,um approach he understands the data
1379.6,2.64,science
1380.24,3.439,and he also has a knowledge of software
1382.24,2.4,engineering best practices and that's
1383.679,2.88,really helped us
1384.64,3.76,to scale and we'll talk about what we
1386.559,2.801,mean by scale but a machine learning
1388.4,2.639,engineer
1389.36,3.6,is a wonderful thing to have and we'll
1391.039,4.241,talk about the fruits of our
1392.96,4.959,machine learning engineering efforts a
1395.28,2.639,little bit later
1398.24,4.96,so in order to talk about what kind of
1401.919,3.12,code you need
1403.2,4.88,i think it's good to talk a little bit
1405.039,5.281,about the predictive analytics processes
1408.08,3.44,and what is it that a data scientist is
1410.32,2.64,doing that we want to try to
1411.52,4.48,operationalize
1412.96,4.959,and there's two pieces to this one is a
1416.0,3.36,development process and let's use the
1417.919,2.561,example of a readmission prediction
1419.36,1.76,let's say we're trying to develop a
1420.48,3.679,model
1421.12,4.96,to predict readmissions
1424.159,3.041,the data scientist is going to first of
1426.08,2.8,all identify
1427.2,3.12,which patients were readmitted and which
1428.88,4.32,patients weren't that's important to
1430.32,3.52,understand what the outcomes were and
1433.2,3.599,then
1433.84,4.56,they're going to gather 30 to 40 feature
1436.799,3.281,inputs and this is where hypothesis
1438.4,3.279,generation takes over
1440.08,3.36,they're gonna they're hypothesizing what
1441.679,5.201,are the 30 or 40
1443.44,6.0,most likely things to drive readmissions
1446.88,3.279,and that data set the 30 to 40 input
1449.44,3.28,features
1450.159,3.441,and the outcome is then split into two
1452.72,3.76,pieces
1453.6,4.48,one we call the training set and one we
1456.48,3.76,call the test set
1458.08,3.52,and the training set is what we crunch
1460.24,3.12,all the numbers on and that's where our
1461.6,4.319,model is generated from
1463.36,4.319,the test set is how we use is what we
1465.919,2.24,use to measure the performance of that
1467.679,2.401,model
1468.159,3.601,so it's important to hold back some data
1470.08,2.479,so we can see how well our prediction
1471.76,2.799,would have done
1472.559,3.441,on predicting the the items in the test
1474.559,3.12,set and so
1476.0,3.12,the data scientist is running multiple
1477.679,2.161,algorithms on that training set they're
1479.12,2.24,looking at
1479.84,3.199,lots of different combinations of
1481.36,2.48,features and lots of different
1483.039,2.081,algorithms
1483.84,3.439,and for each one of those they're
1485.12,3.679,measuring the performance and deciding
1487.279,3.041,what the best model is and it's an
1488.799,2.161,iterative process so i've drawn this
1490.32,1.92,arrow
1490.96,3.28,going back to the beginning sometimes
1492.24,3.52,you need to go back to square one
1494.24,2.96,but eventually you get to the what you
1495.76,2.88,see in the orange box where you've got a
1497.2,3.839,best algorithm
1498.64,4.639,and a list a smaller list of important
1501.039,4.401,features usually around 10 or so
1503.279,4.241,once you've developed your model you can
1505.44,4.64,then store those parameters for
1507.52,4.8,later use again the development process
1510.08,4.479,is where the really intense computation
1512.32,3.68,we're looking at millions of records um
1514.559,2.961,and and crunching numbers and looking
1516.0,2.08,for patterns in those and extracting the
1517.52,3.039,patterns
1518.08,3.92,but once we get to a model the next step
1520.559,4.081,is to
1522.0,3.76,run the model and running the model is
1524.64,4.32,what occurs
1525.76,4.56,every day multiple times a day much less
1528.96,2.8,computationally intensive
1530.32,3.359,using the output of the development
1531.76,3.12,process now if we're going to do a
1533.679,2.641,readmission prediction
1534.88,3.2,we don't need to crunch numbers on
1536.32,4.32,millions of patients of data
1538.08,4.4,billions of patients of data we've done
1540.64,4.48,that in the development process now
1542.48,4.0,it's a matter of looking at who are the
1545.12,3.039,patients who just came in
1546.48,3.6,let's get those 10 important features on
1548.159,3.281,those and run it one record at a time
1550.08,3.12,calculate that prediction
1551.44,3.2,and output it to the data warehouse so
1553.2,3.599,running the model
1554.64,3.44,much less computationally intensive but
1556.799,2.961,this is the part that gets put in
1558.08,3.599,production and is run every day
1559.76,3.44,um either as part of an etl process or
1561.679,2.641,part of a web service we'll talk about
1563.2,3.52,the different ways
1564.32,3.68,that it can be deployed these are just
1566.72,3.76,the two different things that the
1568.0,4.559,machine learning code should be able to
1570.48,3.76,address
1572.559,3.12,in the development process it's
1574.24,3.52,important to standardize
1575.679,3.521,on on pieces of that and running the
1577.76,3.919,model that's where we want to have
1579.2,6.64,really robust tested code so we
1581.679,4.161,can put it in production
1586.32,4.88,so why do you what you know i want to
1588.159,4.961,address why you would want a code base a
1591.2,3.359,software to help you do this there's a
1593.12,3.12,lot of tools out there to make it really
1594.559,5.281,easy to write some of these scripts
1596.24,4.16,but it's important to focus the data
1599.84,2.079,science
1600.4,3.44,on the model development and not
1601.919,4.24,necessarily writing the code
1603.84,3.52,the code is something that's
1606.159,3.601,standardizable
1607.36,3.76,and the data scientist part the
1609.76,3.36,questions that they're asking
1611.12,3.919,what features do i use for input how do
1613.12,4.32,i model those features in the database
1615.039,4.24,how do i compare the performance of
1617.44,4.4,these two different models that's the
1619.279,4.0,real value add of a data scientist not
1621.84,3.439,necessarily writing code
1623.279,3.601,or possibly reinventing the wheel that
1625.279,3.121,somebody in their department may have
1626.88,3.44,already done
1628.4,4.48,so having a standard code base also
1630.32,4.719,allows a team of data scientists
1632.88,4.0,to standardize their methodologies it's
1635.039,2.321,a real problem if your data scientists
1636.88,2.399,are
1637.36,3.28,using two different pieces of software
1639.279,3.041,to create their models
1640.64,2.8,and even more of a problem if they're
1642.32,2.719,measuring the performance of their
1643.44,2.719,models in different ways
1645.039,2.721,how are you ever going to know what the
1646.159,2.481,best model is if we're using different
1647.76,2.96,yard sticks
1648.64,3.519,so that standardization piece is
1650.72,3.36,important here too to have an
1652.159,3.76,organizational code base that data
1654.08,4.64,scientists can use so that they're using
1655.919,2.801,the same methods
1658.96,3.439,and then finally the point that i've
1660.399,2.561,made a bunch of times and i probably
1662.399,2.481,won't make
1662.96,3.28,much more than this is that putting
1664.88,2.96,models into production
1666.24,2.88,really requires that production quality
1667.84,4.4,code we don't want to put anything that
1669.12,3.12,might break into production
1672.64,4.399,and as we were developing our machine
1675.919,3.201,learning code base
1677.039,3.041,we thought it was really important to
1679.12,3.919,adhere
1680.08,5.44,to software development best practices
1683.039,5.041,and software development best practices
1685.52,4.08,are used in the software development
1688.08,2.319,world to solve a lot of these same
1689.6,3.36,problems so
1690.399,4.801,how do we create a robust reusable code
1692.96,2.24,base
1695.52,3.68,one of the first things that we did was
1697.039,3.921,use version control and version control
1699.2,4.24,is used by software developers
1700.96,5.68,it allows multiple developers to
1703.44,6.08,contribute code to a single repository
1706.64,4.399,and by keeping it as a single repository
1709.52,3.92,many people can be editing
1711.039,4.0,the same code base at the same time and
1713.44,2.0,then there's tools to make sure that
1715.039,1.841,people
1715.44,3.76,don't step on each other's toes and when
1716.88,5.84,there is a conflict that it can be
1719.2,6.24,resolved so it's really important for
1722.72,3.36,teams of data scientists to have version
1725.44,4.239,control
1726.08,3.599,with their code base
1730.08,3.599,the other thing that's very important
1731.84,3.12,for this is unit testing and unit
1733.679,4.24,testing has been used
1734.96,3.76,in the software development world uh
1737.919,3.681,many many
1738.72,4.079,for many many years and the idea of unit
1741.6,3.28,testing is that as
1742.799,3.6,software becomes more modular and more
1744.88,3.919,reused it becomes
1746.399,3.681,a lot easier to accidentally break
1748.799,3.36,software
1750.08,4.16,and a good software code base is
1752.159,3.601,efficient and it is reusing code
1754.24,3.28,but you've got to make sure that as you
1755.76,3.36,make changes your
1757.52,4.399,your changes are not resulting in
1759.12,5.36,unexpected consequences so unit testing
1761.919,3.441,basically does testing of almost of all
1764.48,2.799,of the functions
1765.36,3.84,in your software to make sure that the
1767.279,3.441,output is as expected so if i make a
1769.2,3.76,change to the software
1770.72,3.199,and and that change i'm not sure how
1772.96,2.48,it's going to affect the rest of the
1773.919,2.321,software if i run the unit tests and
1775.44,2.479,they all run
1776.24,3.919,i can be fairly confident that i haven't
1777.919,3.601,broken anything downstream
1780.159,3.12,so these are some of the software
1781.52,3.36,development best practices
1783.279,3.28,that are that are required for having a
1784.88,4.32,good code base
1786.559,4.161,there's also things like documentation
1789.2,3.04,how do we get people to find
1790.72,3.52,all of the functionality available in
1792.24,4.64,the software and
1794.24,3.919,continuous integration these are all all
1796.88,4.48,best practices that
1798.159,6.4,that we use in the development of our
1801.36,3.199,machine learning code base
1804.799,2.961,so if you're going to if you're going to
1805.84,2.16,embark on developing a machine learning
1807.76,2.799,code
1808.0,3.76,base and please stay on for the entire
1810.559,2.72,presentation because
1811.76,3.12,we have good reasons why you might not
1813.279,5.28,want to develop your own
1814.88,5.6,but if you are there's a few technology
1818.559,5.201,choices out there
1820.48,3.919,one is are an r is a language that's
1823.76,2.88,been
1824.399,3.76,involved that's been deployed and deeply
1826.64,3.279,entrenched in healthcare
1828.159,3.681,i'm sure most of the analysts and
1829.919,5.201,statisticians are at least familiar with
1831.84,4.319,are on the call um it has been around
1835.12,3.6,for a long time
1836.159,3.441,and because of it being an analytics
1838.72,2.8,environment
1839.6,3.36,it is more familiar to analysts and
1841.52,5.2,statisticians
1842.96,4.319,python is another uh language that's out
1846.72,3.439,there
1847.279,4.241,it's a fully functional software
1850.159,4.161,development language
1851.52,3.92,um the language itself isn't new but a
1854.32,2.8,lot of the tools that have been
1855.44,3.44,developed for machine learning
1857.12,3.679,are newer and there's a lot lots of
1858.88,4.72,momentum behind python
1860.799,3.36,as a matter of fact a lot of the online
1863.6,3.52,learning
1864.159,4.4,based in machine learning uses python as
1867.12,2.799,the language in which
1868.559,3.441,a lot of the new data scientists are
1869.919,2.961,being trained and python is more
1872.0,2.799,familiar
1872.88,4.32,to software developers and data analysts
1874.799,6.081,because it is kind of a full-featured
1877.2,6.16,software programming language azure ml
1880.88,3.76,is a cloud-based solution from microsoft
1883.36,3.52,because it's cloud-based
1884.64,3.44,it's very easy to set up and deploy
1886.88,2.96,there's no
1888.08,3.76,installation required you can just kind
1889.84,2.88,of create a azure ml account and start
1891.84,3.199,creating
1892.72,4.0,models in the cloud because it's
1895.039,2.801,cloud-based and because we're in
1896.72,3.28,healthcare
1897.84,4.16,the adoption of azure ml is a little bit
1900.0,3.919,less than than you might expect and
1902.0,3.679,you have read some stories about
1903.919,2.64,organizations that are leveraging azure
1905.679,2.48,ml
1906.559,3.761,for predictive analytics and healthcare
1908.159,3.681,and they have to de-identify and scrub
1910.32,2.64,their data before they put in an address
1911.84,2.88,to do their models
1912.96,3.199,and even the example that i read they
1914.72,3.28,were working with dates and they had to
1916.159,4.161,mask the dates
1918.0,3.76,and a date is actually an input to your
1920.32,3.199,predictive model to me that
1921.76,3.519,that's a little bit risky to start
1923.519,3.28,manipulating dates to mask the data if
1925.279,3.201,you want to get a good predictive model
1926.799,3.841,so for that reason i think azure ml
1928.48,3.439,hasn't seen the widespread adoption in
1930.64,2.24,healthcare that you might expect and
1931.919,3.36,that we've seen
1932.88,4.24,in other industries there's plenty of
1935.279,2.721,other choices but i think the industry
1937.12,3.679,right now
1938.0,5.039,is standardizing on r and python and
1940.799,4.321,that's where we put our efforts we have
1943.039,4.401,developed software in both r and python
1945.12,4.88,to do our machine learning code base and
1947.44,4.479,the reason why we chose both is r is
1950.0,4.88,probably more popular right now
1951.919,5.281,there's support from major vendors like
1954.88,3.84,sql server microsoft sql server
1957.2,2.959,python is more of the up and coming
1958.72,2.16,approach so we want to be ready for both
1960.159,1.76,of those
1960.88,3.039,and our clients have different
1961.919,4.48,preferences as well so we we address
1963.919,2.48,both of them
1968.64,4.8,so our code base includes tools for data
1971.919,2.961,ingestion
1973.44,3.119,so we've been talking a lot about how do
1974.88,2.96,we leverage the analytics environment
1976.559,1.681,with our machine learning code well
1977.84,2.24,we've
1978.24,3.76,got to be able to very quickly and
1980.08,3.199,easily get data out of that environment
1982.0,3.039,into our code base
1983.279,3.841,so we have routines that load data from
1985.039,5.36,the database or a flat
1987.12,4.559,file date and time is important in
1990.399,4.481,machine learning so
1991.679,4.561,we have tools that allow us to uh expand
1994.88,3.2,date times into
1996.24,3.6,things like day of the week week of the
1998.08,4.4,year make that really easy
1999.84,3.839,missing values can really complicate and
2002.48,3.28,and
2003.679,3.441,make predictions not very good so how we
2005.76,3.36,have a couple of routines for dealing
2007.12,3.919,with those in different ways and
2009.12,3.6,by all means the the way that you deal
2011.039,2.401,with missing values is different for
2012.72,2.16,different
2013.44,3.76,models and different use cases so we
2014.88,4.32,want to provide functions for that
2017.2,3.44,we also provide a large tool set around
2019.2,2.959,the model development this is all the
2020.64,1.919,number quenching that we talked about in
2022.159,2.561,that
2022.559,3.36,in that workflow of a data scientist so
2024.72,3.92,splitting that data
2025.919,3.921,between test and training doing feature
2028.64,3.44,selection how do we
2029.84,3.679,get from 40 features down to 10 features
2032.08,2.719,and then of course the machine learning
2033.519,3.361,algorithms themselves
2034.799,4.24,what are we running on the data random
2036.88,5.76,forest is a very popular algorithm
2039.039,5.681,lasso is a regression based uh method
2042.64,3.759,and then mixed models are coming that
2044.72,2.88,those help us deal with longitudinal
2046.399,3.921,data and healthcare
2047.6,3.279,easier and k-means clustering which we
2050.32,3.359,will be
2050.879,4.0,using uh extensively next year in our
2053.679,2.801,code base as well
2054.879,3.121,and then all of the tools to evaluate
2056.48,2.72,that performance and help the data
2058.0,4.159,scientists decide
2059.2,2.959,what's my best model
2062.72,3.199,in addition to the development tools we
2064.24,2.159,have analysis tools so how do we
2065.919,2.96,generate
2066.399,3.44,a performance report for the models that
2068.879,3.28,we're creating
2069.839,4.24,and then tools like to help identify
2072.159,3.841,with trend identification
2074.079,4.0,and be able to perform risk adjusted
2076.0,5.28,comparisons are part of the analysis
2078.079,3.201,suite in our code base
2081.919,4.24,the great thing about the the software
2084.32,4.72,is that it's really helped us to scale
2086.159,4.401,people and when i when we think about
2089.04,3.2,what are the big challenges and data
2090.56,2.4,scientists and this has come up over and
2092.24,3.119,over again
2092.96,4.0,the big challenges is that feature
2095.359,2.561,engineering piece and how do we how do
2096.96,3.6,we represent
2097.92,4.24,the data and it turns out data
2100.56,2.96,architects have great domain knowledge
2102.16,3.76,of how to do that they've been
2103.52,3.76,moving data and healthcare and analyzing
2105.92,3.28,data and
2107.28,3.68,developing the routine to get data into
2109.2,3.52,different uh transforming data into
2110.96,3.28,usable formats for for years
2112.72,3.44,they're also often looking for
2114.24,4.16,opportunities to advance
2116.16,4.4,their career and skills and what we
2118.4,4.0,found is that given the right tools
2120.56,3.2,data architects make incredible feature
2122.4,3.28,engineers given their
2123.76,3.68,years and years of experience in
2125.68,3.439,manipulating data
2127.44,4.08,we're just applying them to a different
2129.119,4.641,problem and it works really well
2131.52,4.0,and then what our code has done is it
2133.76,2.96,has allowed data architects to easily
2135.52,3.76,get started
2136.72,3.84,in actually running predictive analytics
2139.28,3.12,algorithms
2140.56,3.6,and this is a quote from one of our data
2142.4,3.679,architects who was
2144.16,4.64,using our software to create a
2146.079,5.601,predictive model in one of his products
2148.8,4.72,and this is peter monaco and he said one
2151.68,3.6,awesome thing about the output from the
2153.52,3.52,r package you put together
2155.28,3.36,is the output aligns perfectly with
2157.04,2.4,creating patient stratification
2158.64,2.24,algorithms
2159.44,3.6,the fact that i feel comfortable running
2160.88,2.8,this stuff speaks to how easy you've
2163.04,2.4,made it
2163.68,3.04,thanks again levi and he's thanking levi
2165.44,2.639,who's gonna you're gonna hear from a
2166.72,4.24,little bit
2168.079,4.641,but this is great it allowed peter to do
2170.96,2.639,what he does really well get the data in
2172.72,2.32,a good format
2173.599,3.281,and lower the barrier for him to
2175.04,3.44,actually run these algorithms and do
2176.88,2.08,some of the work that a data scientist
2178.48,2.4,does
2178.96,3.2,so we see it very promising for helping
2180.88,3.84,us to scale
2182.16,3.919,our our machine learning efforts across
2184.72,4.0,a large number of people in the
2186.079,2.641,organization
2191.119,3.681,so now it comes time to put models in
2193.04,3.28,production and we're going to talk first
2194.8,3.039,about how we move them into production
2196.32,4.16,from a technical standpoint
2197.839,4.881,and then how do we move them into a into
2200.48,2.24,a
2202.96,4.48,an application or view that can actually
2205.76,3.68,change business process or
2207.44,3.2,or better yet provide better care for
2209.44,4.639,patients
2210.64,5.04,so modality number one is to
2214.079,4.0,put the model into production leveraging
2215.68,2.88,the etl process and this is appropriate
2218.079,2.481,if
2218.56,3.6,the prediction is not based on highly
2220.56,4.64,dynamic data
2222.16,5.04,or if the intervention strategy is okay
2225.2,3.12,with some level of latency so
2227.2,3.12,an example of this would be a
2228.32,4.32,readmission prediction typically
2230.32,4.0,readmission algorithms are not based on
2232.64,4.24,highly dynamic data
2234.32,4.24,they are based on data that's not
2236.88,3.12,changing super fast so if we're pulling
2238.56,4.32,data on a nightly basis
2240.0,4.56,or every 12 hours a readmission
2242.88,2.4,algorithm is generally going to be okay
2244.56,3.12,with that
2245.28,3.2,and in this case we just put the machine
2247.68,2.8,learning code
2248.48,5.04,in the middle of the etl process so
2250.48,5.92,we've got etl to load the data sources
2253.52,4.319,we've got etl that data scientists or
2256.4,3.199,data architects create
2257.839,3.76,that load those engineered features the
2259.599,3.281,inputs to the predicted model
2261.599,2.561,and then we run that code that can
2262.88,2.0,easily grab those features from the
2264.16,3.199,database
2264.88,4.32,and output a prediction to the database
2267.359,3.681,our machine learning code
2269.2,3.44,can also write these predictions to the
2271.04,4.64,database and this is how we've
2272.64,7.12,deployed several models it's easy and it
2275.68,4.08,just wraps right up with the etl process
2280.32,4.48,modality number two is when the data is
2282.88,2.479,more dynamic so an example of this would
2284.8,2.0,be
2285.359,3.601,sepsis early detection where we're
2286.8,4.16,looking at changes in vital signs
2288.96,3.76,and the intervention strategy we can't
2290.96,3.6,wait up to 24 hours to intervene when
2292.72,4.879,sepsis happens it's something we need to
2294.56,4.24,intervene faster on so in this case we
2297.599,4.24,can deploy
2298.8,5.44,the uh predictive algorithm as a web
2301.839,5.121,service and the web service is receiving
2304.24,4.72,real-time features so those changes and
2306.96,4.159,vital signs are those vital signs
2308.96,3.44,that's going to come in in a very from a
2311.119,2.641,very dynamic setting
2312.4,3.28,and we might still be using some
2313.76,3.44,historic features like what
2315.68,3.28,what are the demographics what's the age
2317.2,3.04,of the patient that we can pull from the
2318.96,3.119,edw
2320.24,3.92,and then the web service will be
2322.079,3.201,combining that live input with that
2324.16,2.72,historic data
2325.28,3.76,to run that machine learning code and
2326.88,3.76,then output the model back into the
2329.04,3.68,into the application so this is
2330.64,6.0,definitely designed for more
2332.72,3.92,dynamic situations and more dynamic
2338.839,4.52,predictions
2340.64,4.479,so going back to this point of deploying
2343.359,4.161,with a strategy for intervention
2345.119,4.081,this is the real i would call this the
2347.52,5.36,most important point of the presentation
2349.2,6.879,so the idea here is how do we deploy
2352.88,5.04,and get these predictions to actually
2356.079,3.28,impact care and we're going to talk
2357.92,3.919,about a little case study
2359.359,4.161,that we did with one of our clients on
2361.839,4.081,central line associated bloodstream
2363.52,4.8,infections or clabsis
2365.92,3.84,approximately 41 000 patients actually
2368.32,3.6,end up with this condition
2369.76,3.2,41 000 patients in the u.s per year that
2371.92,2.64,should read
2372.96,3.68,and actually one in four patients that
2374.56,3.92,get a clab c will die
2376.64,3.28,so it's a very serious condition and
2378.48,2.72,organizations are really struggling to
2379.92,2.88,keep up with this there's great
2381.2,2.48,guidelines out there evidence-based
2382.8,3.36,guidelines
2383.68,3.04,for how to how to care for patients such
2386.16,3.36,that we
2386.72,4.399,reduce the likelihood of a clabsi and we
2389.52,3.599,work with a client to develop
2391.119,3.201,retrospective and analytics to look at
2393.119,2.641,their compliance
2394.32,3.12,and it really helped highlight some
2395.76,4.24,problems and they got really good
2397.44,4.399,at using the data to find problem areas
2400.0,3.44,and then developing interventions to fix
2401.839,3.121,those so they developed the muscle
2403.44,3.12,memory of using data
2404.96,3.28,to improve their care and business
2406.56,4.16,processes then
2408.24,3.839,they said okay take us to the next step
2410.72,2.8,now we don't want to know
2412.079,3.04,where we failed we want to know what's
2413.52,2.319,coming next who are the patients at high
2415.119,3.281,risk
2415.839,3.441,for clabsi so that we can intervene with
2418.4,3.28,them
2419.28,3.76,and and so they came to levi and levi
2421.68,3.52,and his team
2423.04,3.76,developed a predictive algorithm that's
2425.2,3.119,based on 16 features
2426.8,4.08,that predicts the likelihood that a
2428.319,3.841,patient's going to develop epilepsy
2430.88,3.92,we'll see what that looks like in a
2432.16,4.72,minute it's important that
2434.8,4.0,every model that we develop in every
2436.88,3.76,model that we deploy
2438.8,3.36,comes with a performance report and this
2440.64,2.8,performance report is not a highly
2442.16,2.72,technical report
2443.44,4.0,and the idea here is that we're trying
2444.88,4.479,to briefly summarize
2447.44,3.6,what we're trying to predict the
2449.359,3.281,variables that were considered in that
2451.04,4.16,are the future inputs
2452.64,3.439,that were considered in developing that
2455.2,2.96,model
2456.079,4.641,what model we chose and the accuracy of
2458.16,4.32,that model and this report is not used
2460.72,3.68,for technical people but this report is
2462.48,4.32,used for business or clinical people
2464.4,3.919,to help them understand that algorithm
2466.8,4.319,the communication about
2468.319,3.681,what an algorithm does is extremely
2471.119,3.921,critical
2472.0,4.8,to the adoption of that model when
2475.04,3.44,discussing models with clinicians
2476.8,2.72,clinicians will adopt predictive
2478.48,2.879,analytics
2479.52,3.44,in so far as they understand it if they
2481.359,2.72,don't understand what's going on
2482.96,3.04,it's going to be a much harder
2484.079,3.28,conversation because if you think about
2486.0,2.96,what clinicians do
2487.359,3.521,they're running predictive algorithms in
2488.96,3.28,their head all day they're looking at
2490.88,3.84,large amounts of patient data and
2492.24,4.16,boiling that down to hypotheses or
2494.72,3.68,conclusions about those patients
2496.4,3.679,what we try to do with machine learning
2498.4,3.28,is standardize that typically doctors
2500.079,2.24,don't do that all in the same way so we
2501.68,2.24,help them
2502.319,3.201,to standardize it and if they understand
2503.92,2.88,how we're doing it and it's close to
2505.52,1.92,what they're doing they're much more
2506.8,3.519,likely
2507.44,2.879,to adopt it
2510.72,3.119,the other thing point that we should
2512.319,2.641,make when talking about deploying
2513.839,3.201,predictive models
2514.96,4.639,is that complexity comes at a price and
2517.04,5.84,it comes at an extreme price sometimes
2519.599,5.121,so a regression model can often strike a
2522.88,3.04,balance between predictive value and
2524.72,3.52,interpretability
2525.92,3.76,regression models are one way to do
2528.24,3.04,predictive analytics and there are
2529.68,3.36,more sophisticated machine learning
2531.28,2.64,algorithms that are much harder to
2533.04,3.279,explain
2533.92,3.199,so if you have two models a regression
2536.319,2.401,model
2537.119,3.361,and a more advanced model that might
2538.72,4.96,have marginally better
2540.48,3.92,uh accuracy the regression model still
2543.68,2.56,may be the
2544.4,3.6,more favorable model to deploy because
2546.24,4.32,it's easier to explain
2548.0,3.92,and in our code we use a process called
2550.56,4.4,regularization
2551.92,5.28,that that penalized complexity so there
2554.96,3.2,the lasso algorithm is a regression
2557.2,3.28,algorithm that
2558.16,4.8,actually has built into it the ability
2560.48,5.359,for the model itself to tune itself into
2562.96,4.56,create a favor a more simple model and
2565.839,3.601,the way that wikipedia says it is
2567.52,3.36,that enhance the prediction enhances the
2569.44,3.6,prediction accuracy
2570.88,3.6,and the interpretability of the model so
2573.04,3.2,super super important
2574.48,3.119,complexity comes at a price especially
2576.24,3.04,in the clinical setting for
2577.599,5.441,organizations that are new
2579.28,5.36,to predictive analytics
2583.04,3.12,and what does it all look like now this
2584.64,2.0,is the this is the punch line this is
2586.16,3.439,what
2586.64,4.0,this is what predictive analytics looks
2589.599,2.801,like when it's put in front of
2590.64,3.28,clinicians and they're
2592.4,3.12,giving them the ability to make
2593.92,2.64,decisions based on this data this is
2595.52,3.52,like the
2596.56,3.6,uber telling me five minutes to a car on
2599.04,2.799,my watch
2600.16,4.08,and what you see here is a unit
2601.839,5.28,scoreboard for
2604.24,3.52,our client who was doing uh cloud season
2607.119,3.121,i have to say
2607.76,4.319,this was all generated on scrubbed the
2610.24,4.879,identified data set
2612.079,4.561,so but it actually fairly accurately
2615.119,2.96,reflects what the client was seeing in
2616.64,2.959,their environment so i just want to make
2618.079,2.481,it clear that this is all the identified
2619.599,3.921,data
2620.56,4.0,but what you see here is in the green
2623.52,3.599,box
2624.56,4.48,there are 12 patients who are active
2627.119,3.2,risk for clabsi on this unit and the
2629.04,4.64,unit reviews this
2630.319,6.081,dashboard multiple times per day
2633.68,4.159,and what you see below in this list of
2636.4,3.28,patients here
2637.839,3.361,is that the name of the patient again
2639.68,4.24,scrubbed
2641.2,4.24,and the probability that they actually
2643.92,3.28,have epilepsy and the highest risk
2645.44,3.84,patients are at the top here so the
2647.2,3.36,the eye goes immediately to the patients
2649.28,4.16,who are at the highest risk this
2650.56,4.08,this top patient has a 64 chance of
2653.44,3.2,developing
2654.64,3.04,a central line associated blood stream
2656.64,2.719,infection
2657.68,3.6,and the important thing here is not to
2659.359,4.401,just give that number
2661.28,3.36,but what goes on in this far right hand
2663.76,3.12,column
2664.64,4.0,this far right hand column shows the
2666.88,2.959,risk factors that are driving that
2668.64,5.12,prediction
2669.839,5.52,so and moreover it's not just
2673.76,3.52,the risk factors it's what we call
2675.359,4.401,modifiable risk factors so
2677.28,3.44,the patient's age is often a risk factor
2679.76,3.359,in their
2680.72,4.0,their development of a classy however
2683.119,2.321,there's not much a clinician can do
2684.72,2.399,about that
2685.44,3.12,so what we do is we show the factors
2687.119,2.561,that the clinician can actually change
2688.56,3.759,or modify
2689.68,3.439,to to get that patient to a lesser risk
2692.319,3.361,state
2693.119,4.561,and in many cases it is the number of
2695.68,3.84,days that they've been on a central line
2697.68,3.439,the number of uh the placement of that
2699.52,3.04,line there's all sorts of factors that
2701.119,2.321,drive it and what the clinicians do is
2702.56,3.519,they look at this
2703.44,4.159,as a whole and decide what can i do to
2706.079,3.04,reduce that patient's chance of
2707.599,3.441,developing a cloud suit
2709.119,3.041,we are really excited about this it's
2711.04,2.64,been deployed
2712.16,3.12,in our in a production environment one
2713.68,3.36,of our clients and we're working with
2715.28,4.4,them to understand
2717.04,3.039,term how will this affect the the clabsi
2719.68,3.36,rate
2720.079,4.161,for for their patients but this is just
2723.04,3.039,one example
2724.24,4.079,of what it means to put predictive
2726.079,3.04,analytics in the hands of clinicians who
2728.319,5.52,can make
2729.119,4.72,decisions uh of care based on this
2734.72,3.92,the models we've built to date this is
2736.4,4.24,just a listing of them um
2738.64,3.76,klabsy is just one of many models we
2740.64,4.0,wanted to focus on one
2742.4,4.08,high impact example but as you can see
2744.64,3.439,we have a lot of algorithms that we've
2746.48,5.04,built to date that are driving
2748.079,5.361,decisions across the country and um
2751.52,3.52,lots more in development and lots and
2753.44,3.36,lots of ideas so
2755.04,3.52,this highlights i think the need for us
2756.8,3.76,to scale our machine learning
2758.56,3.759,and predictive capability there's only
2760.56,3.2,so much that one team can use and that's
2762.319,3.121,part of our strategy
2763.76,4.16,is to use that software that we've
2765.44,4.56,developed to make it easier for
2767.92,4.56,lots of people in the organization to to
2770.0,5.68,be able to do this
2772.48,5.119,so we've got one more poll question
2775.68,3.679,what are the top three important data
2777.599,2.801,sources to your organization in making
2779.359,4.081,predictions
2780.4,4.08,clinical emr data claims data patient
2783.44,4.32,outcomes data
2784.48,5.76,financial data non-medical patient data
2787.76,4.24,patient satisfaction data or unsure or
2790.24,3.119,not applicable
2792.0,3.52,all right we've got that poll question
2793.359,4.321,up eric and like everybody know this is
2795.52,4.24,a multiple selection so please select
2797.68,3.6,uh up to three if applicable we'll leave
2799.76,4.48,this open for a minute
2801.28,4.559,we'd like to remind everyone to please
2804.24,2.8,type in your questions or comments in
2805.839,3.52,the chat pane
2807.04,4.96,of your control panel got a lot coming
2809.359,5.48,in here that's great
2812.0,4.88,all right let's go ahead and share the
2814.839,4.28,results
2816.88,3.6,this is great okay patient outcomes data
2819.119,2.96,that's that's great
2820.48,3.28,um that's something that we're seeing a
2822.079,2.24,large trend in the in the industry as
2823.76,2.319,well
2824.319,3.601,clinical emr and claims of course are
2826.079,3.921,very popular um
2827.92,3.76,the patient outcomes data is definitely
2830.0,3.52,a hot topic especially
2831.68,4.48,patient reported outcomes how do we how
2833.52,3.839,do we better measure those outcomes
2836.16,4.48,and of course that's what we're trying
2837.359,4.801,to predict is outcomes in most cases so
2840.64,2.959,thank you for taking the time to respond
2842.16,4.0,to the polls they're very they're very
2843.599,2.561,insightful
2847.04,3.519,so just to reiterate our three
2848.559,3.52,recommendations fully leverage your
2850.559,3.201,analytics environment do the data
2852.079,4.641,manipulation in the data warehouse
2853.76,5.599,it's easier to reuse it and it's easier
2856.72,4.96,to operationalize
2859.359,3.2,standardize using uh production quality
2861.68,3.52,code so
2862.559,3.841,having your group using the same
2865.2,2.96,repository
2866.4,3.36,increases economy of scale and it allows
2868.16,3.28,you to deploy
2869.76,4.0,the ability to do predictive analytics
2871.44,3.919,to more people
2873.76,3.2,and then finally deploying with a
2875.359,2.561,strategy for intervention always think
2876.96,2.56,about how
2877.92,4.56,the data is going to be used to make
2879.52,4.4,decisions and before i cut over to levi
2882.48,3.839,i just want to talk briefly about what
2883.92,4.639,the future holds here
2886.319,4.0,what we see of course is that the
2888.559,4.321,clinical workflow engine is still the
2890.319,4.721,ehr clinicians spend most of their time
2892.88,4.08,in in the electronic health record and
2895.04,3.12,that's where the insights are going to
2896.96,3.84,be delivered to them
2898.16,4.08,that influence their care decisions and
2900.8,2.64,in today's world we hear a lot about
2902.24,4.16,smart on fire
2903.44,4.72,this is a technology that allows the ehr
2906.4,4.08,workflow to be augmented
2908.16,3.76,through web applications and fire is an
2910.48,3.28,interface that's
2911.92,3.36,designed to sit over the ehr and make it
2913.76,4.24,easy to pull live data
2915.28,3.76,from the ehr and develop web and mobile
2918.0,3.76,applications
2919.04,3.92,that again augment the workflow of the
2921.76,2.88,ehr
2922.96,3.359,where we see the analytics environment
2924.64,4.719,taking place is really
2926.319,5.52,providing a lot of power to this
2929.359,3.841,idea of putting new applications in the
2931.839,3.52,clinical workflow
2933.2,3.76,and the the data warehouse really
2935.359,3.041,becomes the analytics engine that's
2936.96,3.44,driving a lot of the data
2938.4,3.12,that shows up in that workflow so the
2940.4,2.959,data warehouse has
2941.52,3.12,a host of different data sources that
2943.359,2.72,are driving models
2944.64,3.679,we've like i said i referred it to the
2946.079,3.76,beginning as a
2948.319,3.04,feature a chock full of features we've
2949.839,4.161,got registry definitions
2951.359,3.2,we've got text processing nlp algorithms
2954.0,2.079,we've got
2954.559,3.52,all of these predictive algorithms that
2956.079,3.441,we're generating creating almost like an
2958.079,3.28,algorithm library
2959.52,4.0,and then we want to expose that through
2961.359,6.0,an api such that
2963.52,5.039,the real-time data from the ehr
2967.359,3.521,through the fire interface can be
2968.559,4.161,combined with all of that analytic data
2970.88,3.76,and then deliver that to the web and
2972.72,3.2,mobile applications and really
2974.64,3.199,not only augment their workflow but
2975.92,3.199,augment the data that they're seeing and
2977.839,3.76,make it based on
2979.119,4.401,a larger repository of highly valuable
2981.599,3.681,analytic data
2983.52,3.039,so i'm going to cut over to levi now
2985.28,2.48,levi's going to share with you some
2986.559,3.441,exciting news
2987.76,4.16,about our software that we've developed
2990.0,2.8,we have decided to open source our
2991.92,3.04,software
2992.8,4.4,and levi will walk you through how you
2994.96,6.0,can get to our code repository
2997.2,4.399,and and download the software and use it
3000.96,3.2,for your
3001.599,3.281,your team or yourself hi everyone great
3004.16,2.959,job eric
3004.88,4.32,so eric just got a fantastic overview of
3007.119,4.48,best practices in predictive analytics
3009.2,3.919,so you might ask yourself well how can
3011.599,2.881,we take it from here how can we actually
3013.119,2.561,do what you guys have described in our
3014.48,2.8,organization
3015.68,3.52,and that's why we're so excited to open
3017.28,2.72,source this r package we've been working
3019.2,3.44,on so it's called
3020.0,5.28,hd tools is overall project and as you
3022.64,4.479,can see here we're at httools.org
3025.28,4.079,so if you want to get started today
3027.119,4.72,simply type that into your browser
3029.359,4.48,and the idea is that this enables you to
3031.839,4.72,create models on your data
3033.839,3.041,with very simple examples so if we click
3036.559,3.601,in
3036.88,5.52,to the documentation for hdr tools
3040.16,3.76,it very basically describes why it's so
3042.4,4.56,great for healthcare
3043.92,4.8,how to install the package how to get
3046.96,4.639,started with example
3048.72,4.16,and so let's take some scenario that you
3051.599,3.041,might be interested in so say you have a
3052.88,3.36,great data set put together
3054.64,3.84,you know say diabetic data and you're
3056.24,4.079,wanting to predict say readmissions
3058.48,3.2,so you can ask yourself well how does
3060.319,3.681,this tool help me do that
3061.68,3.28,so if we ship if we hop over to our
3064.0,3.52,studio
3064.96,3.04,after you've installed the package what
3067.52,3.039,you do
3068.0,4.0,is you type okay well library hdr tools
3070.559,3.361,so in r
3072.0,4.64,you often load packages in that bring
3073.92,4.0,you know certain types of functionality
3076.64,3.199,and then you simply type the question
3077.92,4.96,mark htr tools
3079.839,4.641,and that will bring up the examples
3082.88,4.56,associated with our package
3084.48,5.2,so nice built-in documentation you can
3087.44,4.639,immediately use to create a model
3089.68,4.32,so once you have a data set you click on
3092.079,4.641,this a lots of development or random
3094.0,3.599,forest development link and that will
3096.72,3.2,give you
3097.599,3.201,both the descriptions of the arguments
3099.92,3.919,to the function
3100.8,4.64,as well as the example code
3103.839,3.441,so what you can do is scroll down and
3105.44,3.2,you can play with a built-in data set
3107.28,3.68,but let's say okay well you have your
3108.64,4.32,data set already ready to go
3110.96,4.96,and so what you do is just basically
3112.96,6.159,grab this example code
3115.92,6.48,and open up a new script
3119.119,4.96,drop it in and hit run
3122.4,4.24,and what it basically will do will tell
3124.079,4.401,you for this particular data set
3126.64,4.32,how well your model did so you have a
3128.48,5.359,lasso model which eric had mentioned
3130.96,4.96,and you see the auc is .86 so that's a
3133.839,3.681,measure of the accuracy
3135.92,3.28,you know say for predicting 30-day
3137.52,3.039,re-admit flag
3139.2,3.44,and then you have a random forest
3140.559,4.481,example as well that had an ac
3142.64,4.24,of 0.82 and so you're able to quickly
3145.04,2.72,see okay well for this particular data
3146.88,2.64,set
3147.76,3.68,we had this particular model that did
3149.52,3.28,really well and so if that's a random
3151.44,4.0,fourth model
3152.8,3.759,you simply use the documentation to go
3155.44,3.919,and deploy that model
3156.559,4.241,and you can see the links there so as
3159.359,4.72,you have your data put together
3160.8,4.64,please visit the website reach out
3164.079,2.881,let us know what you're working on and
3165.44,3.36,how we can improve these tools because
3166.96,3.84,we really want to build a place where we
3168.8,5.039,can all collaborate and build something
3170.8,5.68,that helps everyone in healthcare thanks
3173.839,4.161,levi so again the the url to go to if
3176.48,5.359,you want to download
3178.0,6.96,uh the software is hcrtools.org
3181.839,4.321,sorry sorry sorry hctools.org that was
3184.96,3.68,my mistake
3186.16,4.0,we will be renaming uh and redirecting
3188.64,3.12,the urls at some point
3190.16,3.36,to a more marketing friendly name our
3191.76,3.12,marketing team has informed us that
3193.52,3.28,this is the kind of name that you get
3194.88,4.479,when you get a bunch of data scientists
3196.8,3.92,into a room to name a product uh so we
3199.359,2.681,will be we will be renaming it
3200.72,3.839,eventually but
3202.04,3.799,hctools.org please go there and give us
3204.559,2.961,your feedback on the software if you
3205.839,4.72,want to download it and play with it
3207.52,3.839,make it uh make it part of something
3210.559,2.56,that you
3211.359,4.24,you use in your organization we're happy
3213.119,5.681,to uh answer questions and
3215.599,4.96,and provide that tool for you
3218.8,3.519,all right that's great thanks eric and
3220.559,2.56,we're we're about ready for our q a time
3222.319,2.721,we've got some
3223.119,4.24,good questions in but while we have
3225.04,4.0,those questions in we do have a final uh
3227.359,3.441,poll question for you while these
3229.04,3.039,webinars are intended to be educational
3230.8,3.039,we've had many requests for more
3232.079,3.52,information about health catalyst
3233.839,2.961,who we are what we do if you are
3235.599,2.401,interested in having someone from the
3236.8,2.72,catalyst reach out to you
3238.0,3.44,to schedule a demonstration of any of
3239.52,3.28,our solutions please answer this poll
3241.44,2.879,question now
3242.8,3.12,and while you're answering that we'll go
3244.319,4.081,right to the first
3245.92,3.439,questioning question that we have you've
3248.4,4.32,had a lot of questions
3249.359,5.041,about why uh people are not yet using
3252.72,3.04,predictive analytics in health care
3254.4,3.679,why health care seems to be behind other
3255.76,5.68,industries whether it may be contrast
3258.079,5.921,contract risk or other or other uh
3261.44,4.8,reasons so that's a great question and i
3264.0,4.48,think a lot of it comes down to
3266.24,3.119,the risk reward it is it is riskier to
3268.48,3.68,start
3269.359,3.361,and i think that we are very risk-averse
3272.16,3.28,of course
3272.72,3.2,for good reason uh risk-averse industry
3275.44,2.639,and you know
3275.92,4.159,i look at the predictive analytics that
3278.079,3.681,are delivered to me in netflix
3280.079,3.121,you know they're the wrong analytics i
3281.76,2.96,don't care about those cartoons that are
3283.2,2.8,being suggested to me
3284.72,2.72,and all of that has to do with
3286.0,2.079,assumptions that we're making on the
3287.44,2.8,data
3288.079,4.961,and i think healthcare of course still
3290.24,6.24,has a lot of issues to work out with
3293.04,5.12,trust in the data and and the
3296.48,3.52,underlying quality of the data so
3298.16,3.199,organizations i think who are wary to
3300.0,3.28,use predictive analytics
3301.359,4.161,may also be wary of their underlying
3303.28,2.72,data quality data governance is a topic
3305.52,2.559,that we hear
3306.0,4.079,about a lot lately and helping to
3308.079,3.52,helping organizations to
3310.079,3.121,actually improve the quality of their
3311.599,2.881,data as part of a data strategy is going
3313.2,4.08,to be very important
3314.48,6.16,for the increased adoption of these of
3317.28,5.44,this technology and these algorithms
3320.64,3.919,are we do have time to check for two
3322.72,4.399,more questions next question is are
3324.559,4.641,what are the system requirements to use
3327.119,2.641,the hc tools the hdr tool set that
3329.2,2.399,you've got
3329.76,3.68,yeah that's a great question so if you
3331.599,4.881,have our installed on your machine
3333.44,4.639,you can simply visit hctools.org and
3336.48,3.119,follow the quick install guide
3338.079,4.48,it's just a few simple commands and that
3339.599,2.96,should get you up and running
3343.119,4.161,all right we have another question can
3345.44,3.679,you integrate our models developed
3347.28,3.44,outside your organization or alzheimer's
3349.119,3.361,catalyst
3350.72,3.28,absolutely the co the code base is
3352.48,3.119,designed to run
3354.0,3.2,it's designed to allow you to develop
3355.599,3.361,your own models so
3357.2,3.44,you as long as you get your feature
3358.96,4.48,inputs in such a way that the
3360.64,4.32,software can address it um the software
3363.44,2.399,will help you develop your own models
3364.96,3.2,and
3365.839,3.52,either you know make those available to
3368.16,2.56,somebody publicly or
3369.359,4.161,just leave them in your own environment
3370.72,4.96,the tool definitely supports
3373.52,3.2,running and creating models with from
3375.68,3.36,health catalyst
3376.72,3.839,as well as creating your own our next
3379.04,2.72,question is there an api that you have
3380.559,4.081,to deploy
3381.76,3.28,no no api is necessary so basically
3384.64,2.08,install
3385.04,3.84,our package and you can use the built-in
3386.72,4.879,documentation or the documentation
3388.88,3.679,on the web page you see here and it will
3391.599,2.48,have some
3392.559,3.28,built-in data actually that you can use
3394.079,2.641,to start with those examples run right
3395.839,2.801,out of the box
3396.72,3.119,and then you can use those examples to
3398.64,3.28,tailor them to
3399.839,4.48,your specific data and your tables and
3401.92,5.36,databases etc
3404.319,4.081,okay we have time for one last question
3407.28,2.64,and that is
3408.4,3.12,can you please share some of your
3409.92,5.199,experience in terms of demonstrating the
3411.52,6.72,value of predictive analytics
3415.119,6.081,absolutely so as whenever we
3418.24,4.0,develop models and deploy them with a
3421.2,3.119,customer
3422.24,3.119,one of the things that we track is
3424.319,3.681,outcomes
3425.359,3.521,and we have in each of our applications
3428.0,3.52,we have
3428.88,3.12,tools to track the variation in those
3431.52,2.559,outcomes
3432.0,3.839,and we look for that's another area
3434.079,2.72,where where data scientists and analysts
3435.839,3.841,are very helpful
3436.799,5.52,is helping to understand that trend and
3439.68,4.72,is that trend actually going down
3442.319,3.361,after since the uh implementation of
3444.4,3.679,that predictive model
3445.68,3.04,so we we do have ways to measure that
3448.079,2.161,the other
3448.72,3.839,the other thing that we do is we make
3450.24,3.839,sure that as we're deploying them
3452.559,3.201,that there's an organizational
3454.079,3.441,understanding of how to use them
3455.76,4.079,that's actually very critical in
3457.52,3.92,creating that
3459.839,3.361,all right well we are at the top of the
3461.44,3.679,hour there is one last thing is asking
3463.2,3.84,if you're not using the help catalyst
3465.119,3.281,uh data warehouse and bi platform can i
3467.04,3.6,still utilize these models
3468.4,4.32,yeah for sure so it's very flexible so
3470.64,4.159,if you have csv files or
3472.72,3.28,a database you can connect to any of
3474.799,2.881,those but
3476.0,3.2,we want to be clear you can use the
3477.68,4.96,software the
3479.2,5.119,models specific readmission models do
3482.64,2.8,not come with the software the mod the
3484.319,3.361,software is just for running and
3485.44,5.119,creating those algorithms
3487.68,3.84,all right well thank you so much eric
3490.559,2.161,thanks levi
3491.52,3.279,i'd like to let everyone know shortly
3492.72,3.599,after this webinar you will receive an
3494.799,2.081,email with links to the recording of the
3496.319,2.401,webinar
3496.88,4.0,the presentation slide we'll have the
3498.72,3.92,link to hctools.org
3500.88,2.88,and also an audio download also please
3502.64,2.32,look forward to the transcript
3503.76,2.72,notification
3504.96,3.839,that will send you once it's ready and
3506.48,4.48,also the special invitations to the
3508.799,3.841,upcoming webinars in the predictive
3510.96,3.92,analytics webinar series
3512.64,3.439,on behalf of eric just levi thatcher as
3514.88,2.08,well as the rest of us here at help
3516.079,2.641,catalyst
3516.96,4.96,thank you for joining us today this
3518.72,3.2,webinar is now concluded
3525.44,3.76,thank you please stand by
