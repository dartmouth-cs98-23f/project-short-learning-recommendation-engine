[Music]
the carnegie mellon vaccination database
talks are made possible by autotune
learn how to automatically optimize your
mysql
and postgres configurations at
autotune.com
and by the stephen moye foundation for
keeping it real find out how best to
keep it real
at stevenmoyefoundation.org
hi guys uh welcome to the last
vaccination seminar talk for the
semester
we're super excited to finish off with
the professor eugene wu
uh he is an associate sorry so serious
assistant professor without tenure uh
at columbia university um he did a phd
uh under
um sam madman and mit um so he's gonna
talk about his his research on
human data interaction and building
systems around that space
so as always if you have a question for
eugene as he's giving the talk
uh please don't meet yourself say who
you are and where you're coming from and
feel free to interrupt him at any time
we want him you know want this to be a
conversation and not him
talking by himself and eugene hopefully
you will introduce the
uh the museum ps behind you before you
talk
okay sure yeah so thanks andy for uh
inviting me to close out the vaccination
season
uh and this museum piece um you know
is a a a prized um part of my art
collection
it's um a t-shirt signed by mike
stonebreaker who's uh
you know one of the the pioneers and
illustrious researchers and uh
in our field so with that um let me
get started i also noticed by the way
that andy added to my bio that i
wear pants and i wanted to clarify that
it
depends on the temperature and situation
uh whether or not it's needed
i also just want to clarify that you
know in contrast to the rest of the
series
i'm not going to talk about a particular
database system but instead share the
vision that
you know my students and collaborators
have been working on uh in this area of
system for human data interaction
um so as we all know programming is
powerful because users don't need a
program right they could use human
computer interfaces
that we all use today but the premise of
course of this entire seminar
is that the center of gravity is
shifting from compute to data
to the point where data rolls everything
around us
and so there's correspondingly shift
towards human data interfaces right
they're a center around visualizing and
interactively analyzing and manipulating
data
and we're seeing this kind of like trend
across nearly every discipline and every
task right across
monitoring finance marketing science and
industry as well as data tasks such as
data wrangling
data modeling data exploration analysis
and so on
and in fact if you think about databases
themselves the reason why i have this
seminar
you know in this field at all is because
they represented a leap in human data
interaction from imperative programming
to declarative specification and now
the demand for better interfaces is not
just from programmers but really from
everyone
and you can think of human data
interfaces as both the front end
interface
right that renders data as pixels on the
screen and let users point and
manipulate this
and underlying systems that transform
and process the data in response
and yet despite the active research in
both of these areas
right it's still very difficult if you
think about it to build
and design data interfaces today and i
wanted to start with
two of the major reasons why which are a
combination of scale and what i call
design dependence
so if you think about scale if your data
set is small
and you can just process it in the in
the browser and the client
then the problem is arguably solved and
there's many many great solutions today
but as the data size grows your
architecture necessarily becomes more
complex
for instance you might need to shift
processing out of the browser to your
laptop
and if the data gets any larger then you
might have to use a remote server and
database or
even the cloud right and all of this
introduces latency at every
system boundary as well as network
bottlenecks cloud api calls and so on
and the fact that you're now kind of
like implementing and running a
distributed system
but in addition to that right the
interface itself hasn't really changed
the user still expects the same amount
of responsiveness
because you know they don't really see
any of this and in order to kind of like
provide that
then we often end up adding caches the
high latency ever in the system
and if the caches become quite smart
right because you might want to query
the data in a cache
then they start looking like databases
and it's not to say that at
every level of this kind of architecture
there aren't great libraries right
there's really good libraries and
systems
to solve each of these individual
problems but optimizing and combining
them
together is still non-trivial in fact if
you squint this is kind of akin to the
memory hierarchy and we kind of know
programming efficiently against such a
hierarchy is one of the fundamental
challenges of computer science
however in this particular case we're
asking designers to actually
manually program and optimize against
this hierarchy
and combine disparate software systems
and part of this signals a lack of great
abstractions
for specifying and describing the
interfaces right
and enabling this kind of optimization
that we know and love
now you might say well there's a lot of
fast like and responsive visualizations
out there
so clearly if you know exactly what you
want to build you can hire
a crack team of developers and designers
and just build and optimize it
but for most cases actually interface
design is an iterative process
and the problem is that design choices
affect the system architecture as well
so let's consider a very very simple
example we'll look at different designs
of the left interface
that that is just you know analyzing
legislative votes from chambers of
congress
and then on the right we'll have a
trivial architecture and just look at
how
what kinds of data structures you might
pre-compute
so if you want to add two buttons to
provide interactivity to choose
the decade then we it's pretty simple
right we can pre-compute both decades
and then when the user clicks on one of
them then we can send the pre-computed
results back to the user
so that's very simple but what if we
change the design slightly
to a range slider well this would then
require pre-computing a quadratic number
of views
and so you would need a different data
structure maybe a data cube or something
else
you might even consider another
optimization to push that to the browser
and then cache and process it in in
place
right but then maybe maybe the designer
wants to try adding a radio button or
something else
and that requires yet another type of
optimization
right and this is just for a very simple
example but the bottom line is that any
minor change in the interface design
directly affects the architecture and
the types of optimizations that are
needed
and so we see here is that the trend in
most application development today on
web and on mobile
is veering towards no code levels of
simplicity when data is not really an
iss issue yet the combination of scale
and design dependence makes this
increasingly harder for data intensive
interfaces
so what our lab has been focused
focusing on over the past
you know several years are three classes
of problems under the umbrella of the
data visualization management system
at the foundations level we're thinking
about identifying
system primitives that can more easily
express and build data interfaces and
enable optimization
on top of that we're also developing
systems algorithms to more easily design
interfaces themselves and then at the
very top
we've been thinking about if we can
design interfaces easily and we can
implement them easily then what other
capabilities beyond displaying data and
clicking on widgets
could we think about right and so
there's many projects at each of these
different layers
but what i wanted to do today is focus
on one slice through this architecture
two of these projects are going to
showcase a decades-old concept called
data lineage
helps us innovate at the systems and
interaction levels
and then the third project shows how we
might expand the range of interfaces
that are designed and created today so
i'll start with smoke which reframes
interaction as lineage operations
this is work uh initially led by
photosellidus
and it's now continued by hanyin and
charlie so
what is fine grain lineage given a
workflow such as
you know for example here i have two
tables i want to join them by id or
color and i want to aggregate them by id
or color
right so fine grain lineage corresponds
to the record level dependencies
between operator inputs and outputs
concretely
if we look at output 1 the orange record
then it depends
on the join results j1 and j2 and j1
course finally then depends
on a1 and b1 in the base tables and this
forms a graph that can tell you well
which
inputs contributed to any output records
and vice versa
and it's very useful in terms of
debugging privacy
fixing distributed uh protocols
incremental view main and some more
right so it's been around for several
decades and so it's very useful
however actually materializing and using
this lineage graph today is
very very expensive on even recent
papers and systems
on analytical queries the existing
overheads can be between 10 to over a
thousand x slowdown of the query
and it kind of makes sense because a
very very optimized read query
has now transformed into a massive write
operation right
and in addition lineage is often
inefficiently represented
uh for instance you know i'll i'll
describe this next
and so to give you a sense there here
are two common approaches today
the first is what we call the logical
approach and we stay within the
relational model
and the goal is to just rewrite the base
query so that output is annotated with
this lineage information
for example here in the output it would
be the gray attributes right a
and b now the nice thing is this is
compatible with any database system
but as you can see the output 101 has
now been replicated for each combination
of input records it depends on from the
base tables
and the query now needs to do a lot more
work to actually compute these
annotations
because of this denormalized
representation there's a kind of some
inherent overheads in this
approach now alternatively a physical
approach which is popular in big data
systems
like spark is to modify each operator
implementation
to write pointers out to some external
linear subsystem like a graph or a key
value store
um but it turns out that you know even
just making these virtual function calls
can be very expensive in any fast
database system
we experimentally see that just making
empty virtual function calls
can slow things down by over 2x in
addition to
the subsystem also replicates a lot of
functionality that database provides
and so in both cases you know like it's
it's quite expensive
now the reason we care about this and
i'm harping on performance
is because lineage is actually a natural
way to express
interactions and and define interfaces
if we
think about everything that you see on
the screen that's data
and you point to any pixel that
represents data what you're really
referring to is its lineage
for instance if i select points in the
scatter plot
what i'm really selecting is the data
that backs it or it's lineage
and similarly if this base data has been
rendered in a different way
for example like in a bar chart then we
typically will see that the bar chart
changes when we select data in another
chart right
this corresponds directly to either
forward lineage or view refresh
and so ultimately all of this is
declarative right what's happening here
uh is the user interaction is data that
we're joining
with what's rendered in the scatter plot
and we're then getting its
lineage and then refreshing the other
views in other words if you think about
interactive visualizations
the initial charts are the output of sql
and the interactions is then lineage and
more sql
beyond just visualization right we can
data is manipulated and rendered in many
other forms
yifon wu's uh with 2020 work explored
how ref
how you might want to for example
reference lineage and code for instance
you might want to
query against the lineage that you
selected and pointed to in some other
visualization or chart
or you might want to fit a model against
it right so this is all
programming at this point and it makes
sense because what lineage really
represents is the correspondence between
your input data
and what you see on the screen so
ultimately lineage is great
however existing systems are super super
slow right it's just
unrealistic to use them to drive
interactive applications
and yet we know that fast visualization
implementations do exist
and so what smoke is trying to do is
bridge this gap
so what smoke is it's a row-based
in-memory query engine
during query execution right as you
execute each operator it materializes
the lineage information
for each operator the idea is really to
overlap
lineage capture and operator execution
by reusing data structures built such as
hash tables
and use it for lineage capture and what
we do is we're going to instrument
physically these operators and use query
compilation to remove that overhead
uh if you know that you only care about
end-to-end lineage information
we can actually then propagate this
information as we generate the lineage
so that you end up with just uh kind of
like end to end
information and avoid storing this per
operator
all right so this is an overview of
smoke and what i wanted to do is give
you two examples of how lineage capture
works
in the system and then you can
extrapolate for something like filter
what we're going to do is populate the
lineage information which is represented
as this red integer array
so for example let's take a look at the
first row it's going to pass the filter
because 40 is greater than
20. in addition we're going to store the
input rows id
in the corresponding offset in the in
the integer array
right the second row fails the filter
but then the third row passes and so
we're going to store its corresponding
record id as well so that's pretty much
all we have to do
lineage in this case is just a simple
integer rate it's cheap to create and
populate
now for something more complicated like
group by it works in two phases
during the build phase we're going to
augment each bucket for example here
when i look at row one i'm going to
create the bucket and then
in addition uh augment it to also store
a list of record ids
right so then when i go to record two
and i update that bucket i'm going to
store its information
and similarly for record 3 when i create
the new bucket
i'm going to also allocate this
information and store its record id
now in the scan phase all we do is just
emit the output table as normal
and for the lineage we simply create a
top level array
which we know the exact size of and it
just points to the
rid lists that i've already constructed
right so all of this
ends up being super cheap too and none
of this is really additional work and
additionally scanning the data
to generate lineage information and
piggybacks as much as it can
off of query execution so rather than
showing you like you know like
performance graphs i just wanted to show
this in action
that it can actually power interactive
applications
so this is a cross filter visualization
over 14.5
million rows of flight data and it's
going to be rendered as four charts each
one is just a query result right
now when you click on a state or a bar
we filter the input table for its
corresponding subset
and then re-aggregate and re-render each
of these other charts
and the benchmark will build whatever
data structures it needs
and then load these initial charts and
then simulate every
uh every one of the 18 uh you know 1800
possible interactions now the one
popular approach that's dominate
that's dominant today is to build
visualization specific cube structures
to accelerate these interactions
however recent papers end up taking
between four minutes
to one hour to build a data cube for
this data set right so it's really not
realistic
for in ad-hoc cases and these data cubes
themselves limit the types of queries
that you can express
so we're going to show in this benchmark
is instead of waiting for four minutes
we design a custom cube it takes about
nine seconds to build
uh just for this benchmark in addition
here in the top right uh this is going
to show the cumulative time taken
for each interaction including the time
to load these initial charts
all right so now the benchmark has
started
and what we're doing is waiting for the
data cube to be built
and once it is built we can see that
every interaction is super fast right
and so that's why in the cumulative
chart it starts fairly high along the
y-axis
and then it's basically horizontal
now to show you smoke in action what
we're going to do is when we generate
these initial charts
we just instrument it to capture lineage
information right
so as if we created all these charts in
an ad hoc manner
now when you click on a state we can use
this backward lineage information
to help us find the input subset and we
can use the forward lineage information
to incrementally update all these other
charts so you can think of lineage now
as an optimization
that comes for free it's helping us do
selective view refresh so now the
benchmark has started
and you can see the dashboard basically
loads very very quickly because
lineage capture is actually very very
fast right
and it turns out that we end up
finishing this particular benchmark
faster than it took to build the cube
and so what that suggests
right is that lineage uh has the
possibility of being
fully interactive and practical for
developing these interactive interfaces
in addition we can capture and use this
information on the fly
so this is work that we did several
years ago
and the big question that everyone often
asks is like that you know this is kind
of a toy engine
and no one in real life is going to
instrument their their engines
to actually do this right and who uses a
road based query compiled engine for
analytics
blah blah blah so what about real
engines and can you do it with minimal
surgery
so the idea that we've taken here is
we're looking now
at polymer engines that are actually
used for analytics and if you think
about
how something like filter works uh the
idea is the following
right so if you have something like
quantity and you have a filter
then the output ends up being a
selection vector right where each bit is
set depending on whether or not the row
passed the filter if we stare at what
this is
as compared to the lineage that i showed
you earlier then it turns out that
they're basically equivalent a selection
vector is a dense representation
of the same information that lineage was
encoding as well
and so what we've basically found at
least in some late materialization
engines
is that query execution itself is not
just
easy to piggyback off of it is literally
computing lineage in many cases
and so what we're doing actively right
now and so this is unpublished work and
we're kind of like still working on it
is we're in instrumenting systems like
duckdb
which is an embedded column or
vectorized engine to see how far we can
get
using this kind of idea so i wanted to
show you is just like very early numbers
of lineage capture on tpch with scale
factor one
for four of the queries these blue bars
are just the smoke numbers copied from
the original paper
and so it all of these are relative
right in terms of overhead to
get the compiled queries essentially you
can think of it as handwritten tight
loops
now smokeduck is duckdb with lineage
capture
the numbers now are relative to duckdb
right so it's not apples to apples
but we end up seeing is that the
overhead varies between 1.8 to 4.8
percent
overhead in addition it turns out that
the lineage instrumentation
took about four to 20 lines of code per
operator and because a lot of the code
is just pinning
existing arrays in memory and overall in
terms of memory overhead it's about 20
to
40 megabytes on tpch scale one right
so what all of this means is that
lineage is a critical piece for
expressing interactive applications and
interfaces
but it's got to be fast right and what
we've shown is that lineage
is interactive and is practical and so
what we're doing right now is studying
the extent that these ideas can
uh can apply to other late
materialization engines and how to
cleanly integrate lineage management
into these engines as well and it turns
and what we seem to see
is that many aero based uh analytics
engines like data fusion
and aquaro and javascript are also
amenable to some of these ideas
so we're very excited about this um and
so this is kind of like how
something from database theory can be
applied to something
in hci so now i want to shift to
a usage of lineage and this is work led
by lampreus focus
young woo with collaborators janon and
knuckle
so this this animation is from the kai
17 paper same stats different graphs
all of them have the same summary
statistics on the right
but obviously they're very different
right and visualization is exactly
powerful because it shows you what a
priori statistics cannot
now our work seeks to empower users to
identify unexpected patterns in the
interface
and be able to ask why these exist right
and then to be presented with sensible
explanations and this hopefully can help
them debug data errors or simply better
understand what's going on
so i want to just share two examples via
demos of our work
the first demo is this idea of query
explanations
that we introduced in 2012 through the
scorpion paper
and so here like you can imagine a
sensor deployment collecting a bunch of
data
um per minute right across like an
entire deployment uh and we might want
to group by hour
and then compute the average sanitation
temperatures and then plot on a
visualization
now the user might immediately then ask
well like all of this looks kind of like
periodic and it might look pretty good
but why is this region look so high
right what's going on there
and we'd like some explanation as to
what's going on
so this is kind of what i wanted to
first demo if you can see
my uh screen so this is the exact same
data so the blue corresponds to the
average temperature on an hourly basis
and you can see there's kind of some
daily periodic pattern right
so what we'd like to be able to do is
literally just like in the slides ask
what is going on here and specify them
as anomalies
and then say hey you know i thought they
would look closer to these other
points that i just selected and then i
can run the system called scorpion
and what we're going to do is look for
potential explanations
as to why this might be the case um and
so what we'll see
here is uh uh you know i think i screwed
up some demo no i didn't screw up a demo
but i need to show you more information
uh so
you know what screw it i don't care um
so here uh if we look at mode 18 and you
re-visualize just the data from mo
18 we can see that like the data is just
totally crazy right and it turns out
that if you actually
rerun this which i'll actually do right
now um
because this is a little bit better um
is if you do
exactly the same kind of like uh
uh thing sorry i'm just gonna do this
again because i can't show you all the
coolness
uh if i'm missing some of the interviews
good keep going
you got it all right so we're gonna like
re-execute this right and we're going to
look at different rules and see their
effects if we remove the data
so what that means is earlier we saw
that sensor 18 is a potential kind of
like explanation
and if i hover over this now what i'm
going to do is remove
all of its readings and then
re-visualize the visualization
and so we can see here is like the
anomalies we specified disappeared
and so this can be a useful heuristic uh
for an analyst and try to understand
what's going on
right and so what the semantics are here
is that we've generated this predicate
rule
that says centers with low voltage or
you know sensor 18 and so on
and what we mean is if we ignore all
that data that matches the predicate and
and re-execute the query then it would
help address the user's question right
their complaint and there's been really
great follow-up work by peter bayless by
sudbury roy alexandra and many other
areas
of people in this particular area but
what i wanted to focus
show you another demo is a different
example of explanation
in the context of machine learning
analytics in our recent rain work
and the setup is very similar right
however what we know is that machine
learning is not only used for end user
recommendations
but it's also often used in one step of
a data analytics workflow
for example here maybe what i want to do
is first filter to just get the readings
for faulty sensors
right and i have some model that
predicts whether or not a sensor is
faulty
and then i want to group them by hour
and just count it so maybe this is for a
dashboard or a monitoring system
now here the user might see that there's
been a drop
in hours three and four and want to ask
questions why
now existing explanation approaches will
help us find issues in the readings
in the readings data set but even if
that
that data is fully correct we can still
have these errors due to errors in the
training data right because that can
cause the model to mispredict in a way
that ultimately causes what the user is
asking about
and there's not many good solutions for
this problem normal data cleaning looks
for errors in the training data but
ignores kind of how the model or the
downstream analysis is used
and influence analysis techniques for
machine learning relies on labeling
individual model mispredictions
but that doesn't really work if if
prediction is just one step in this
larger analytics process
and so what rain is is the first work to
perform
training data debugging based on user
questions of the analysis outputs
and specifically what we're doing is
we're estimating how much deleting
different training records will affect
what the user has asked about the user's
complaint
so i just want to show you a quick demo
of this in action
so what i have here is just a very
popular data set mnist
and i'm going to group by its predicted
label and it's just going to count
so this is kind of the visualization and
we can see that for the most part we
expect a uniform distribution in this
training
in this data set right but we flip some
of the labels
and so we like to be able to do you can
imagine the user asks well
why is seven so high so i'm gonna just
specify
that i think it's higher than what i
expected and i'd like to see what
training data kind of
might be the reason and so here i've
returned the top
20 training records where if you remove
it from the training set it would help
it would most
decrease this particular output value
and you can see that all of these kind
of highlighted in red correspond
to one digits that have been mislabeled
as seven
right you can similarly click on the bar
for one
and say why is it so low and we would
then identify
is that oh actually you know like
removing all of these missed labels
is you know will help you with this
particular question
right so you can see that depending on
the kind of questions you ask you'll get
kind of like better or worse results
because they may be better
more or less aligned with the type of
error that actually affects what the
user sees
in fact you know an interesting thing is
you can ask nonsen
seemingly nonsensical questions for
example you might ask
you know what kind of data can i delete
from my training set
so that four actually is increased right
the count of
four predictions and what you'll end up
getting is a bunch of
uh digits that look pretty much you know
like closer to fours and
uh than uh than other digits and if you
delete them
then that would help the model predict
more force
right and so it's kind of nonsensical
and so you can see how
visualization and interfaces are an
important component
to trying to understand and interpret
explanations
all right uh this itself of course is
not like a full or complete solution to
this problem but hopefully it
demonstrates the value of not just
the algorithmic side but also the
interface side
now the reason why i talk about this at
all
is because these explanations that we're
talking about right
end up being a function of lineage
information because what we need
is to know for what the user is asking
questions about
what data was it derived from and also
how was it computed
right and so that is precisely lineage
and what
is commonly called data provenance and
so that's why smoke is
useful right it's not just for building
and specifying interfaces
but also for enabling these kinds of
capabilities efficiently
all right so i'm done kind of talking
about the use of
lineage and i wanted to switch gears and
talk about this third project called
precision interfaces
that's led by guru chan with early
versions
led by tebow owen and haochi
so we focus so far on building
interfaces
and expanding their capabilities but
interface design itself is ultimately
dictated by the analysis task right
analyses typically start off very ad hoc
you might be programming or like writing
ad hoc queries
and then we ultimately want to design an
interface once we settle on the
set of queries that would be actually
useful for an analysis
and yet if we think about how interface
creation tools work
they actually work in the reverse
fashion right they limit themselves to a
particular class of queries or analyses
and then make creating interfaces for
those easy to build
so here if we think about the tools for
creating interfaces
at the x-axis here corresponds to the
expressiveness of the analysis task
and the y-axis is how easy it is to
create interfaces
using them now tools like metabase and
tableau and excel and so on
make it very easy to build interfaces
that are essentially parameterized
queries or data cube operations right
so you can say that for these very very
common classes of analyses
they're fairly limited but they're very
easy to use
but if you want anything more complex
towards the right then you basically
enter the world of writing lots of code
or paying someone
and although there's engineering kind of
like programmatic
libraries to help with this it's still
just too difficult or expensive
for the vast majority of potential
interfaces right to be creative so
oftentimes they just simply will not and
so the the
kind of like the long-term goal that we
have for this project is to basically
place present interfaces at the top
right for any analysis test how can we
make it
you know nearly trivial to actually
create the corresponding interfaces
and the philosophy that we take is the
following
and this is you know just one approach
if you think about what an interface is
there's
really two components there's a
visualization that renders the output of
some
queries or programs and then there's
interactions that let users change those
queries right
and when you change them then the
interface updates
now if you imagine listing all the
queries that result from
all possible combinations of
interactions with the interface
you could call this the expressiveness
of the interface in other words the
interface itself
is a compact representation of the set
of queries that it can produce
and we we would ideally hope is that
that set includes what we need for an
analysis task
all right so what our work is trying to
do is given a sample of the analysis
task that you would like we want to
derive the latent interface right
and the core challenge here is kind of
uh you know actually took us a long time
to figure out something reasonable
which is how do you map query strings
from the input
to interactive fully interactive
interfaces right
we have a series of papers that look at
different approaches and so what i'll
give you is just kind of a demo
a walkthrough of the four main steps and
what we're currently thinking is a
reasonable approach
uh first we're going to model queries as
their parse trees
so here i'm going to focus on just the
highlighted parts of query one and query
two right because that's the part that's
different
and we can see that these two parse
trees are rooted with equals and have
two children
now the corresponding interface is going
to be simple right because
ultimately we have to execute these
queries and render them
so the simplest interface would be just
render the output of these queries
and then just visualize them but we want
interactive interfaces sometimes
and so we introduced a new class of
nodes called choice nodes to encode
subtree variations
for instance here we can add a choice
node to choose between these two
subtrees right and so what that means is
you can choose
one of its k children and parameterizing
it
will result in a standard parse tree
that you can execute so you can think of
this as
generalizing sergible kind of like
queries that we
that we know and love today the nice
thing about this is choice nodes can be
directly mapped to interactions
for instance here clicking on the second
button would correspond to
choosing the second child of this choice
note right and of course
the buttons are not the only types of
interactions you could map this to you
radio buttons can also choose one of
many and so we can rank the different
candidates by borrowing from existing
interface cost models
that can account for things like
usability complexity
screen size and so on so that's the
third step is costing
and then the fourth step is to recognize
that the middle tree here
is the result of merging the initial
pairs of trees right by adding this
choice node
so it went from two trees to one these
types of
tree transformations allow us to
generate structurally different
interfaces as well
like we saw here now we could also
imagine
refactoring the highlighted equalities
out right so that we have a tree
rooted at equality but then now you can
actually make independent choices
for the left and right operands and so
this ends up generalizing beyond the
input two queries that we had
this also maps to a different interface
right because now we have two
interactions instead of one
and in practice of course you know we
have many different types of
transformation rules and types of
choices
choice nodes that we can express beyond
this trivial example
but hopefully this gives you kind of a a
sense so to put these steps together you
can imagine
initializing the system by parsing the
initial query
input queries into a sequence of trees
and then mapping both the tree execution
results to visualizations
the choice nodes to interactions and
then the tree structure
to a particular layout you can cost the
output interface and then we can either
return
it or then transform the trees further
and so this is the problem statement as
well given a query log we want to search
the space of possible trees and mappings
for the lowest cost interface that
expresses all of them
and potentially more and you may just
said this but what is what is the cost
function here like what
the cost function uh you know we borrow
from existing ui literature
you can you can think of things like
personal preference usability
how easy is it to express the analysis
you know like the workload and so on
uh how you know like whether or not like
there's too many options in a radio
button
uh list and so on right um so you know
there's
different ways that you can cost it and
what we kind of like use is both
per interaction like cost as well as you
know what's the expected effort that the
user needs to take
to express the input queries in sequence
so like
this is stuff you can compute offline
you don't need to put humans in front of
us and test it
uh yeah so you could put humans in front
of uh
different can interfaces offline and
then train some model and then use that
right okay correct yeah
and you know implementation wise we use
monte carlo research it's basically
randomly searching the space
and it's a little bit better than that
because it uses some reinforcement
learning
kind of ideas okay but you know the cool
thing is a demo right so let's take a
look at a demo
uh here hopefully you can see the
queries um so i'll zoom in and here i
just have a simple cars data set
and maybe here initially i want to just
uh
look at you know different miles per
gallon and uh and
uh for a different horsepower
and then maybe now the analysis kind of
like we've changed this we want to see a
different range for the horsepower
right so this is fairly simple uh and
what we're going to do is kind of like
run the interface a little bit and we
can see that it kind of like immediately
generates like an interactive interface
that you can use to basically filter uh
and avoid writing queries
but then what if i actually have kind of
a more complicated query structure for
example
now i want to actually change the
subquery here i'm going to use a very
simple example
for um uh for demo purposes but let's
say i just care about uh
cars from the usa because you know we're
like
super uh i'll stop there
um and so here you know what we can see
is that we've actually generated a weird
interface right
it turns out that you know like for this
particular three queries
you know it's actually might be best to
just allow you to specify each of these
queries individually
um and maybe that's totally nonsensical
right
but if i now say i actually care about
kind of like europe
then uh then present interfaces in the
background will crunch for a little bit
and it'll actually generate a different
type of interface so you still have the
slider as
normal right this range slider but you
can now select either just the full cars
or you can manipulate the sub query
structure itself
right so what this basically does is
when you click on europe it switches to
the subquery
that it corresponds to and then
manipulates
this this text string here
and then now maybe you want to do
something else right maybe you want to
also look at horsepower and look at the
average miles per gallon
from this data set grouped by horsepower
and if you run something like this then
precision interfaces will chug for you
know a couple of seconds
and what it'll end up generating is a
multi-view visualization
where it actually visualizes both the
horsepower versus
miles per gallon here right and it
allows you to interact with it directly
and so you can use this visualization to
then interact and see kind of like
what's going on uh in the initial query
right and you can also of course
use these other kind of controls as well
and so
by just writing a few queries or you can
imagine mining some existing query log
we can identify and fully generate a
fully interactive multi-view
visualization
that you can use in lieu of programming
right
if it turns out to work well so that's a
a demo of kind of like the types of
functionality to show that it is
possible
to uh to do this right and the nice
thing here is that you can have
arbitrarily complicated queries
as well as transformations of its
substructures
because what we're doing is we're just
encoding all of this as transformations
in these trees
so just to kind of wrap things up i've
talked about these three projects in
this data visualization management
system
and hopefully you know i've convinced
you that it's pretty interesting
i want to connect it now with the rest
of our projects to see how
all kinds of pieces together the
interfaces that we generate in this last
part
are in this internal declarative
representation it's very similar to the
trees and choice nodes that we saw
earlier right because what we want to
capture in interfaces
is not really the front end rendering
but the data flows right that back it
because that's the hard part in a lot of
these
in modern interfaces physical
visualization design
is another project that i didn't talk
about and it consumes its representation
and its goal is to recommend a system
architecture to optimize the
interactions
it's very similar to physical database
design if you're familiar with it but
the data structures
are created and stored outside the
database and we viewed
contacting a database as a slow path the
reason is because
there's lots of work on new custom
systems
and data structures and papers out there
today that are often
external to the database system itself
right and it also tries to account for
the client and communication
now so what this allows us do is to
design an interface right either
manually or
using this you know present interfaces
point it at a snowflake or some remote
database
and just fill in the rest of the
architecture
and and of course this can now execute
and leverage a lot of the kind of system
parameters that we've been developing
such as smoke for a lineage and
interaction or communication management
in chameleon
as well as kind of query optimization in
addition because we're generating these
interfaces we can imagine embedding
these novel interactions and
functionalities into the interfaces that
we generate optimize and deploy
and so in other words we think of
precision interfaces and the reason i
talk about it
as a way of kind of like bootstrapping
the use of a lot of these other projects
that we have
and that we're working on and so just if
there's one thing you take away from
this talk
is that data management and
visualization and hci
you know are very interesting standalone
right but really the inner
intersection of it there's lots and lots
of really interesting and rich research
problems
that can benefit both communities uh
we've tried to
study some of them in this dvms like
data visualization management system
at the levels of functionality interface
design and
system primitives so at this point this
is my uh
url and a a
bunch of pictures of ninjas all right
thanks everyone
okay awesome i i will clap on behalf of
everyone else for eugene um so we have
time for questions if you have any
questions for jadine please
meet yourself and fire away
okay so my question is about the um
the precision interface stuff uh that
seems super cool
like but you're taking whatever the sql
queries they throw at you and trying to
turn into interface like
and i understand there's a cost function
and maybe i don't fully understand
where they can handle this but like what
happens if users like writing stupid
queries they're like
it's not necessarily stupid queries but
like they're trying to figure out what
the data
even looks like to begin with without
asking questions like how do you prevent
them from polluting
the interface and also because i can
imagine also too what you really want to
do
is let a knowledgeable person write the
sql queries
and then the novice users don't get
direct sql access they instead get the
interview that's generated
so how do you prevent it's not an
adversarial thing as if they're trying
to hurt you or hurt the interfaces
they don't know what they're doing
definitely definitely you know like
i i think of precision interfaces as
like the the underlying mechanism
right it just takes a sequence of
queries and it generates stuff
um now there's you can imagine many ways
of using this
so one that i demonstrated here is a
knowledgeable person writes perfect
queries that are well crafted for a demo
um but another could be you know you do
this ad hoc analysis at the you know
command line or using other kind of like
complex tools
and then once you've decided like this
is the analysis i want to share with
other people
or that i think i'll do repeatedly then
you want to basically say like
those queries log those and generate
something specialized
right so that's another way that you can
imagine using it
um but you for for instance maybe like
you're writing these queries and then
you just mark the ones that you think
are useful
right or you could explicitly use it as
like an interface generation tool
by uh by directly uh writing queries
that you know will be useful right so
that is if you're using it
specifically as a design tool um
there's many kind of like limitations
and opportunities too for example like
we generate like one interface but
there's actually like how much do you
want to generalize beyond the queries
that the user gave you
is a parameter that you want to be able
to control right and maybe you
personally have like preferences on the
layout or on the types of visualizations
and so on
and it turns out that like you'd like to
be able to maybe specify that and then
use those as kind of hard constraints
let's say um so
i i think of this as an optimization
problem
and then what you can use it for so we
we we're kind of really like
at the point where we think this is like
a good formulation of the
problem um and we're we're hoping to
write it up like later this year yeah
okay all right um
and then sort of another question b is
like
you know sql there's
you know you can write the same query in
some in
semantically the same queries in a bunch
of ways in sql
like does it how well it can handle that
does it always generate the same
interface or
does it like if i use nested queries
versus ctes but again
the high level answer is the same thing
like i asked my students
for the first homework assignment they
had to write queries they asked you know
questions written in english
and they come back with all sorts of
crazy different ways to write it
would your thing still generate the same
interface or i think we probably
generate something
pretty um it it's like
as you can see right where we actually
uh
just look at the syntax and some
database statistics and schema
information so we know
nothing about the internals right we're
not looking at the query plan or the
logical plan or anything like that
if you still look at the career plan
what could you do so i think if we
looked at the query plan it would allow
us to do more canonicalization
right so i think that would end up uh
end up generating like equivalent
interfaces if they have equivalent like
you know the closer you are to the
semantics the closer you are to
kind of like canonicalizing um i i would
say i imagine that you know like for
like with any tool you can use it in
like
unintended ways and intended ways right
um and
and so i i would imagine that you
probably don't want to use this as just
like
throw a bunch of random queries and
generate something um
you probably would end up using it um
to either like mine for for example like
you can i could imagine having like some
interface complexity measure or some
kind of like entropy measure
where you can say oh this interface is
actually reasonable
with some criteria and then kind of like
just uh
scanning through like query logs and
looking for like you know sequence
sub sequences uh that kind of are
self-consistent right so that's like one
crazy thing that you can imagine doing
but ultimately like people will use
tools based on where it shines and
probably
not um uh you know
not for the corner cases necessarily so
um
it probably isn't good for like uh the
setting you described at least now
i wonder if you take like tableau and
you you know they you love some public
dates that they have and then set up a
basic visualization
uh and then but capture the queries and
then
run the queries you capture yeah we've
done that in the past
yeah okay and it generates one of our
goals
is to be able to synthesize like the
tableau
interface right because like the primary
like degrees of freedom
is the the group i clause and the where
clause yeah
right and the project clause so um but
you know like so
it's really just degrees of freedom uh
another way you can think about it right
another
like a baseline um would be if you if
like how would you write in
interfaces today you would literally
write and and construct query strings
yeah right and so at minimum here it
kind of like
generalizes sargeable things so for
example we can generate only
syntactically correct
statements right and we could you can
imagine doing some checks to make sure
that you only execute queries
that are not faulty right so those are
things that you could do
with this sort of functionality so the
baseline i imagine is like
the alternative to creating an interface
which is writing a bunch of like query
templates and then like filling in like
query strings
yeah yeah okay um
i had a question about smoke too much
pedals yeah so
i mean smoke i guess if you i mean if
you're running duck db
it sort of answers my question but
there's no there's no sort of level
complexity of the query where you
like when you would lose like how many
like layers of nesting
you know about embedded queries uh sub
queries ctes where like you would lose
when the information like all with feel
accessible it's all at the physical
level
okay so you don't lose anything yeah
okay
okay cool good good okay uh any quite
any other questions from the audience
because eugene is a one-year-old and i'm
a one-year-old old i'm gonna
take care of them although he stands
office so he doesn't have to oh i think
the nanny leaves in ten minutes
so okay you gotta keep going okay any
last question for eugene
you
