second,duration,transcript
1.079,4.601,I'm sure it's been fascinating to see
2.879,6.041,all of the AI algorithms you know and
5.68,5.48,all their capabilities and it's easy to
8.92,4.96,forget what has made all those
11.16,5.76,algorithms possible is the constantly
13.88,4.44,improving capabilities of the hardware
16.92,5.599,that powers
18.32,6.0,them and a significant component of that
22.519,3.721,Improvement has been coming from
24.32,4.799,something called Mo's law I'm sure
26.24,4.92,you've all seen graphs of Mo's law I'm
29.119,5.681,going to save you from having to see one
31.16,5.32,but I will present evidence that Mo law
34.8,4.56,is
36.48,7.36,over that is fundamentally because Mo's
39.36,7.0,law was an economics law and
43.84,5.6,specifically on it talked about the
46.36,5.719,exponentially decreasing cost of
49.44,6.24,transistors and while actual transistor
52.079,6.44,costs tend to be closely guarded secret
55.68,5.679,there is evidence that they're not going
58.519,5.441,down as shown in in this
61.359,5.561,chart where which is showing Gates which
63.96,6.12,are small collections of transistors and
66.92,6.4,what we see is that mors law is flatten
70.08,7.28,flattened out the response has been a
73.32,6.799,focus by computer Architects to use
77.36,5.439,specialization to create more efficient
80.119,4.721,accelerator architectures and I
82.799,4.68,participated in that and here is a
84.84,5.599,subset of the accelerator designs that
87.479,4.881,I've worked on over the past few years
90.439,4.96,each has unique attributes and
92.36,5.2,capabilities but like most accelerators
95.399,4.841,they tend to be described by pictures
97.56,5.48,like this which are somewhat inscrutable
100.24,5.839,and long natural language descriptions
103.04,5.079,that tend to be incomplete and even
106.079,5.32,worse lack
108.119,6.561,structure that makes it difficult to be
111.399,4.08,able to separate the big picture from
114.68,3.88,the
115.479,6.081,details to improve the situation I
118.56,6.0,suggest taking inspiration from the
121.56,6.559,world of science where it is generally
124.56,6.319,accepted that science advances by
128.119,5.001,systematizing and categorizing knowledge
130.879,5.041,in the world and I agree with Nobel
133.12,5.52,laurat herb Simon who sees many
135.92,5.399,parallels between science and
138.64,5.879,engineering and so one might assert that
141.319,6.761,engineering advances by systematizing
144.519,6.321,and categorizing knowledge about
148.08,4.36,designs to apply that
150.84,4.2,AI
152.44,4.68,accelerators we note that a commonality
155.04,5.08,amongst many of the
157.12,5.039,algorithms is that they can be expressed
160.12,4.839,as tensor
162.159,4.36,computations so what's a tensor well
164.959,6.121,tensor is
166.519,7.321,simply a set of values organized into a
171.08,5.56,multi-dimensional structure where we
173.84,7.0,will call the dimensions
176.64,7.4,ranks so we have a rank zero structure
180.84,5.84,zero tensor which is just a single value
184.04,6.199,we have a rank one tensor which is an
186.68,5.559,array of values and so on the other
190.239,6.961,thing that's very nice is that there's a
192.239,8.681,very nice way to represent a tensor so
197.2,7.28,we have the tensor a i j k where I is
200.92,7.36,the row J is the column and D is the
204.48,6.479,depth of a tensor a however to
208.28,5.08,systematize things a bit
210.959,5.321,and avoid dealing with the Myriad of
213.36,4.68,different concrete ways that people
216.28,4.8,represent
218.04,4.96,tensors we're going to use a single
221.08,4.28,abstract
223.0,6.12,representation which we call the fiber
225.36,6.0,Tree in this in this abstraction the
229.12,5.64,tensor is represented as a
231.36,8.12,tree here each level of the tree
234.76,8.28,corresponds to a rank each row or column
239.48,7.8,in in the tree in the tensor corresponds
243.04,7.96,to what we call a fiber and each
247.28,6.799,fiber is indexed by a coordinate and
251.0,6.239,then finally each point in the
254.079,6.56,tensor is identified by a path of
257.239,7.24,coordinates through the
260.639,7.761,tree I've Illustrated a dense tensor so
264.479,6.881,there is a nonzero value in every
268.4,6.0,Point how however many problems are
271.36,6.36,sparse meaning that they have many zero
274.4,4.68,values why do we care about this we care
277.72,3.64,about it because there are many
279.08,4.559,opportunities for exploiting that
281.36,6.08,sparsity in
283.639,6.961,specific if the tensor is sparse it can
287.44,7.72,be compressed taking up less
290.6,8.159,space and using these simple arithmetic
295.16,5.319,Expressions we see that having a zero
298.759,2.72,means that there there's really nothing
300.479,4.0,to
301.479,6.321,do and so an architecture that can
304.479,6.641,exploit these characteristics in sparse
307.8,6.92,operands can result in Savings in
311.12,5.919,storage execution time and energy
314.72,5.08,consumption to include this information
317.039,3.921,in our data abstraction we return to our
319.8,3.839,fiber
320.96,7.72,tree and note that if the tensor is
323.639,6.361,sparse like this then we can simply
328.68,4.92,remove
330.0,6.759,coordinates and their children in the
333.6,6.28,tree so now turning to the computations
336.759,7.601,on these tensors there's actually a very
339.88,5.8,nice notation for describing the essence
344.36,4.24,of those
345.68,5.92,computations this notation has has its
348.6,4.8,origins in a paper by Albert Einstein
351.6,5.28,over a hundred years
353.4,6.68,ago and is referred to as
356.88,6.159,einom here's a very simple example
360.08,6.239,of matrix multiplication in this
363.039,8.841,notation note how the
366.319,8.121,formula is simply references to a the
371.88,5.08,points in the tensors so it's very
374.44,5.92,simple so how do we interpret this Ein
376.96,6.12,sum we interpret the Ein sum by saying
380.36,7.72,that what we're going to do is Traverse
383.08,7.8,all the points sorry in the space of all
388.08,4.36,legal values this is referred to as the
390.88,5.039,iteration
392.44,7.0,space and at each point in the iteration
395.919,5.761,space we're going to calculate the value
399.44,5.479,on the right hand side we're going to
401.68,6.239,then take that value and assign it to
404.919,5.081,the upper end on the left hand side
407.919,3.84,unless there's already something been
410.0,5.24,assigned there in which case we're going
411.759,5.681,to reduce the value into it notice how
415.24,2.2,the
417.8,7.399,insom does not specify the traversal
422.16,7.159,order so what it's really doing is
425.199,7.961,saying what the computation is but not
429.319,7.201,how to do it the ism also tells us a lot
433.16,4.759,about the work that needs to be done
436.52,3.92,here's an
437.919,5.96,example when we have a situation where
440.44,5.159,the same in index in this case k appears
443.879,5.04,in both
445.599,5.841,operand sparse operand we need to get a
448.919,4.4,value from each of those operand we call
451.44,4.199,these shared
453.319,4.521,indices and or in order for there
455.639,5.641,actually to be any work to
457.84,7.16,do both of the operands must be
461.28,6.12,nonzero what this corresponds to is an
465.0,6.4,intersection and what we have found is
467.4,6.799,that this and many other operations that
471.4,6.12,are implied by calculation of the
474.199,6.28,inom correspond to manipulations of
477.52,5.88,fibers in the fiber tree
480.479,6.641,so for intersection if we have two
483.4,6.919,sparse fibers what we notice is that
487.12,7.199,these two sparse fibers only have
490.319,6.241,coordinates common coordinates at two
494.319,4.961,and eight and so the result of the
496.56,6.28,intersection is a fiber with only
499.28,5.039,coordinates 2 and 8 there are many
502.84,4.199,Hardware
504.319,5.641,implementations of intersection but we
507.039,6.36,have done is separate out the semantic
509.96,6.72,requirements of the calculation from the
513.399,5.161,details of the design with that
516.68,7.0,knowledge we have found that what we can
518.56,8.2,do is very concisely Express the essence
523.68,4.8,of many published design breaking it
526.76,6.079,down into the
528.48,6.6,einum and the traversal which we call
532.839,6.961,the the
535.08,7.96,mapping but now with a concise
539.8,5.32,and comprehensive design specification
543.04,6.16,there's a number of different things
545.12,7.56,that we can do one is build a model to
549.2,4.96,characterize a design such a model takes
552.68,4.76,in the
554.16,4.28,inom the traversal information and
557.44,6.72,design
558.44,8.399,details and can generate speed area and
564.16,6.76,energy but these also these ideas can
566.839,5.801,also help with design so a student took
570.92,4.2,this accelerator design that was
572.64,6.16,published and studied this diagram from
575.12,6.52,the paper and looked at the long English
578.8,4.88,natural language description and
581.64,5.16,actually found that that wasn't complete
583.68,5.32,so contacted the author and found out
586.8,5.039,the remaining
589.0,6.279,details and with that information was
591.839,5.921,able to write a comprehensive concise
595.279,5.841,and complete description of the
597.76,5.639,architecture for for that accelerator
601.12,6.8,which she looked at and said oh there's
603.399,7.081,some very obvious optimizations here and
607.92,5.32,wrote them down and generated
610.48,5.2,dramatically improved performance so in
613.24,5.039,summary I would argue that taking this
615.68,5.08,approach allows for implementation
618.279,4.921,independent expression of computations
620.76,4.6,format agnostic specification of
623.2,4.6,processing activities descriptions of
625.36,4.32,accelerators that are decomposable into
627.8,4.599,separation of concerns
629.68,4.12,concise precise allows for the creation
632.399,4.041,of analytic and more detailed
633.8,4.8,performance models and provides a path
636.44,6.519,for exploring the new architectures that
638.6,4.359,we're going to need in the future thank
646.959,3.0,you
