you kind of mentioned the different
maybe radical computational medium like
biological and there's other ideas so
there's a lot of spaces in a6 or
domain-specific and then there could be
quantum computers and wood so we can
think of all those different mediums and
types of computation what's the
connection between swapping out
different hardware systems in the
instruction set do you see those as
disjoint or they fundamentally coupled
yeah so what's so kind of if we go back
to the history you know when Moore's Law
is in full effect and you're getting
twice as many transistors every couple
of years you know kind of the challenge
for computer designers is how can we
take advantage of that how can we turn
those transistors into better computers
faster typically and so there was an era
I guess in the 80s and 90s where
computers were doubling performance
every 18 months and if you weren't
around then what would happen is you had
your computer and your friend's computer
which was like a year year and a half
newer and it was much faster than your
computer and you he he or she could get
their work done much faster than your
typical achiever so people took their
computers perfectly good computers and
threw them away to buy a newer computer
because the computer one or two years
later was so much faster so that's what
the world was like in the 80s and 90s
well with the slowing down of Moore's
Law that's no longer true right he's not
now with you know not death side
computers with the laptops I only get a
new laptop when it breaks right well
damn the disk broke or this display
broke I got to buy a new computer but
before you would throw them away because
it just they were just so sluggish
compared to the latest computers so
that's you know that's a huge change of
what's gone on so but yes since this
lasted for decades kind of programmers
and maybe all of society is used to
computers getting faster regularly it we
now now believe those of
who are in computer design it's called
computer architecture that the path
forward is instead is to add
accelerators that only work well for
certain applications
so since Moore's law is slowing down
we don't think general-purpose computers
are and get a lot faster so the Intel
process of the world are not going to
haven't been getting a lot faster
they've been barely improving like a few
percent a year
it used to be doubling every 18 months
and now it's doubling every 20 years so
it was just shocking so to be able to
deliver on what Moore's Law used to do
we think what's going to happen what is
happening right now is people adding
accelerators to their microprocessors
that only work well for some domains and
by sheer coincidence at the same time
that this is happening has been this
revolution in artificial intelligence
called machine learning so with as I'm
sure your other guests have said you
know a I had these two competing schools
of thought is that we could figure out
artificial intelligence by just writing
the rules top-down or that was wrong you
had to look at data and infer what the
rules are the machine learning and
what's happened in the last decade or
eight years this machine learning has
won and it turns out that machine
learning the hardware you built for
machine learning is pretty much multiply
the matrix multiply is a key feature for
the way people machine learning is done
so that's a godsend for computer
designers we know how to make metrics
multiply run really fast so
general-purpose microprocessors are
slowing down we're adding accelerators
for machine learning that fundamentally
are doing matrix multiplies much more
efficiently than general-purpose
computers have done so we have to come
up with a new way to accelerate things
the danger of only accelerating one
application is how important is that
application turns a translate machine
learning gets used for all kinds of
things so serendipitously we found
something to accelerate that's widely
applicable
and we don't even we're in the middle of
this revolution of machine learning
we're not sure what the limits of
machine learning are so this has been
kind of a godsend if you're going to be
able to Excel deliver on improved
performance as long as people are moving
their programs to be embracing more
machine learning we know how to give
them more performance even as Moore's
Law is slowing down and
counter-intuitively the machine learning
mechanism you can say is domain-specific
but because it's leveraging data it's
actually could be very broad in terms of
in terms of the domains that could be
applied in yeah that's exactly right
sort of it's almost sort of people
sometimes talk about the idea of
software 2.0 we're almost taking another
step up in the abstraction layer in
designing machine learning systems
because now you're programming in the
space of data in the space of hyper
parameters it's changing fundamentally
the nature of programming and so the
specialized devices that that accelerate
the performance especially neural
network based machine learning systems
might become the new general yes so the
this thing that's interesting point out
these are not coral these are not tied
together the it's enthusiasm about
machine learning about creating programs
driven from data that we should figure
out the answers from data rather than
kind of top down which classically the
way most programming is done and the way
artificial intelligent used to be done
that's a movement that's going on at the
same time coincidentally and the the
first word machine learnings of machines
right so that's going to increase the
demand for computing because instead of
programmers being smart writing those
those things down we're going to instead
use computers to examine a lot of data
to kind of create the programs that's
the idea and remarkably this gets used
for all kinds of things very
successfully the image recognition the
language translation the game playing
and you know he gets into pieces of the
software stack like databases and stuff
like that
we're not quite sure how journal
purposes but that's going on independent
this hardware stuff what's happening on
the hardware side is Moore's law is
slowing down right when we need a lot
more cycles it's failing us it's failing
us right when we need it because there's
going to be a greater in peace a greater
increase in computing and then this idea
that we're going to do so-called
domain-specific here's a domain that
your greatest fear is you'll make this
one thing work and that'll help you know
5% of the people in the world well this
this looks like it's a very general
purpose thing so the timing is
fortuitous that if we can perhaps if we
can keep building hardware that will
accelerate machine learning the neural
networks that'll beat the timing be
right that that neural network
revolution will transform your software
the so called software 2.0 and the
software the future will be very
different from the software the past and
just as our microprocessors even though
we're still going to have that same
basic risk instructions to run a big
pieces of the software stack like user
interfaces and stuff like that we can
accelerate the the kind of a small piece
that's computationally intensive it's
not lots of lines of code but there it
takes a lot of cycles to run that code
that that's going to be the accelerator
piece and so this that's what makes us
from a computer designer's perspective a
really interesting decade but Hennessy
and I talked about in the title of our
Turing warrant speech is a new golden
age we we see this as a very exciting
decade much like when we were assistant
professors and the wrists stuff was
going on that was a very exciting time
was where we were changing what was
going on we see this happening again
tremendous opportunities of people
because we're fundamentally changing how
software is built and how we're running
it so which layer of the abstraction do
you think most of the acceleration might
be happening if you look in the next ten
years that Google is working on a lot of
exciting stuff with the TPU sort of
there's a closer to the hardware that
could be optimizations around the IROC
closer to the instruction set that could
be optimization at the compiler level it
could be even at the high
level software stack yeah it's got to be
I mean if you think about the the old
risks this debate it was both
it was software hardware it was the
compilers improving as well as the
architecture improving and that that's
likely to be the way things are now with
machine learning they they're using
domain-specific languages the languages
like tensorflow and pi torch are very
popular with the machine learning people
that those are the raising the level of
abstraction it's easier for people to
write machine learning in these
domain-specific languages like like pi
torch in tensorflow
so we're the most optimization right
yeah and so the and so they'll be both
the compiler piece and the hardware
piece underneath it so as you kind of
the fatal flaw for hardware people is to
create really great hardware but not
have brought along the compilers and
what we're seeing right now in the
marketplace because of this enthusiasm
around hardware for machine learning is
getting you know probably a billions of
dollars invested in start-up companies
we're seeing startup companies go
belly-up because they focus on the
hardware but didn't bring the software
stack along
you
