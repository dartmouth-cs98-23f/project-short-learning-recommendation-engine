second,duration,transcript
0.08,2.96,let's say that you are designing a
1.28,3.999,marketplace for your website selling
3.04,4.16,firearms is prohibited by your website's
5.279,4.081,terms of service agreement not to
7.2,3.84,mention the laws of your country to this
9.36,3.52,end you want to create a system that can
11.04,3.679,automatically detect if a listing on the
12.88,3.68,marketplace is selling a gun how would
14.719,3.681,you go about doing this yeah it's a
16.56,3.52,interesting question i'm curious about a
18.4,3.44,few things here first i guess one thing
20.08,3.6,i'm wondering about is like how if i was
21.84,3.519,given this task like how is this working
23.68,3.599,in kind of production you know the task
25.359,3.441,is to automatically identify the
27.279,3.601,listings what happens with those
28.8,4.88,identifications do they go to a human
30.88,5.04,who checks it or is it like immediately
33.68,4.24,changes the website yeah so let's say
35.92,4.479,that the current setup is that it's all
37.92,5.76,crowd source so let's say we have a flag
40.399,5.761,and then a user can flag the listing if
43.68,4.559,they see it's a gun or maybe someone who
46.16,4.079,works in terms of like moderating the
48.239,4.561,marketplace can also flag it and then
50.239,3.84,it'll go to another internal let's say
52.8,3.2,someone who works in customer support
54.079,3.281,that can then remove the listing if they
56.0,2.879,determine it as a gun and that's the
57.36,3.6,only thing that kind of happens right
58.879,3.601,now okay and then how would this fit in
60.96,3.12,is the end of the line always going to
62.48,3.12,be customer service or how would this
64.08,3.76,fit in that pipeline you think so i
65.6,4.24,guess reframe the question back to you
67.84,4.319,but let's say that
69.84,5.279,this is the current system right and i
72.159,6.64,guess given the kind of context would
75.119,5.841,you think that the goal is to first
78.799,4.081,disable it when they're posting do you
80.96,3.76,think we should be like detecting it
82.88,3.919,afterwards after a few days and then
84.72,3.039,sending it to customer support what part
86.799,2.881,do you think
87.759,2.961,yeah so i guess yeah the reason i was
89.68,3.28,asking that question i guess i could
90.72,4.96,have said that too because if if
92.96,4.56,customer service is is going to look at
95.68,3.36,it afterwards which i think would
97.52,3.76,probably be the better way to go to
99.04,4.24,start but if they're doing it that way
101.28,4.08,then that means that i really need to do
103.28,5.68,a very good job of detecting all the
105.36,5.759,possible firearm listings it's bad if i
108.96,4.24,miss something that is a firearm but i
111.119,4.0,miss it because at the end the customer
113.2,4.239,service is going to look at it and so
115.119,3.921,i'm assuming it'll be okay it might cost
117.439,4.0,some money but it's probably going to be
119.04,4.56,okay if i give it false positives like
121.439,4.481,listings that aren't guns and then let
123.6,4.4,the customer service deal with it but it
125.92,3.199,seems like this pipeline would be very
128.0,3.039,costly
129.119,4.081,in that in the sense that you're maybe
131.039,4.321,breaking laws of your country or your
133.2,4.32,terms of service if you totally miss the
135.36,4.72,listing and you have this uh false
137.52,5.6,negative right yes so that's that's yeah
140.08,5.92,so that's how i would think about it so
143.12,6.0,i guess given the costs do you think we
146.0,5.84,would then err on the side of reducing
149.12,4.72,the number of false negatives or false
151.84,4.24,positives in this case so my initial
153.84,4.399,point was about false negatives because
156.08,3.84,we want the customer service to get
158.239,4.321,everything that's pertinent and they can
159.92,4.88,decide because they won't see anything
162.56,5.28,that's not been flagged or not been
164.8,5.28,identified so but if costs are a concern
167.84,3.92,then yeah we would be concerned with the
170.08,3.28,false positives
171.76,3.92,because that would mean that the
173.36,4.08,customer service gets extra ones that
175.68,4.16,are not really relevant at all they're
177.44,4.079,adding to their workload okay so let's
179.84,4.479,like maybe think about an assumption of
181.519,4.64,what is okay in terms of how the process
184.319,4.241,might go and maybe this could be an
186.159,5.761,example so you know in my mind i think
188.56,5.2,like if we let's say someone posts a gun
191.92,3.84,it gets put onto marketplace it gets
193.76,3.52,removed within one hour that's probably
195.76,4.16,good probably not a lot of time for
197.28,4.879,someone to go out reach out message try
199.92,4.56,to get that gun from the seller in that
202.159,5.041,scenario maybe a bad case scenario is
204.48,4.479,that we get overloaded with a bunch of
207.2,4.08,other items that are trying to be sold
208.959,4.801,that are not guns and then you know they
211.28,4.4,get the typical your posting was flagged
213.76,3.199,until it's reviewed by customer service
215.68,3.119,therefore you know reducing kind of
216.959,3.121,liquidity in this marketplace as well
218.799,3.761,all of a sudden you know sellers are
220.08,4.719,like oh like this marketplace sucks like
222.56,4.239,i can't even list you know my plants on
224.799,3.921,here or else they get flagged right and
226.799,3.041,then therefore kind of reducing that
228.72,2.879,when we're thinking about the different
229.84,4.399,scenarios here what is it kind of like
231.599,4.72,the optimal one you kind of have an idea
234.239,4.64,of what you think is kind of best case
236.319,4.321,scenario here for both parties bringing
238.879,3.201,it back if you're facebook what would
240.64,3.28,you think facebook would want to do in
242.08,4.239,this scenario it does depend on what
243.92,4.239,happens with the the model at the end so
246.319,3.84,there are different scenarios we've laid
248.159,3.761,out so if one of the scenarios is the
250.159,3.36,one i was talking about which is that
251.92,3.84,you don't want to miss anything that
253.519,3.921,case false negatives are important in
255.76,3.84,other cases like you're mentioning which
257.44,3.519,is if if it ends up being a sticker
259.6,3.28,that's posted because my the model
260.959,4.24,identifies it but it's not there and
262.88,4.56,then it can lead to a lot of issues so
265.199,4.401,that's the the false positive case what
267.44,3.92,we'll want to be concerned with then in
269.6,3.28,our model so i guess if we're concerned
271.36,3.2,with both those scenarios then we want
272.88,3.039,to minimize both false positives and
274.56,3.68,false negatives
275.919,4.401,and so we can use metrics like f1 score
278.24,3.84,to to sort of minimize that i guess
280.32,4.24,another thing that's related to this of
282.08,4.72,why we might choose something like an f1
284.56,5.68,score which is basically a combination
286.8,5.839,of precision and recall and precision is
290.24,4.399,where you're really concerned with you
292.639,3.761,make a bunch of predictions and how many
294.639,4.0,of them are right the recall is the case
296.4,4.16,where you make a bunch of predictions
298.639,3.681,how many of the real case scenarios do
300.56,3.04,you get so how much stuff do you miss i
302.32,3.04,guess recall is like what do you miss
303.6,4.24,another reason to use these two measures
305.36,3.92,and i think that seems pertinent here is
307.84,3.44,because the number of guns are probably
309.28,3.44,gonna be small right the i'm assuming
311.28,3.04,the actual number of gun posts is
312.72,3.199,probably very small maybe even less than
314.32,3.12,one percent in like a facebook
315.919,4.161,marketplace that sells all sorts of
317.44,4.16,stuff it's an imbalance sample too those
320.08,3.04,measures that i just mentioned are good
321.6,4.08,for that because they they sort of
323.12,4.799,ignore the fact of the true negative
325.68,3.84,which is that the post isn't about guns
327.919,3.041,and you get that right but there's so
329.52,2.959,many of those so you want to ignore
330.96,4.239,those so measures like precision and
332.479,4.641,recall will ignore the thing that's very
335.199,4.321,obvious and predominant which is the
337.12,4.639,listing not having guns and focuses on
339.52,3.76,just getting the positive case of
341.759,4.081,getting the listing with guns the
343.28,4.56,imbalance case will also maybe come into
345.84,3.199,consideration for the models we choose
347.84,3.04,so that seems good for the imbalance
349.039,3.361,sample and i guess false positives and
350.88,3.28,false negatives i think the other things
352.4,3.68,i was thinking is that what sort of the
354.16,4.0,scale and scope and the kind of data we
356.08,3.839,might have because that'll pertain to
358.16,4.0,how we can answer this question so i'm
359.919,4.641,guessing maybe speed at least for
362.16,4.4,training the model isn't a big issue and
364.56,3.6,the importance is accuracy like we're
366.56,2.96,very concerned with an accurate model
368.16,3.52,yeah definitely
369.52,3.519,and i'm guessing online you had
371.68,3.04,mentioned like maybe it's okay i mean
373.039,2.801,the model doesn't need to be that fast i
374.72,2.96,guess right
375.84,4.24,in terms of when it gets deployed and
377.68,4.079,it's making predictions for each post
380.08,4.08,yeah and then i guess in terms of the
381.759,5.201,prior data so i guess we would have
384.16,4.56,access in this case to other postings
386.96,3.6,and where customer service has flag
388.72,3.599,stuff so we have like a large data set
390.56,3.6,where we know that there was a posting
392.319,4.72,and we know whether it was of guns or
394.16,4.24,not yes yeah i think we can identify if
397.039,4.801,there were guns there's probably
398.4,5.04,something where the actual value itself
401.84,3.919,is flagged and then probably also a
403.44,4.72,categorization of why it was flagged for
405.759,4.081,and for this scenario we could say that
408.16,3.84,confidently probably customer service is
409.84,4.24,labeling them as guns or firearms in
412.0,4.08,that category okay yeah so we're we can
414.08,4.08,select those flags that are for guns and
416.08,5.2,farms okay it seems kind of
418.16,4.56,straightforward then so the idea is that
421.28,3.68,you know we really want to identify
422.72,4.319,these gun postings part of it might be
424.96,4.079,related to the law part of it also might
427.039,4.16,be related to you know people don't want
429.039,3.121,to see these gun postings and and they
431.199,3.201,also don't want to be necessarily
432.16,4.08,wrongly flagged for the guns postings so
434.4,4.56,it's very important to have a very
436.24,5.76,accurate model at identifying these very
438.96,4.639,small cases of gun postings and we don't
442.0,3.599,seem to have as many concerns about
443.599,3.361,model training time or anything like
445.599,4.081,that we're really concerned with
446.96,4.88,accuracy with that in mind with the fact
449.68,4.48,that we already have this data i'm
451.84,3.759,guessing our data set is fairly large
454.16,3.2,maybe there's some things we might want
455.599,3.04,to do with augmenting the data which i
457.36,2.64,think so if i think about data
458.639,3.041,collection and then i can think about
460.0,3.52,features and then think about the model
461.68,2.959,so if now i think about the features
463.52,2.799,that i have
464.639,4.24,i have flags that might have been given
466.319,4.241,by users so i have that as a feature i
468.879,3.6,might have the particular user who
470.56,3.919,posted it their demographic information
472.479,4.0,location information so maybe you know
474.479,3.921,parts of the country have more guns
476.479,3.761,being sold or not there might be other
478.4,3.44,contextual information i don't know why
480.24,3.679,this might be but maybe early in the
481.84,3.68,morning people like to post their gun
483.919,3.601,sales
485.52,4.799,and then i think what's critical for me
487.52,4.56,here is the text as a feature the body
490.319,3.521,of the text itself i guess another
492.08,3.679,feature could be the maybe they post
493.84,3.759,some images but i'm thinking maybe for
495.759,4.081,now just to focus the model a little bit
497.599,3.921,i can i might ignore the images part
499.84,3.44,just for now but i think when i talk
501.52,3.84,about the text a lot of what i'll say
503.28,3.599,can apply to the images and i think the
505.36,3.519,the big part here to me it seems like
506.879,4.16,the text in that data and being able to
508.879,4.481,leverage that information to get you
511.039,4.24,know keywords or to get patterns of
513.36,3.599,words yeah so so you might have that you
515.279,3.2,might have other data to like the length
516.959,4.161,of the text or something like that so we
518.479,4.961,have those features user data flags
521.12,3.839,context information the text so now i
523.44,4.399,want to focus a little bit on on the
524.959,4.721,text itself and what we can do with it
527.839,3.601,to use for our model i think just to
529.68,2.96,start i mean i often feel this way when
531.44,2.72,dealing with text data you want to start
532.64,2.879,with a simpler model i suppose and so
534.16,3.679,the simpler model might be something
535.519,4.081,like a bag of words one also reason to
537.839,3.361,start with that is that you can get a
539.6,3.28,nice baseline and i guess a related
541.2,3.84,question here i i meant to ask this
542.88,4.16,earlier was that you know we do we have
545.04,4.239,other baselines like i guess we have
547.04,3.68,previous performance of how the user
549.279,3.201,flags worked or
550.72,3.76,so we have some other data in the past
552.48,4.08,yeah yeah let's assume that yeah we have
554.48,3.68,all this data that facebook itself has
556.56,3.6,yeah so we have some baseline data of
558.16,3.28,how their current process is working and
560.16,3.44,then i'm suggesting we can have this
561.44,4.56,other baseline where we just we use the
563.6,4.0,simplest approach for text analysis
566.0,3.519,even though i did mention we did talk
567.6,3.04,about this that there is a lot of time
569.519,2.801,so technically we could use more
570.64,3.12,complicated models like attention-based
572.32,3.12,transformers that take contextual
573.76,3.36,information into account but for now
575.44,3.36,i'll just focus on the simpler model and
577.12,3.36,and talk through with that maybe we can
578.8,4.159,talk at the end of the value of these
580.48,4.4,more uh sophisticated approaches so i
582.959,3.841,guess if we have the text data then you
584.88,3.6,know we want to we can extract the bag
586.8,3.76,of words which just means that we get
588.48,3.68,for each body of text we get the unique
590.56,3.6,words in the text and the counts of
592.16,3.679,those words we have like i don't know
594.16,4.08,how many millions of posts so we have
595.839,4.721,this for each post we have these bag of
598.24,5.839,words the other thing we can do is an
600.56,5.6,approach called the tfidf where we scale
604.079,4.641,the value of each word based on its
606.16,4.32,frequency in different postings and the
608.72,4.0,reason this can be valuable is that you
610.48,4.4,might expect that postings about guns
612.72,3.92,for instance tend to use specific words
614.88,3.519,not found in other postings you know
616.64,4.319,before even running the model i might be
618.399,4.961,helping my model by selecting words that
620.959,4.961,are unique to particular listings and so
623.36,4.8,this will up weight those words that
625.92,4.08,tend to be very specific to specific
628.16,3.679,listings because the bag of words can be
630.0,3.6,like millions and millions of words so
631.839,2.881,it's a huge sparse matrix so sometimes
633.6,2.72,you might want to do additional
634.72,3.76,reductions so you might do something
636.32,4.079,like a pca to reduce that to something
638.48,3.44,like 500 dimensions the point of all
640.399,3.521,this process is is that you're taking
641.92,3.68,the text and you're putting it into some
643.92,3.359,embedding space and the value of the
645.6,3.2,embedding space is it's almost like what
647.279,3.441,you're doing is you're putting each
648.8,3.52,listing you're plotting it in space and
650.72,4.239,the idea is that you want to plot points
652.32,4.4,in space that mean similar things this
654.959,4.401,processing technique should before we
656.72,4.88,even build our model should place each
659.36,4.0,listing next to each other that mean the
661.6,4.0,same thing that's the sort of idea of
663.36,3.84,this process and since we know that idea
665.6,3.6,we can always substitute it with other
667.2,2.879,methods that are more sophisticated if
669.2,2.319,we want
670.079,2.401,that sort of like how we get the
671.519,2.721,features
672.48,4.16,so now i guess we want to think a bit
674.24,4.08,about the model that we want to build
676.64,3.36,because we have the imbalance sample i
678.32,3.92,would think maybe the model to start
680.0,4.24,with at least we can again iterate as we
682.24,3.839,go but for our first prototype maybe we
684.24,3.36,could start with a tree based model
686.079,3.681,particularly something like a gradient
687.6,4.239,boosted tree because what's nice about
689.76,4.24,these models is that you have each tree
691.839,3.841,that makes a prediction so in this case
694.0,4.64,it's taking all our features and
695.68,5.44,predicting whether it's a gun posting or
698.64,4.4,not a gun posting and then it takes the
701.12,4.48,points the data points that have the
703.04,5.28,most error and it scales them so it up
705.6,4.4,weights those data points that were in
708.32,3.92,error for the next tree what this
710.0,4.88,effectively will do is it'll upweight
712.24,5.599,the sort of minority sample points those
714.88,4.56,listings that are for guns are very few
717.839,3.601,and so they'll if they keep causing an
719.44,3.36,error in the model their weight will
721.44,3.36,keep going up and they'll be more and
722.8,4.08,more important in making the prediction
724.8,3.599,that's maybe why a gradient booster tree
726.88,2.959,would be good to start with yeah the
728.399,3.44,only one issue could be if you want
729.839,4.0,online training and so maybe if there's
731.839,3.921,an issue of online training the gradient
733.839,3.761,boosted trees may not be optimal for it
735.76,3.759,and so we could try other models if we
737.6,4.32,wanted and the difference between online
739.519,4.401,and offline training is that online
741.92,4.32,training happens while the model is
743.92,4.32,deployed and does continual improvement
746.24,3.92,is that right yeah yeah exactly but i'm
748.24,4.159,guessing in this case what we probably
750.16,4.16,would want to do is maybe every so often
752.399,3.201,we update the model in which case the
754.32,3.04,usually the gradient boosted trees are
755.6,4.08,pretty quick to train and they're fast
757.36,4.4,at also delivering the predictions at
759.68,3.839,inference time so we could just retrain
761.76,3.84,the whole model but say for whatever
763.519,3.361,reason every time customer service
765.6,2.96,answers that you want to update the
766.88,2.88,model then this tree based approach
768.56,3.04,depending on what package you use it
769.76,3.12,it's not going to be very optimal so you
771.6,3.52,might want to use other approaches like
772.88,3.92,a neural network that could allow for
775.12,3.44,this online training we talked a bit
776.8,3.68,about like so we're building this model
778.56,3.2,with the gradient boosted tree we talked
780.48,2.88,earlier about we can't really use
781.76,4.4,accuracy like just plain and simple
783.36,5.039,accuracy because it's such a small
786.16,3.6,sample that we have there's very few gun
788.399,2.88,posts the one thing i could have
789.76,3.44,mentioned earlier is one way to deal
791.279,3.521,with that too is to balance the sample
793.2,3.439,so if we have a lot of data we also
794.8,3.76,could have evened out the the two
796.639,4.561,samples so we could have taken how many
798.56,5.2,gun postings we have and just gotten a
801.2,4.8,matched sample of the equivalent other
803.76,4.319,sample but say we're not doing that say
806.0,4.24,there they're not enough gun postings to
808.079,4.161,really have that match then accuracy
810.24,3.2,won't be that great and what we'll want
812.24,3.44,is like what we talked about is
813.44,4.639,precision and recall are there other
815.68,4.56,things to consider evaluating the model
818.079,3.921,and maybe when we roll it out yeah like
820.24,4.0,i said we can use precision and recall
822.0,4.8,and we can combine them in this f1 score
824.24,4.88,that just range between zero and one and
826.8,4.64,that can tell us how well the model is
829.12,4.24,performing at predicting those gun sales
831.44,4.0,when we're building the model we we
833.36,3.839,probably would be training i guess on
835.44,3.6,our historic data we'd be taking some
837.199,4.0,sample of data in time that's our
839.04,3.84,training sample and then the test set is
841.199,3.281,something later in time we should
842.88,3.36,probably mimic how it's occurring in
844.48,3.599,production where our model is trained
846.24,3.599,with a given set of data in time and
848.079,3.12,it's predicting new data in the future
849.839,3.44,one thing we might want to consider is
851.199,4.401,how long this prediction is good for how
853.279,4.321,often we want to keep rebuilding this
855.6,4.08,model because i guess as everything on
857.6,4.08,the internet or the spam calls i get
859.68,3.76,they get more and more creative at doing
861.68,3.839,these things so yeah you know we might
863.44,4.56,want to update the model uh to deal with
865.519,3.921,these creativities that people have yeah
868.0,3.839,and that's actually a good question
869.44,4.24,because i think as people realize that
871.839,4.081,their posts are being flagged we're
873.68,4.159,dealing with very malicious actors that
875.92,3.84,are active in their campaign to sell
877.839,3.68,guns on the internet maybe one of the
879.76,3.68,aspects is that they start creatively
881.519,3.76,disguising their posts right and so that
883.44,4.16,right the traditional nlp test of
885.279,4.321,detecting bullets or guns for sale turns
887.6,3.84,into like code names or something in
889.6,4.16,which then we still have to then reuse
891.44,3.839,that use of identifying manual tagging
893.76,3.6,sorts so i guess one additional question
895.279,4.481,i have is how do we know the performance
897.36,4.479,edition of doing like more advanced
899.76,3.759,approaches right and so let's say that
901.839,3.44,we want to dive into computer vision i
903.519,3.76,know you have a computer vision back
905.279,4.881,under our and so like i guess like how
907.279,5.68,would you assess the necessity of maybe
910.16,5.2,using like images into your analysis
912.959,3.841,versus just only using text and you know
915.36,3.44,you know that image is probably harder
916.8,4.08,to train there's probably a lot more
918.8,4.88,difficulty with having expertise and
920.88,5.36,images versus just text which has like
923.68,3.839,great packages on python to use and so
926.24,3.039,yeah how would you kind of like approach
927.519,4.081,that situation how would you know that
929.279,4.0,it's worth doing the image analysis into
931.6,3.76,the features versus just going with a
933.279,3.761,basic model yeah so i guess the question
935.36,4.159,is like what's the added value of the
937.04,4.799,images and is it worth bringing all that
939.519,4.481,what whatever time it takes then exactly
941.839,3.841,yes i think i mean a very simple
944.0,3.199,approach that you could use that i like
945.68,3.839,using is you just like sort of build the
947.199,4.64,model you have all the features in there
949.519,4.401,and then you get its prediction what the
951.839,5.601,full model's prediction accuracy is so
953.92,5.839,say for instance the full model is at 90
957.44,5.199,seems really good and then you drop the
959.759,4.721,images from the model and then you see
962.639,4.481,you know when you remove the images what
964.48,5.68,the value is and the accuracy and say
967.12,4.88,it's 85 and then say you you do it again
970.16,3.679,but you remove the text data and the
972.0,4.72,text data when you remove it it's like 7
973.839,4.401,60 accuracy or something so so so yeah
976.72,4.4,there you can know oh wow the text is
978.24,4.8,very valuable but the images does drop
981.12,3.44,it so you're like okay well you told me
983.04,3.44,now when you drop the images the
984.56,3.92,accuracy does drop but is that a
986.48,4.4,meaningful drop so one thing i think you
988.48,4.0,could do is just simulate like randomly
990.88,3.12,sampling the data especially because we
992.48,2.88,have enough data like i said here i
994.0,2.959,guess in this facebook huge amount of
995.36,3.76,data set so what you could do is
996.959,4.8,randomly sample from the data and
999.12,4.24,retrain the model each time and get this
1001.759,3.76,drop in accuracy when you remove the
1003.36,4.719,images and so say i guess i could say if
1005.519,4.401,95 of the time the drop in accuracy is
1008.079,4.32,more than zero it's like it's like
1009.92,4.96,there's a negative drop so then i might
1012.399,4.481,say yeah images are important because
1014.88,4.0,almost all the time that i've tried it
1016.88,3.84,out in simulating across multiple
1018.88,3.68,samples there is that drop in accuracy
1020.72,3.599,gotcha okay cool yeah i mean that makes
1022.56,3.04,a lot of sense any additional kind of
1024.319,2.72,thoughts on this question oh i was going
1025.6,3.52,to say something about like if we could
1027.039,3.28,augment the data too for text data but
1029.12,3.679,some of those things are cool where you
1030.319,5.281,can use machine translation to change
1032.799,5.441,the specific text words but keep the
1035.6,4.56,meaning but i guess other things this
1038.24,3.92,model would be sort of a first step
1040.16,4.639,we're taking in all these features the
1042.16,4.56,user data the flags and the text we're
1044.799,3.841,making some predictions so i think the
1046.72,4.16,final thing would just be to check where
1048.64,4.159,the errors are happening and maybe use
1050.88,4.799,that to help with the model so one thing
1052.799,5.521,you might find is that say we flag toy
1055.679,4.321,guns all the time are being flagged yeah
1058.32,3.84,so one simple thing you could do with
1060.0,4.4,just the simple model where i use a bag
1062.16,4.48,of words is i could use like uh n-gram
1064.4,4.399,model so i could take every two words
1066.64,4.0,and and use the pairs of words and then
1068.799,3.921,in that case i would get toy gun and
1070.64,3.6,that way i could identify oh these
1072.72,2.959,things that are actually toy guns that
1074.24,2.88,are being wrongly flagged and i could
1075.679,4.161,solve that problem by just changing the
1077.12,4.64,model to include this local history um
1079.84,3.76,with pairs of words cool yeah that makes
1081.76,4.159,sense final question and this is kind of
1083.6,3.6,just more in relation to the question
1085.919,2.64,itself like what do you think about this
1087.2,2.96,question like how well do you think it
1088.559,3.761,assesses the candidate's performance
1090.16,3.68,just overall how do you feel your answer
1092.32,3.359,would kind of like fit in into like a
1093.84,3.04,broader facebook interview i mean i'm
1095.679,2.481,not sure about the broader facebook
1096.88,2.799,interview but
1098.16,3.759,i guess this i mean this question is
1099.679,4.161,pretty it seems very standard like it's
1101.919,4.0,very machine learning right you test
1103.84,4.0,your knowledge of minority like when you
1105.919,3.521,have very few things you're predicting
1107.84,3.839,you have to sort of build the model but
1109.44,4.4,you go from end to end and i think it's
1111.679,4.481,also to me seems like a fairly common
1113.84,4.079,problem you might face not this specific
1116.16,3.28,one but something like this where you
1117.919,3.281,know you need to identify something from
1119.44,4.16,a particular listing a particular post
1121.2,5.04,or or some you know in my case i deal
1123.6,4.48,with videos a lot and so it's it's like
1126.24,3.52,in line with i think what what you also
1128.08,3.28,might have to work on yeah what were
1129.76,3.12,your thoughts you have any yeah i mean i
1131.36,3.76,liked it a lot and i liked your answer
1132.88,4.56,and how you structured everything and i
1135.12,4.799,feel like that's a great approach for
1137.44,4.0,most machine learning type questions as
1139.919,4.081,well because i feel like most of them
1141.44,4.88,have a very defined beginning middle and
1144.0,3.919,end in terms of where's the data how do
1146.32,3.76,you build the model and then how would
1147.919,4.241,you deploy and evaluate it and i think
1150.08,3.52,focusing on those approaches is really
1152.16,3.2,key and so i think you did a great job
1153.6,4.0,there yeah good
1155.36,2.24,awesome
