welcome to day two
we've talked a lot so far about the
impact of research on society
on the collective
but of course the collective is really a
collection of individuals
this morning we'll take a look at the
human side of technology at individual
experiences
we see a need for increased focus on
creating technologies that can meet us
where we are as individuals wherever
that happens to be in terms of identity
ability or societal context
this brings us to the field of human
computer interaction
one of microsoft's longest standing
research areas which has led to things
like connect smart assistance and many
other innovative technologies
it's now my pleasure to introduce one of
the pioneers in the field mary serwinski
to share some of the latest work to make
our interactions with technology more
natural
more intuitive and more empowering
human computer interaction or hci is a
critical part of the design of computing
systems
it has formerly been around as a field
since the 1960s though of course the
study of human factors predates that
time period
most technology companies that care
about their users have hci researchers
as part of their product design process
so it made sense that microsoft research
which pushes ahead of the product teams
by several years would also incorporate
hci researchers as new systems are
invented and evaluated before they are
released to the public i happen to be
the first social scientist hired into
microsoft research back in 1996 or so
today my latest count shows over 70 phd
scientists that identify as hci
researchers worldwide in microsoft
research
hci deals with the design execution and
assessment of computer systems and
related phenomenon that are for human
use
the goal of the discipline is to make
sure that the technologies we develop
are useful usable safe and enjoyable to
interact with for all people so that
humans can achieve their tasks whether
that's to get work done learn be
creative communicate or just play
in order to fulfill these goals tech
companies must attempt to
understand how people use technologies
and build systems based on that
knowledge
design or invent efficient effective and
safe interaction techniques for using
new systems
and put people first people's needs
capabilities and preferences should come
before the technology the new technology
invented should be designed to meet
users needs and requirements first not
the other way around
why is this important
a good user interface can improve
financial outcomes because the
experience can increase users loyalty
trust and make users happy ensuring
continued interaction with the system
we know this first hand on my team
because we study user emotion and we can
see happiness go up when interactions
with software allows user to get their
work done more effectively for example
we've seen evidence that users smile
significantly more often when good
search results come back quickly and
users don't have to reformulate their
search queries
happy users come back to use software
that pleases them
most hci researchers come from a varied
background of multi-disciplinary and
training
including computer science obviously
psychology because it studies people and
application of theories and methods
sociology for understanding
computer-mediated technology and
organizations and even industrial design
for the design of interactive products
like cars laptops mobile phones etc as
technology evolves ever more rapidly
human cognition remains relatively
stable
it's important now more than ever to
ensure that humans can use and
understand the design of new systems as
they emerge especially with the advent
of newly automated systems like
self-driving cars robots and telehealth
how do we as a society leverage hci to
ensure that these systems are designed
for human safety comfort and well-being
to that end let's take a tour of some of
the labs and researchers we have at msr
microsoft research working in hci i'm
super excited to give you a mini tour of
hci in the redmond labs here in building
99
first we're going to meet mar gonzalez
franco
mar is a principal researcher with the
epic research group she wonders what
spatial computing can tell us about
human behavior and how we can expand
digital content for immersive
experiences
beyond helping to create new ways of
experiencing technology through virtual
and augmented reality mars work explores
how new devices and technologies impact
our perception and
behavior
i'm margunthalet franco and this is the
epic lab the standard perception
interaction and cognition lab what we do
here is we explore the digital content
around us and we try to implement new
sensory abilities to it so that means we
studied very deeply perception
and we studied the behaviors that come
with the perception that we produce with
the new devices
so you might think perception and
behavior are very far apart it's a big
leap
but the truth is not and i think
studying behavior with understanding
perception is just missing part of it
because perception how we perceive the
world is going to change how we behave
and the ecological way to explore how
our prototypes are going to affect the
lives of millions when they go out
uh is to study both together
we're humans are visual animals right
it's been traditionally our method of
displaying information for
um
digital content it's been through
screens
and through bissell but
we are multi-sensory animals and we have
evolved to have multi-sensory
experiences which are far more
fulfilling
for example
audio is
we have more precision with audio
sometimes on a spatial because we our
eyes only look forward and audio is you
know all around us so we can hear things
that are on our back but we cannot see
things that are on our backs so it's
clearly something that we can enhance
our experiences with devices if we think
on a holistic way
what are the different sensors that
humans have
so you might ask why is this important
now right computers have been here for a
while and i think there's been a change
in paradigm in how we understand
computing
until now we had the screens in front of
us digital content was inside
more recently we're seeing a lot of
spatial computing devices that render
the digital content around us and that
that's changing a little bit how we're
gonna interact with the content
and in particular it's augmenting how we
need the sensory abilities to match
um and interact with the content if we
see an
object in front of us that we can grab
we want to feel that experience right
this is a sensory motor loop
that needs to be fulfilled
so i have some prototypes here
uh that i can show you
and i think they will explain a bit more
what we mean by augmenting the sensory
experience and perception so i want to
start with this one i love this one
this is called the keyboard and and you
you'll clearly see very easily how it
works but
um imagine well now it's in the northern
hemisphere it's apple picking season
right so imagine you're picking apples
inside virtual reality or augmented
reality
and as you go for the apple
the people comes to you and you just
grab it and it's such a fulfilling
experience
that once you've tried it it feels empty
to go into virtual reality without
haptic controllers
so we experiment a lot with these type
of things and we have
devices that
basically
extrude and will change the
normal as you're touching through a
surface
um
and you'll see there are many of them
that are controllers that you'll be
wearing but we're also trying to amend
the desktop so this is a new type of
fabrication based on aesthetic materials
we just publish it in nature
communication we're able to reduce a
shape display into nine actuators only
by changing how we
fabricate the materials on top of it
right so you you will touch for example
a shape and move around and be able to
experience a whole shape
and we have also these very cute ones
which are mini robots that come
just in time for whatever you want to
touch
so all of this is the
nice set of prototypes but there is
science behind it right
there are things we found
for example like the uncanny valley of
haptics that you can
go deep into this sensation that you
break the illusion because the haptics
don't match correctly uh what you're
expecting we publish that in science
robotics and i think it's hard sometimes
to detach theory from practice because
if you don't learn the theory behind it
you will not be able to replicate right
so many days we're inside but we also go
outside right we want to see how these
technologies go in the wild and we study
beyond touch
all sort of perception behind me you see
a bit of a vestibulator which will you
know we used to study vestibular system
but we also study a lot of sound because
once you are inside the content sound is
spatial like in real life which is also
a very big change in paradigm compared
to using the screens and
so in that area i'm working a lot with
the team of soundscape microsoft
soundscape they are enabling blind and
low bcn people uh to go around the world
and travel with a different form of gps
which is based on beacons it's a
different form of navigation if you
think of
the human
you know we were living in forests we
would hear sounds travel towards a
source of water we would hear
and you know that that would create a
mental map as we were going there so we
could return to the origin
and and more recently you know this idea
of sound emitting
systems
has been used in modern society right uh
churches minarets
and
mosques produce sound for people to know
and locate where they are so we're
trying to do that but with digital
content all around us i it would be
great if the pharmacy made noise so i
could go there and find where the
pharmacy is so we're trying to create
that type of experience
in particular i recommend reading this
uh
piece we put out in scientific american
just a couple of months back
so there are many of these subjects
we're thinking like apples that are
static right but
the truth is once you put this on you
might be talking to an avatar for
example
so we work a lot on avatars and we're
trying to figure out how people behave
with alberto lavataran there's so many
possible applications that it's even
beyond what we can do
for the particular case uh you know all
these applications of avatars but for
the particular case of ai i think we
have a duty there right um
we're
creating synthetic data currently people
are creating
training
self-driving cars
in environments that have no avatars
so you you basically have no
representation of humans there in many
cases we're just training
you know can you stop a designer and we
need to have the random
behavior of humans also inside there to
really have a good
digital substitute of real life
scenarios
and it's even more interesting because
once you have
humans in there you can trigger actions
that would happen very rarely in real
life
so you can cover many more scenarios
than if you're to record real footage so
synthetic training i think it's
something that
is of particular interest for ai and and
the merge of this avatar and
an ai content
so we have been very strong at releasing
libraries for everyone to use like the
microsoft rocketbox avatar library so
people around the world are also
exploring these ideas of how you
interact with this digital content and
avatars in this case and we have people
in the nih uh studying the psychological
responses
uh
in stanford ucl you know universities
around the world are using our avatars
so there is you know all sort of
applications and we're trying to see
where it goes uh also by providing means
for everyone to do research in this area
let's move on to hear from teddy sayed
teddy is a senior researcher in the ryze
group at microsoft research and also
leads the future of wearables team he
looks into the question of how
technology allows us to express our
identity we'll join teddy now to hear
more about how smart fabrics and
wearables are used to create unique new
user interfaces to explore human
computer interaction in new areas
including fashion
my name is teddy said and i'm a senior
researcher at microsoft research i'm in
the ryze group and i lead the future of
wearables team
and so what i work on is really about
connected textiles intelligent textiles
wearables and really anything in between
that involves textiles in general
so speaking of textiles you can think of
textiles is you know stuff that's around
you every day it's in your clothing it's
on the couch you sit on it's in your gym
bag
really it's well into the fabric of
everything and every
thing we do every day
so you can imagine being at home or
working from home you're sitting on a
couch you're using your laptop you know
you're wearing clothing there's you know
the couch all those things have fabric
embedded in them and they're all really
woven around what we do today and then
when we think of
you know the textile experiences also
the social experiences right there's the
aspect of creativity and expression and
the clothing that we wear the activities
that we go to you'd imagine being at a
large concert or some really interesting
event out in public
but the whole point of this is a lot of
it you know clothing and textiles is a
part of what we do and it's really woven
into what we do every day
so when i talk about smart textiles and
intelligent textiles and connected
experiences what i'm really meaning here
is imagine there's some sort of
intelligence some type of sensing some
type of
interesting interaction enabled by a
textile um so when i think of this
entire space of textiles that i've
mentioned you know i like to think of it
as an ocean right and there's this whole
ocean and we're moving across this ocean
you know there's many companies
microsoft apple google et cetera et
cetera we're all trying to move in this
ocean but the problem here is that you
know the vehicle which we're moving on
this ocean is really a boat but it's a
boat made of car parts we're using
really chunky hardware we're using you
know maybe not the most optimized
software or hardware experiences
it's really chunky and so i like to
think of the space as moving forward
with a boat made of car parts but what
we're really looking for is moving this
space forward with boat parts so we need
stuff to be integrated into the fabric
we need gestures to be integrated into
the fabric we need the technology really
to be woven into what we do in everyday
life which i had mentioned earlier and
this really ties it back to mark
weiser's vision of ubiquitous computing
where you know computing is woven in
into the fabric of everyday life
so this is really what i mean about
smart textiles experiences and we want
to do is make the technology uh make
that boat better so how are we going to
do that as i mentioned before you know
we're in this ocean space and we want to
do a lot of
work that moves it forward we want
technology to be integrated into the
fabric we want it to be part of the
textile so you can imagine how are we
actually going to integrate sensing into
fabric because you can imagine there's
not going to be cameras available all
the time if we're sitting on a couch at
home watching a sports game or a movie
and our hands are dirty what are we
going to do well here would be a nice
use case of actually having textile
sensing integrated
and in forbitrio we looked at how do we
actually integrate sensing into the
fabric and we came up with a pretty
clever way of using machine learning to
detect different gestures whether it's
tap or click or even wave so traditional
techniques that you see with camera we
can actually do
directly with fabric which i think is
really really cool
but moving beyond just sensing one of
the challenges that we have with
textiles is that textiles deform and
there's a challenge of deformation and
so what i mean by deformation is imagine
you have a sensor directly on your shirt
and if i scrunch it up this sensor is
probably not going to work so how do we
solve that this is what i mean by
deformation
so in
this next part you're going to see here
we're actually trying to solve for
deformation and so the first place we
solved this for was the pocket because
the pocket is one place where you can
put objects in it deforms quite heavily
and if you can solve for a pocket you
can most likely solve for the shirt in
your gym bag that you take out and put
on
so here we're actually using some pretty
cover again clever techniques with
machine learning detect different
objects that go into your pocket and not
just objects but also uh gestures as
well which is again one again going back
to the to the areas of focus for hdi
and last but not least i haven't touched
upon in the previous two projects the
human side right if you think of this
whole space as an ecosystem
right now this ocean is really being
pulled by technology companies which is
something i am you know not really
passionate about i am passionate about
wearables and we are as well at
microsoft but this space should be
really led by designers by apparel
brands by the people that actually
understand and have history with
apparels and textiles
so the idea here with project brookdale
is can we empower that community to have
that experience can we create textiles
can we create apis can create hardware
software experiences that enable them to
then design
you know the experiences that we want in
the future for myself i don't want to be
creating the next wearable textile or
the next wearable but i want them to be
empowered by the work that we do at
microsoft which is our goal here
and in project brookdale we ended up
going to new york and the reason why we
went to new york is obviously new york
is a global fashion hub
and a lot of interesting creative
designers live there
and work there and so by going there we
want to actually explore what is it like
to empower you know a set of designers
with tools hardware software to create
interesting wearable experiences and by
this i mean you know using tattoo
sensors using motion sensors using
lights all the things that kind of exist
today in the space of wearables can we
design tools for them to empower them
the idea here being
if we want the space to be led by that
industry we need to work with them side
by side hci we really focus on the human
side as well as the computer side but
when you think of you know the space of
textiles and really the broad space in
general it's really about the people
that are working in it
and the impact that it has on the
environment in the case of textiles you
know there's a huge environment impact
impact on the type of textiles we use
the waste how it's manufactured now
you're adding an element of intelligence
right now there's hardware there's
software involved that does all the
computation so really there's a whole
ecosystem that's involved
in this space and because we're at such
a point now where
everything is really new we can actually
define in design for sustainability in
mind in the space of intelligent
textiles so whether it's the case of you
know the factory worker who's trying to
integrate you know smart textiles at the
apparel level
to the brand that's actually making it
to the end customer as well so we can
design for sustainability in mind which
is one of the really unique uh
really aspects about the space that
we're working in at microsoft research
and so coming back to that analogy i
described earlier with the boat parts
and car parts you know what we're trying
to do today is really emphasize building
out that better boat whether it's better
sensing through the fabrics
sustainable fabrics that are also
intelligent to designing better back-end
systems that take into account the
environmental impact it's really all
about the ecosystem
in moving forward in this ocean and that
is one of the things i'm really excited
about here at microsoft research
so obviously this insane vision we have
here at microsoft research is not just
myself but it's a whole team of us at
the future of wearables lab
first off it's led by evelina barhudaran
who is a co-innovator as well as a
leader of business development and
strategy for myself and the team
as well as becky gangnon and gabrielle
demon who handle a lot of the
engineering as well as industrial design
as well i have some amazing academic
collaborators taehyun wu who's a phd
student at dartmouth college as well as
a supervisor and a colleague of mine and
close friend dr zhing dong yang also at
dartmouth college i also wanted to give
a quick shout out to the microsoft
research hardware lab who does amazing
amazing work here moving this vision
forward so before i leave you today i
kind of want to leave you with a nice
quote from walter benjamin was a
personal favorite of mine
work on good pros has three steps a
musical stage when it is composed
an architronic one one is built and
textile one when it is woven
so microsoft research we're composing a
lot of the components you've seen today
you know we're trying to make that boat
better
we're building all the necessary
software and infrastructure for it
but really what it comes down to
especially you as the audience and
looking for collaborators as well is
weaving this all together through
textiles so if you're someone who's
interested in smart textiles connected
experiences through you know jerseys or
areas like that
or even really just interested in
wearables in general please get in touch
and let's collaborate because that's
really what we're all about at microsoft
research
up next you're going to meet anne
paradiso anne is a principal research
designer in the enable group here at
microsoft research and she works to pair
human-centered design and technology to
enable expression and creativity the
enable group creates technologies that
improve the lives of people with
disabilities
community is one important aspect to the
group's work so they design new
technology with users not just for users
i'm ann paradiso and i am a research
designer at msr and i'm sitting here in
the enable lab in
redmond one of the things that i care
deeply about and that i know all of my
collaborators care deeply about is
that gap between what happens in a
research lab or in
this lab or in a development environment
and getting technologies that are
critical into the hands of users who can
use them and benefit from them we have
several collaborators inside of
microsoft who have different skill sets
that when we combine them together we
find we can be more effective as a team
expressive pixels is a
technology-mediated creativity toolkit
it's the result of a long-standing
collaboration among the enable group and
central engineering teams at msr redmond
and the microsoft small medium and
corporate business team the platform
embodies our shared belief that
expression through technology art and
communication should be an empowering
experience for all
expressive pixels can be paired with a
variety of inclusive inputs and led
displays to enable all sorts of novel
alternative avenues for creative
expression and communication here's a
short introduction to the platform by
expressive pixels co-creator gavin janke
hi i'm gavin and i'm excited to share
with you microsoft expressive pixels
a fun new app that provides an inclusive
experience for people with all abilities
to express
connect and share with others
expressive pixels originally came about
from a desire to provide a means of
expression
for some people verbal expressions may
be challenging
whether because of speech disabilities
or they are just less comfortable with
it
often a visual statement through
creative and artistic expression
can communicate as much or more than
words can ever say
the result takes into account
neurodiversity in lowering barriers
with expressive pixels you can create
your own animations or
use animations from our online community
gallery
and make them come alive on simple led
display devices
microsoft expressive pixels can be
downloaded for free from the microsoft
store on your pc
download the app today start creating
and expressing yourself
you can learn more by watching the
tutorial videos on our website
aka dot ms forward slash expressive
pixel
you'd think it would be obvious
if you were going to do
design work or
r d
incubation work innovation work or
whatever that you would understand the
community that you're designing for it's
not always obvious our mission at
microsoft is to empower every
human and organization on the planet to
achieve more
so that word every
is really where we focus that means
every human
not
e
not just humans who don't have hard
problems to solve
when you build relationships
uh like we do with our creative
collaborators and with our our community
you're invested you're deeply deeply
invested in their well-being as your
friend your collaborator and your fellow
human being and so that sense of
investment purpose and empathy is what
keeps
everybody that's the glue that sense of
of purpose that sense of common purpose
is the connective tissue
for all of us working in the space
music is a primitive and powerful means
of human communication and expression
through it we can share emotions
experiences and meanings with people
across cultures and languages and even
across time
for the last five years we've been
experimenting with a number of
approaches to inclusive music in support
of people with little to no motor
ability who may also be experiencing
speech loss
the projects i'm going to show you next
share one primary input constraint they
have to be able to be played using just
the eyes
that is no speech no other movement the
hands-free music project is an ongoing
collaboration between microsoft local
musicians and the als community
it includes a suite of eye-controlled
instruments and playing interfaces to
support composition performance and live
collaborations
we are
dangerfield
[Music]
a newbies my world was turned upside
down that was when i was
diagnosed with als
uh obviously a lot of negatives here
um but it's been very interesting
to see
some amazing things occur
in the aftermath
[Music]
[Laughter]
[Music]
do you enjoy
[Music]
[Music]
the cyclops is an eye instrument
designed by music technologist willie
payne during his summer internship with
the enable experiences group here in msr
willie built the cyclops to support
improvisation and performance scenarios
for musicians who only have control over
their eyes and yes willie is doing his
entire demo using just his eyes the
cyclops has three main screens
instrument sequence and audio effects
the instrument page is the primary page
for producing sounds
tiny red dot is where the eye tracker
detects my eye gaze
when i stare long enough at a button on
the screen it will cause it to press
playing a sound
the sequence tab contains extra controls
for playing with the loop
we can change the tempo to affect the
speed of the music
we can add accents to increase the
volume of individual notes or rests to
silence portions of the loop
on the effects page
we can fine tune how our sound actually
sounds
with just a few controls
the cyclops is an expressive instrument
capable of a wide range of sounds
all controlled and triggered just with
your eyes
to me one of the best ways to learn is
to do
and the way that you learn uh how you
can best serve a user community
who is facing profound constraints due
to ability loss because of disease
progression
is to
meet them
listen to them observe them
and you know take part in as many
activities as we can because
experiential observation is super
critical especially when you're trying
to decide we're going to focus your
efforts
our space in general um we do a
combination of things we do formal
fundamental research
in computer science write papers conduct
studies create prototypes proofs of
concepts evaluate them and release those
findings into the world so that we can
better inform the design of assistive
and expressive technologies in the
future
we also incubate our own designs and
technologies and we work iteratively and
collaboratively with our user community
to direct those activities we
are honest about what we learn we put
everything out
in the open source or in the public
domain
we share
we mentor students we mentor talent we
we do all of those activities because
that is how the progress is going to
happen
what way can we use technology in
service
to our goal of restoring or reinventing
lost capability if you don't center your
user community in every single activity
every single research design
or development or innovation activity
that you have you will not be successful
finally we'll hear from ken hinckley a
senior principal researcher who's also
from the epic group
ken is going to talk with us about how
devices can help to make sense out of
our increasingly complex work lives and
interactions with others he's worked on
everything from sensing techniques
cross-device interaction to novel device
form factors today he's going to tell
you a little bit about surface fleet
this technology has implications for
something many of us can relate to these
days the home workspace
i'm ken hinckley i work here at the lab
microsoft research
we innovate across hardware software and
the human experience
we do all kinds of cool research in
terms of interaction with devices and
also studying how people use devices and
also interact with one another
sweet study collaboration we study how
people do you know work in productivity
uh we study how they actually use their
devices in terms of just picking things
up looking at them moving them around
all this kind of stuff so we have lots
of you know very fun things that we do
one of the you know exciting areas that
we work in a lot is you know sort of
figuring out how to leverage smarts
about the world around us so bringing in
things like sensors or you know sort of
unusual inputs like speech or gaze or
you know camera inputs that kind of
thing to have a more natural way of
working with our devices the actual name
epic is an acronym right so it stands
for extended perception interaction and
cognition
so we work across all these topics so
the perception is the human perception
of how they see the world how they feel
the world how they hear the world
the interaction of course is you know
interacting with things around you
moving things like even just a simple
mouse or picking up a device that's an
interaction looking at someone that's an
interaction you might not think of it
that way but if our computers and
devices can sense these things and
respond to them appropriately
it makes it so they can be much smarter
about how they interpret you know what
we're doing in the world um and then the
cognition part is ties into you know not
so much artificial intelligence but
actual human you know genuine
intelligence right and how people think
about you know the world their tasks um
how they think about interacting with
one another uh and a lot of this is it's
not just in your brain but it's actually
externalized to the world so it um you
know the so the e and epic stands for
the you know this externalization or
this extended nature of intelligence
where your artifacts in the world
actually encode information and you so
you don't have to remember something you
stick it on a post-it note and you stick
it on your monitor and that's you know
externalizing that reminder
so that that's sort of the whole scope
of the research in the group um so for
example we do you know we've contributed
to the new future work in terms of
studying like what people do with their
devices while they're working at home
what kind of tasks are they doing how
are they dougling tasks with their kids
and their families and their
multiple clients and all these things we
work across
things like you know haptic feedback for
example so devices that can actually
respond to you with you can feel
tactilely you know what's going on in
terms of
you know if you're interacting with
virtual reality you can feel objects in
the world you can feel the shape of
things maybe if you're not if you're a
person who's not sighted
so we do all kinds of things along those
lines so we're kind of hurtling towards
this future where everyone has these
multiple screens and multiple devices
all around them and this is a trend
that's been particularly you know
aggravated by this you know new future
of work that we find ourselves in uh
where suddenly we're working at home you
know you've got your screen there maybe
you've got your kids ipad there you know
maybe you've got another computer at the
at the ready um and you're you're just
trying to get your work done across all
these devices
and so it's not unlike how people might
work you know in an office space where
they're spreading out uh information
across multiple pieces of paper they're
trying to you know lay out their
thoughts you know organize things and
kind of this informal way of putting
things in space and so multiple devices
can serve this same role so in a sense
people you know they like to use space
to think
so we like to think of ways to do that
in the digital world as well the main
idea of surface lead is to make it easy
to work across these multiple devices
and sort of the objective is to make it
easy to transition you know whatever
activity you're working on from one
device to another or from one location
to another or even to pass it off to
another person or to defer it to a
future time when you're actually ready
to work on it
so having both technologies and user
experiences that speak to that
is something that we're looking at very
closely um so in particular today i can
talk a bit about something we're doing
called the ultimate flexible workspace
which is a device concept we've been
exploring which makes us you can just
walk up to your your your workstation at
home uh with your tablet you just plop
it into this armature that's kind of
like an articulated lamp and then you
can kind of posture your tablet anywhere
in space
and then the really cool thing is when
you have multiple of these they're all
aware of one another you can sort of
juxtapose them you can split them apart
you can put one down one up
so you know just depending on your task
and what you're trying to do and what
information you want to see side by side
it makes it super easy to dynamically
create these formations of devices
because it also senses these transitions
all you have to do is put the device
next to another one and it says like hey
you want your document here and then you
can just drag it over or you know
whatever you want to do so it it makes
it really easy to work in these kind of
ad hoc workspaces that we have in our
homes where often you know space is very
limited uh you might have multiple
screens but you're just you're kind of
in this little you know sort of
improvised desks in the corner of your
living room somewhere right so uh making
it really easy to work in those kind of
spaces which you know also can translate
to the future when we get back to the
office and we're sharing spaces and
conference rooms and so forth
so in particular um last year which is
actually during this you know pandemic
remote work time we had a visiting
researcher by the name of nick marquart
uh who's a professor at universe
university college london uh also
happens to be a former intern who worked
with me you know over 10 years ago now
um and so nick and i have sort of done a
number of projects over the year and he
had this brainstorm about you know
basically repurposing articulated lamps
as a smart uh appliance you know desk
appliance for the for the work at home
situation um so the idea is that you
basically take you know we literally
ordered these armatures off of amazon i
think right so they're like 20 bucks uh
just added some sensors to them which
are just you know they're basically just
simple motion sensors it's the same as
you would have in your smartphone or
tablet already put a bunch of those on
you know the different
armature links there and then we can
actually sense where the tablet is in
space so as you you know position it you
can let go of it it just stays there it
knows its angle it knows its position is
notice how it's postured relative to you
you can put it over next to another
device if that one also has an armature
then that one also can know where it is
relative to the other so they have this
kind of spatial sense of like who's
nearby just like you would be aware of
if someone walked up behind you or next
to you you collaborate with them so in
that in that sense your devices are
spatially aware they're aware of one
another and they can present these
opportunities to just easily move
documents or pieces of work or to take
the input or camera from one device and
kind of access it on another so it makes
it really easy to work in the sort of
multi-device multi-screen future that we
see ourselves moving towards in terms of
technology trends as well as the
experiences that people want to have
with all these different screens in
their lives
right so when people are you know in
these spaces where they're working
across their digital content often we
have these very you know complex sort of
knowledge work type tasks we're doing
where there's you're trying to get
insights from across multiple documents
across different sites on the web so
you're trying to bring these together
into new artifacts that you're creating
for you know here's my idea here's my
report is my presentation um so and this
is you know it's cognitively demanding
work so just being limited to one little
screen to do that is just doesn't work
so well and particularly people we find
more and more are co-opting device other
devices they already have into this so
and and it goes beyond just um you know
documents so for example if i'm in a
teleconference call i might want to have
the document up on one screen i might
put the camera on my mobile phone which
is somewhere else so it gets the right
camera angle i can still see the content
that i'm talking about and i can divide
that across multiple devices you know or
say someone sends me a you know a
contract to sign right i don't want to
take that and try to sign it on a big
monitor i want to take that put it on my
tablet i use the pen there to hand write
it or annotate the document for
something more or maybe i sketch up an
idea there and then i want to get it
back onto my big screen to do the work
there
so people like being able to extend
their workspaces mix and match sort of
the the best of each device that they
have so they kind of leverage those
strengths and use their multiple screens
and multiple devices with all these
different capabilities to compliment one
another to kind of get more work done
think more clearly about what they're
doing and just kind of have that great
idea
