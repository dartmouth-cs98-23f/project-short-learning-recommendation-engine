second,duration,transcript
8.88,5.52,in this lesson you are going to
10.16,5.679,understand the concept of text mining
14.4,5.28,by the end of this lesson you will be
15.839,6.321,able to explain text mining execute text
19.68,4.96,processing task
22.16,4.32,so let's go ahead and understand text
24.64,4.08,mining in detail
26.48,4.4,let's first understand what text mining
28.72,4.72,is text mining is the technique of
30.88,5.44,exploring large amounts of unstructured
33.44,5.6,text data and analyzing it in order to
36.32,4.399,extract patterns from the text to data
39.04,4.16,it is aided by software that can
40.719,4.801,identify concepts patterns topics
43.2,5.12,keywords and other attributes in the
45.52,5.12,data it utilizes computational
48.32,4.399,techniques to extract and summarize the
50.64,5.52,high quality information from
52.719,5.68,unstructured textual resources let's
56.16,4.64,understand the flow of text mining
58.399,3.921,there are five techniques used in text
60.8,3.84,mining system
62.32,3.92,information extraction or text
64.64,4.159,pre-processing
66.24,4.559,this is used to examine the unstructured
68.799,3.841,text by searching out the important
70.799,3.32,words and finding the relationships
72.64,4.159,between them
74.119,5.401,categorization or text transformation
76.799,5.36,attribute generation categorization
79.52,4.48,technique labels the text document under
82.159,4.561,one or more categories
84.0,5.439,classification of text data is done
86.72,4.56,based on input output examples with
89.439,4.481,categorization
91.28,4.879,clustering or attribute selection
93.92,4.8,clustering method is used to group text
96.159,5.041,documents that have similar content
98.72,4.8,clusters are the partitions and each
101.2,4.959,cluster will have a number of documents
103.52,4.72,with similar content clustering make
106.159,4.32,sure that no document will be omitted
108.24,5.04,from the search and it derives all the
110.479,5.68,documents that have similar content
113.28,4.56,visualization technique the process of
116.159,4.64,finding relevant information is
117.84,5.279,simplified by visualization technique
120.799,4.801,this technique uses text flags to
123.119,5.12,represent a group of documents or a
125.6,4.56,single document and compactness is
128.239,4.401,indicated using colors
130.16,5.12,visualization technique helps to display
132.64,3.679,textual information in a more attractive
135.28,3.28,way
136.319,3.681,summarization or interpretation or
138.56,3.36,evaluation
140.0,4.239,summarization technique will help to
141.92,4.959,reduce the length of the document and
144.239,4.72,summarize the details of the documents
146.879,4.72,it makes the document easy to read for
148.959,4.081,users and understand the content at the
151.599,3.28,moment
153.04,4.64,let's understand the significance of
154.879,4.64,text mining
157.68,3.919,document clustering
159.519,4.561,document clustering is an important part
161.599,4.72,of text mining it has many applications
164.08,4.72,in knowledge management and information
166.319,4.801,retrieval clustering makes it easy to
168.8,4.64,group similar documents into meaningful
171.12,4.32,groups such as in newspapers where
173.44,4.32,sections are often grouped as business
175.44,4.24,sports politics and so on
177.76,3.759,pattern identification
179.68,3.919,text mining is the process of
181.519,4.64,automatically searching large amount of
183.599,3.601,text for text patterns and recognition
186.159,2.961,of features
187.2,4.16,features such as telephone numbers and
189.12,3.92,email addresses can be extracted using
191.36,3.2,pattern matches
193.04,3.68,product insights
194.56,4.399,text mining helps to extract large
196.72,4.56,amounts of text for example customer
198.959,4.56,reviews about the products
201.28,4.8,mining consumer reviews can reveal
203.519,5.521,insights like most loved feature most
206.08,5.6,hated feature improvements required and
209.04,5.52,reviews of competitors products
211.68,5.199,security monitoring text mining helps in
214.56,4.16,monitoring and extracting information
216.879,4.161,from news articles and reports for
218.72,4.72,national security purposes
221.04,4.72,text mining make sure to use all of your
223.44,4.24,available information it is a more
225.76,4.24,effective and productive knowledge
227.68,5.04,discovery that allows you to make better
230.0,5.36,informed decisions automate information
232.72,4.159,intensive processes gather business
235.36,3.92,critical insights and mitigate
236.879,4.72,operational risk
239.28,4.8,let's look at the applications of text
241.599,2.481,mining
244.239,4.401,speech recognition speech recognition is
246.72,5.12,the recognition and translation of
248.64,5.04,spoken language into text and vice versa
251.84,4.239,speech often provides valuable
253.68,5.119,information about the topics subjects
256.079,4.641,and concepts of multimedia content
258.799,4.561,information extraction from speech is
260.72,5.199,less complicated yet more accurate and
263.36,5.36,precise than multimedia content this
265.919,5.681,fact motivates content speech analysis
268.72,5.28,for multimedia data mining and retrieval
271.6,5.12,where audio and speech processing is a
274.0,4.479,key enabling technology
276.72,4.16,spam filtering
278.479,4.881,spam detection is an important method in
280.88,4.96,which textual information contained in
283.36,4.24,an email is extracted and used for
285.84,4.0,discrimination
287.6,4.48,text mining is useful in automatic
289.84,4.0,detection of spam emails based on the
292.08,4.08,filtering content
293.84,5.68,using text mining an email service
296.16,5.759,provider such as gmail or yahoo mail
299.52,5.119,checks the content of an email and if
301.919,4.881,some malicious text is found in the mail
304.639,5.041,then that email is marked as spam and
306.8,4.16,sent to the spam folder
309.68,3.44,analysis
310.96,4.48,it is done in order to determine if a
313.12,4.639,given sentence expresses positive
315.44,4.4,neutral or negative sentiments
317.759,5.041,sentiment analysis is one of the most
319.84,5.76,popular applications of text analytics
322.8,5.04,the primary aspect of sentiment analysis
325.6,4.72,includes data analysis of the body of
327.84,5.12,the text for understanding the opinion
330.32,5.599,expressed by it and other key factors
332.96,4.72,comprising modality and mood
335.919,4.481,usually the process of sentiment
337.68,5.359,analysis works best on text that has a
340.4,5.04,subjective context than on that with
343.039,5.201,only an objective context
345.44,5.599,e-commerce personalization
348.24,5.28,text mining is used to suggest products
351.039,5.041,that fit into a user's profile
353.52,5.119,text mining is increasingly being used
356.08,5.119,by e-commerce retailers to learn more
358.639,5.041,about the consumers as it is the process
361.199,4.481,of analyzing textual information in
363.68,3.28,order to identify patterns and gain
365.68,3.84,insights
366.96,4.4,ecommerce retailers can target specific
369.52,4.32,individuals or segments with
371.36,4.399,personalized offers and discounts to
373.84,4.56,boost sales and increase customer
375.759,4.88,loyalty by identifying customer purchase
378.4,4.0,patterns and opinions on particular
380.639,4.081,products
382.4,5.359,let's look at natural language toolkit
384.72,5.12,library in detail
387.759,4.88,natural language toolkit is a set of
389.84,5.12,open source python models that are used
392.639,5.921,to apply statistical natural language
394.96,5.519,processing on human language data
398.56,4.479,let's see how you can do environment
400.479,5.041,setup of nltk
403.039,4.961,go to windows start and launch python
405.52,4.48,interpreter from anaconda prompt and
408.0,4.0,enter the following commands
410.0,3.84,enter command python to check the
412.0,3.039,version of python installed on your
413.84,4.32,system
415.039,6.801,enter import nltk to link you to the
418.16,5.159,nltk library available to download then
421.84,3.6,enter
423.319,4.761,nltk.download function that will open
425.44,5.12,the nltk download window
428.08,4.959,check the download directory select all
430.56,6.56,packages and click on download
433.039,6.72,this will download nltk onto your python
437.12,4.96,once you have downloaded the nltk you
439.759,3.28,must check the working and functionality
442.08,3.04,of it
443.039,4.961,in order to test the setup enter the
445.12,5.84,following command in python idle
448.0,4.479,from nltk
450.96,5.6,import brown
452.479,7.361,brown dot word parenthesis parenthesis
456.56,5.28,the brown is an nltk corpus that shows
459.84,4.56,the systematic difference between
461.84,3.84,different genres available words
464.4,3.68,function will give you the list
465.68,4.16,available words in the genre
468.08,4.88,the given output shows that we have
469.84,5.12,successfully tested the nltk installed
472.96,4.079,on python
474.96,5.84,let's now understand how you can read a
477.039,6.081,specific module from nltk corpora if you
480.8,6.32,want to import an entire module from
483.12,6.88,nltk corpora use asterisk symbol with
487.12,5.0,that module name import command
490.0,4.879,enter the command from
492.12,4.96,nltk.book import asterisk
494.879,4.72,it will load all the items available in
497.08,5.559,nltk's book module
499.599,6.241,now in order to explore brown corpus
502.639,4.801,enter the command nltk.corpus
505.84,3.84,import brown
507.44,3.439,this will import brown corpus on the
509.68,4.4,python
510.879,5.761,enter brown dot categories function to
514.08,5.12,load the different genres available
516.64,5.759,select a genre and assign that genre to
519.2,5.84,a variable using the following syntax
522.399,3.681,variable name is equal to brown dot
525.04,4.0,words
526.08,5.04,categories is equal to genre name
529.04,4.799,now in order to see the available words
531.12,6.48,inside the selected genre just enter the
533.839,6.12,defined variable name as a command
537.6,5.12,let's understand text extraction and
539.959,5.081,pre-processing in detail
542.72,3.92,so let's first understand the concept of
545.04,4.16,tokenization
546.64,4.879,tokenization is the process of removing
549.2,5.199,sensitive data and placing unique
551.519,4.801,symbols of identification in that place
554.399,4.161,in order to retain all the essential
556.32,3.68,information concerned with the data by
558.56,3.52,its security
560.0,5.04,it is a process of breaking running
562.08,5.52,streams of text into words and sentences
565.04,5.68,it works by segregating words using
567.6,5.919,punctuation and spaces
570.72,4.799,text extraction and pre-processing
573.519,4.801,engrams
575.519,5.121,now let's look at what n-gram is and how
578.32,4.24,it is helpful in text mining
580.64,4.639,n-gram is the simplest model that
582.56,4.8,assigns these probabilities to sequences
585.279,4.161,of words or sentences
587.36,4.88,n-grams are combinations of adjacent
589.44,4.32,words or letters of length and in the
592.24,4.48,source text
593.76,5.199,so engram is very helpful in text mining
596.72,3.679,when it is required to extract patterns
598.959,3.841,from the text
600.399,4.401,as in the given example this is a
602.8,4.479,sentence all of these words are
604.8,4.64,considered individual words and thus
607.279,5.68,represent unigrams
609.44,5.839,a 2 gram or bigram is a two-word
612.959,3.281,sequence of words like this is
615.279,3.68,is a
616.24,4.96,or a sentence and a three gram or
618.959,7.841,trigram is a three word sequence of
621.2,7.92,words like this is a or is a sentence
626.8,4.039,let's now understand what stop words are
629.12,4.56,and how you can remove
630.839,5.081,them stop words are natural language
633.68,5.04,words that have negligible meaning such
635.92,3.599,as a and and
638.72,3.2,or
639.519,4.481,the and other similar words
641.92,4.32,these words also will take up space in
644.0,4.64,the database or increase the processing
646.24,5.44,time so it is better to remove such
648.64,5.439,words by storing a list of stop words
651.68,5.36,you can find the list of stop words in
654.079,5.281,the nltk data directory that is stored
657.04,4.239,in 16 different languages
659.36,4.479,use the following command to list the
661.279,5.361,stop words of english language defined
663.839,6.081,in nltk corpus
666.64,6.199,importing nltk will import the nltk
669.92,6.479,corpus for that instance enter from
672.839,6.521,nltk.corpus import stopwords will import
676.399,5.68,stop words from nltk corpus
679.36,5.44,now set the language as english so use
682.079,6.801,set function as set under braces stop
684.8,6.159,words dot words set genre as english
688.88,4.56,stop words are filtered out before
690.959,5.041,processing of natural language data as
693.44,4.48,they don't reveal much information
696.0,4.0,so as you can see in the given example
697.92,4.96,before filtering the sentence the
700.0,5.279,tokenization of stop word is processed
702.88,4.24,in order to remove these stop words and
705.279,3.601,the filtering is applied in order to
707.12,4.24,filter the sentence based on some
708.88,4.959,criteria
711.36,4.32,text extraction and pre-processing
713.839,4.0,stemming
715.68,4.719,stemming is used to reduce a word to
717.839,6.24,stem or base word by removing suffixes
720.399,5.44,such as helps helping help and helper to
724.079,4.401,the root word help
725.839,4.881,the stemming process or algorithm is
728.48,4.64,generally called a stemmer there are
730.72,4.96,various stemming algorithms such as
733.12,4.719,porter stemmer lancaster stemmer
735.68,4.24,snowball stemmer etc
737.839,5.361,use any of the stemmers defined under
739.92,6.0,nltk stem corpus in order to perform
743.2,4.96,stemming as shown in the example here we
745.92,4.479,have used porter stemmer when you
748.16,4.4,observe the output you will see that all
750.399,4.961,of the words given have been reduced to
752.56,5.36,their root word or stem
755.36,4.96,text extraction and pre-processing
757.92,4.64,limitization
760.32,4.88,lemmatization is the method of grouping
762.56,4.88,the various inflected types of a word in
765.2,5.92,order that they can be analyzed as one
767.44,6.079,item it uses vocabulary list or a
771.12,6.0,morphological analysis to get the root
773.519,5.44,word it uses wordnet database that has
777.12,3.68,english words linked together by their
778.959,4.401,semantic relationship
780.8,4.88,as you can observe the given example the
783.36,4.8,different words have been extracted to
785.68,5.12,their relevant morphological word using
788.16,6.0,limitization
790.8,4.88,text extraction and pre-processing pos
794.16,3.119,tagging
795.68,3.68,let's now look at different part of
797.279,4.721,speech tags available in the national
799.36,5.36,language toolkit library
802.0,5.6,a pos tag is a special label assigned to
804.72,5.119,each token or word in a text corpus to
807.6,4.4,indicate the part of speech and often
809.839,4.56,also other grammatical categories such
812.0,6.399,as tense number either plural or
814.399,7.041,singular case etc pos tags are used in
818.399,5.521,text analysis tools and algorithms and
821.44,3.839,also in corpus searches so look at the
823.92,4.08,given example
825.279,5.921,here alice wrote a program is the source
828.0,6.399,text given the pos tags given are alice
831.2,6.319,is a noun wrote is a verb a is an
834.399,4.961,article and program is an adjective look
837.519,4.721,at the given example to understand how
839.36,5.12,pos tags are defined so the given
842.24,4.24,sentence or paragraph contains different
844.48,2.96,words that represent different parts of
846.48,3.039,speech
847.44,4.8,we will first use tokenization and
849.519,5.521,removal of stop words and then allocate
852.24,4.56,the different pos tags these are shown
855.04,2.88,with different words in the given
856.8,4.719,sentence
857.92,6.159,pos tags are useful for lemmatization in
861.519,6.721,building named entity recognition and
864.079,7.281,extracting relationships between words
868.24,4.719,text extraction and pre-processing named
871.36,3.68,entity recognition
872.959,5.12,now let's understand what named entity
875.04,5.2,recognition is all about ner seeks to
878.079,4.641,extract a real-world entity from the
880.24,4.88,text and sorts it into predefined
882.72,5.2,categories such as names of people
885.12,4.719,organizations locations etc
887.92,4.0,many real-world questions can be
889.839,3.281,answered with the help of name entity
891.92,3.279,recognition
893.12,4.0,were specified products mentioned in
895.199,3.76,complaints or reviews
897.12,4.0,does the tweet contain the name of a
898.959,3.921,person does the tweet contain the
901.12,3.92,person's address
902.88,5.36,as you can see in the given example
905.04,5.12,google america larry page etc are the
908.24,3.2,names of a person place or an
910.16,3.359,organization
911.44,4.8,so these are considered named entities
913.519,4.161,and have different tags such as person
916.24,6.56,organization
917.68,8.399,gpe or geopolitical entity etc
922.8,5.12,nlp process workflow
926.079,5.2,now you have an understanding of all
927.92,6.08,nltk tools so now let's understand the
931.279,6.081,natural language processing workflow
934.0,6.56,step one tokenization it splits text
937.36,6.64,into pieces tokens or words and removes
940.56,6.0,punctuation step two stop word removal
944.0,6.32,it removes commonly used words such as
946.56,5.04,the is are etc which are not relevant to
950.32,4.16,the analysis
951.6,5.28,step three stemming and limitization
954.48,5.52,it reduces words to base form in order
956.88,6.56,to be analyzed as a single item step 4
960.0,6.88,pos tagging it tags words to be part of
963.44,6.24,speech such as noun verb adjective etc
966.88,5.519,based on the definition and context
969.68,4.88,step 5 information retrieval
972.399,3.921,it extracts relevant information from
974.56,4.24,the source
976.32,5.04,mo1 brown corpus
978.8,4.959,problem statement the brown university
981.36,5.52,standard corpus of present-day american
983.759,6.401,english also known popularly as brown
986.88,5.6,corpus was compiled in the 1960s as a
990.16,5.44,general corpus in the field of corpus
992.48,6.159,linguistics it contains 500 samples of
995.6,5.359,english language text totaling roughly 1
998.639,5.681,million words compiled from works
1000.959,5.601,published in the united states in 1961.
1004.32,4.879,we will be working on one of the subset
1006.56,3.92,data set and perform text processing
1009.199,4.721,tasks
1010.48,6.799,let us import the nltk library and read
1013.92,6.64,the ca underscore 10 corpus
1017.279,4.8,import nltk
1020.56,4.479,we will have to make sure that there are
1022.079,5.041,no slashes in between hence we will use
1025.039,5.241,the replace function within pandas for
1027.12,3.16,the same
1043.839,4.521,let's have a look at the data once
1078.84,7.839,tokenization after performing sentence
1081.919,4.76,tokenization on the data we obtain
1096.559,4.641,similarly after applying sentence
1098.64,6.919,tokenizer the resulting output shows all
1101.2,4.359,individual words tokens
1113.36,5.8,stop word removal let's import the
1116.0,7.84,stopword library from
1119.16,4.68,nltk.corpus import stop words
1124.08,3.839,we also need to ensure that the text is
1126.4,4.88,in the same case
1127.919,5.601,nltk has its own list of stop words we
1131.28,5.36,can check the list of stop words using
1133.52,5.92,stop words dot words and english inside
1136.64,2.8,the parenthesis
1141.919,5.721,map the lowercase string with our list
1144.16,3.48,of word tokens
1160.24,5.6,let's remove the stop words using the
1162.48,5.68,english stop words list in nltk
1165.84,7.0,we will be using set checking as it is
1168.16,4.68,faster in python than a list
1179.679,5.481,by removing all stop words from the text
1182.0,3.16,we obtain
1193.76,4.48,often we want to remove the punctuations
1195.76,4.72,from the documents too since python
1198.24,4.319,comes with batteries included we have a
1200.48,6.36,string dot punctuation
1202.559,4.281,from string import punctuation
1219.28,5.56,combining the punctuation with the stop
1221.2,3.64,words from nltk
1239.28,4.44,removing stop words with punctuation
1255.28,4.32,stemming and limitization
1257.76,4.0,we will be using stemming and
1259.6,6.16,limitization to reduce words to their
1261.76,6.88,root form for example walks walking walk
1265.76,5.2,will be reduced to their root word walk
1268.64,3.44,importing porter stemmer as the stemming
1270.96,2.04,library
1272.08,5.76,from
1273.0,4.84,nltk.stem import porter stemmer
1278.08,3.8,printing the stem words
1295.2,5.24,import the wordnet lemmitizer from
1297.44,3.0,nltk.stem
1310.559,3.881,printing the root words
1325.039,5.961,we also need to evaluate the pos tags
1327.6,3.4,for each token
1338.64,4.159,create a new word list and store the
1340.72,6.16,list of word tokens against each of the
1342.799,6.88,sentence tokens in data 2. for i in
1346.88,2.799,tokenized
1357.28,4.48,also we will check if there were any
1359.44,4.64,stop words in the recently created word
1361.76,2.32,list
1371.12,4.96,we will now tag the word tokens
1373.039,6.081,accordingly using the pos tags and print
1376.08,3.04,the tagged output
1387.2,4.959,for our final text processing task we
1389.84,4.719,will be applying named entity
1392.159,5.76,recognition to classify named entities
1394.559,5.761,in text into predefined categories such
1397.919,4.801,as the names of persons organizations
1400.32,5.28,locations expressions of times
1402.72,5.88,quantities monetary values percentages
1405.6,3.0,etcetera
1445.12,3.919,now press the tagged sentences under the
1447.52,4.32,chunk parser
1449.039,5.921,if we set the parameter binary equals
1451.84,4.48,true then named entities are just tagged
1454.96,3.599,as ne
1456.32,5.04,otherwise the classifier adds category
1458.559,5.521,labels such as person organization and
1461.36,2.72,gpe
1465.12,4.4,create a function named as extract
1467.279,6.52,entity names along with an empty list
1469.52,4.279,named as entity names
1478.08,6.079,we will now extract named entities from
1480.559,7.281,a nltk chunked expression and store them
1484.159,3.681,in the empty created above
1519.2,4.64,again we will set the entity names list
1521.36,5.52,as an empty list and will extract the
1523.84,6.68,entity names by iterating over each tree
1526.88,3.64,in chunked sentences
1556.24,5.28,great we have seen how to explore and
1558.48,5.84,examine the corpus using text processing
1561.52,4.72,techniques let's quickly recap the steps
1564.32,6.16,we've covered so far
1566.24,7.12,one import the nltk library to perform
1570.48,4.4,tokenization three perform stemming and
1573.36,4.799,allematization
1574.88,6.24,four remove stop words five perform
1578.159,6.481,named entity recognition
1581.12,6.48,structuring sentences syntax
1584.64,5.279,let's first understand what syntax is
1587.6,3.6,syntax is the grammatical structure of
1589.919,3.041,sentences
1591.2,4.479,in the given example this can be
1592.96,5.68,interpreted as syntax and it is similar
1595.679,4.961,to the ones you use while writing codes
1598.64,4.72,knowing a language includes the power to
1600.64,5.12,construct phrases and sentences out of
1603.36,4.16,morphemes and words the part of the
1605.76,3.36,grammar that represents a speaker's
1607.52,4.159,knowledge of these structures and their
1609.12,4.559,formation is called syntax
1611.679,4.401,phrase structure rules are rules that
1613.679,4.561,determine what goes into a phrase that
1616.08,4.16,is constituents of a phrase and how the
1618.24,4.48,constituents are ordered
1620.24,4.88,constituent is a word or group of words
1622.72,4.959,that operate as a unit and can be used
1625.12,5.6,to frame larger grammatical units
1627.679,4.961,the given diagram represents that a noun
1630.72,3.92,phrase is determined when a noun is
1632.64,4.0,combined with a determiner and the
1634.64,4.0,determiner can be optional
1636.64,4.639,a sentence is determined when a noun
1638.64,4.8,phrase is combined with a verb phrase
1641.279,3.921,a verb phrase is determined when a verb
1643.44,5.04,is combined optionally with the noun
1645.2,5.2,phrase and prepositional phrase and a
1648.48,3.92,prepositional phrase is determined when
1650.4,3.36,a preposition is combined with a noun
1652.4,3.68,phrase
1653.76,4.88,a tree is a representation of
1656.08,4.64,syntactic's structure of formulation of
1658.64,4.48,sentences or strings
1660.72,6.319,consider the given sentence the factory
1663.12,6.24,employs 12.8 percent of bradford county
1667.039,5.201,what can be the syntax for pairing this
1669.36,4.24,statement let's understand this a tree
1672.24,3.36,is produced that might help you
1673.6,4.0,understand that the subject of the
1675.6,4.72,sentence is the factory
1677.6,6.24,the predicate is employees and the
1680.32,6.0,target is 12.8 percent which in turn is
1683.84,5.12,modified by bradford county
1686.32,5.2,syntax parses are often a first step
1688.96,6.4,toward deep information extraction or
1691.52,3.84,semantic understanding of text
1695.44,4.359,rendering syntax trees
1698.48,3.76,download the
1699.799,4.921,corresponding.exe file to install the
1702.24,4.88,ghost script rendering engine based on
1704.72,5.52,your system configuration in order to
1707.12,5.12,render syntax trees in your notebook
1710.24,4.159,let's understand how you can set up the
1712.24,4.24,environment variable
1714.399,4.321,once you have downloaded and installed
1716.48,5.12,the file go to the folder where it is
1718.72,4.959,installed and copy the path of the file
1721.6,4.319,now go to system properties and under
1723.679,4.801,advanced properties you will find the
1725.919,5.36,environment variable button click on
1728.48,4.16,that to open the pop-up box tab of the
1731.279,4.241,environment
1732.64,4.88,now open the bin folder and add the path
1735.52,3.519,to the bin folder in your environment
1737.52,3.519,variables
1739.039,3.921,now you will have to modify the path of
1741.039,4.161,the environment variable
1742.96,4.8,use the given code to test the working
1745.2,5.359,of syntax tree after the setup is
1747.76,5.76,successfully installed
1750.559,4.801,structuring sentences chunking and chunk
1753.52,4.08,parsing
1755.36,4.08,the process of extraction of phrases
1757.6,4.559,from unstructured text is called
1759.44,4.719,chunking instead of using just simple
1762.159,4.161,tokens which may not represent the
1764.159,4.64,actual meaning of the text it is
1766.32,5.599,advisable to use phrases such as indian
1768.799,5.6,team as a single word instead of indian
1771.919,4.561,and team as separate words
1774.399,4.801,the chunking segmentation refers to
1776.48,5.199,identifying tokens and labeling refers
1779.2,4.479,to identifying the correct tag
1781.679,4.961,these chunks correspond to mixed
1783.679,5.841,patterns in some way to extract patterns
1786.64,5.039,from chunks we need chunk parsing
1789.52,4.48,the chunk parsing segment refers to
1791.679,4.161,identifying strings of tokens and
1794.0,3.84,labeling refers to identifying the
1795.84,4.559,correct chunk type
1797.84,5.199,let's look at the given example you can
1800.399,5.601,see here that yellow is an adjective dog
1803.039,5.12,is a noun and the is the determiner
1806.0,3.36,which are chunked together into a noun
1808.159,3.601,phrase
1809.36,4.799,similarly chunk parsing is used to
1811.76,4.399,extract patterns and to process such
1814.159,4.321,patterns from multiple chunks while
1816.159,4.0,using different parsers
1818.48,3.84,let's take an example and try to
1820.159,5.201,understand how chunking is performed in
1822.32,5.44,python let's consider the sentence the
1825.36,5.199,little mouse ate the fresh cheese
1827.76,5.6,assigned to a variable named scent
1830.559,5.921,using the word tokenize function under
1833.36,4.88,nltk corpora you can find out the
1836.48,3.36,different tags associated with the
1838.24,3.52,sentence provided
1839.84,3.68,so as you can see in the output
1841.76,4.0,different tags have been allocated
1843.52,4.879,against each of the words from the given
1845.76,5.919,sentence using chunking
1848.399,5.441,np chunk and parser
1851.679,3.921,you will now create grammar from a noun
1853.84,4.16,phrase and will mention the tags you
1855.6,4.88,want in your chunk phrase within the
1858.0,4.72,function here you have created a regular
1860.48,5.199,expression matching the string
1862.72,5.28,the given regular expression indicates
1865.679,4.961,optional determiner followed by optional
1868.0,4.96,number of adjective followed by a noun
1870.64,4.639,you will now have to parse the chunk
1872.96,5.599,therefore you will create a chunk parser
1875.279,5.76,and pass your noun phrase string to it
1878.559,4.801,the parser is now ready you will use the
1881.039,4.961,parse parenthesis parenthesis within
1883.36,4.4,your chunk parser to parse your sentence
1886.0,3.919,the sentence provided is the little
1887.76,4.32,mouse ate the fresh cheese
1889.919,3.76,this sentence has been parsed and the
1892.08,3.76,tokens that match the regular
1893.679,4.961,expressions are chunked together into
1895.84,5.199,noun phrases np
1898.64,4.879,create a verb phrase chunk using regular
1901.039,4.321,expressions the regular expression has
1903.519,5.04,been defined as optional personal
1905.36,5.439,pronoun followed by zero or more verbs
1908.559,3.761,with any of its type followed by any
1910.799,4.081,type of adverb
1912.32,5.92,you'll now create another chunk parser
1914.88,5.6,and pass the verb phrase string to it
1918.24,4.64,create another sentence and tokenize it
1920.48,5.039,add pos tags to it
1922.88,5.44,so the new sentence is she is walking
1925.519,6.081,quickly to the mall and the pos tag has
1928.32,6.079,been allocated from nltk corpora
1931.6,5.84,now use the new verb phrase parser
1934.399,5.041,to parse the tokens and run the results
1937.44,3.839,you can look at the given tree diagram
1939.44,4.079,which shows a verb parser where a
1941.279,5.561,pronoun followed by two verbs and an
1943.519,7.921,adverb are chunked together into a verb
1946.84,6.92,parse structuring sentences chinking
1951.44,4.88,chinking is the process of removing a
1953.76,5.12,sequence of tokens from a chunk how does
1956.32,4.64,chunking work the whole chunk is removed
1958.88,4.56,when the sequence of tokens spans an
1960.96,5.199,entire chunk if the sequence is at the
1963.44,5.76,start or the end of the chunk the tokens
1966.159,5.201,are removed from the start and end and a
1969.2,4.319,smaller chunk is retained
1971.36,4.24,if the sequence of tokens appears in the
1973.519,4.081,middle of the chunk these tokens are
1975.6,4.319,removed leaving two chunks where there
1977.6,4.959,was only one before
1979.919,4.961,consider you create a chinking grammar
1982.559,4.881,string containing three things chunk
1984.88,5.12,name the regular expression sequence of
1987.44,4.079,a chunk the regular expression sequence
1990.0,3.519,of your
1991.519,5.361,here in the given code we have the chunk
1993.519,5.921,regular expression as optional personal
1996.88,5.44,pronoun followed by zero or more
1999.44,6.16,occurrences of any type of the verb type
2002.32,5.76,followed by zero or more occurrences of
2005.6,4.72,any of the adverb types the
2008.08,4.319,regular expression says that it needs to
2010.32,4.8,check for the adverb in the extracted
2012.399,4.801,chunk and remove it from the chunk
2015.12,3.279,inside the chinking block with open
2017.2,3.599,curly braces
2018.399,5.28,and closing curly braces you have
2020.799,6.081,created one or more adverbs
2023.679,4.0,you will now create a parser from nltk
2026.88,4.88,dot
2027.679,5.521,reg exp parser and pass the
2031.76,3.759,grammar to it
2033.2,4.88,now use the new parser to parse
2035.519,3.841,the tokens sent three and run the
2038.08,2.959,results
2039.36,4.159,as you can see the parse tree is
2041.039,4.321,generated while comparing the syntax
2043.519,4.321,tree of the parser with that of
2045.36,5.44,the original chunk you can see that the
2047.84,4.239,token is quickly adverb chinked out of
2050.8,3.44,the chunk
2052.079,3.84,let's understand how to use context-free
2054.24,4.8,grammar
2055.919,7.2,a context-free grammar is a four-tuple
2059.04,6.16,some ntrs where sum is an alphabet and
2063.119,5.441,each character in sum is called a
2065.2,5.919,terminal nt is a set and each element in
2068.56,6.4,nt is called a non-terminal
2071.119,8.161,r the set of rules is a subset of nt
2074.96,7.6,times the set of sum u and t s the start
2079.28,5.44,symbol is one of the symbols in nt
2082.56,4.96,a context-free grammar generates a
2084.72,3.76,language l capturing constituency and
2087.52,3.2,ordering
2088.48,4.399,in cfg the start symbol is used to
2090.72,4.24,derive the string you can derive the
2092.879,4.321,string by repeatedly replacing a
2094.96,4.879,non-terminal on the right hand side of
2097.2,5.2,the production until all non-terminals
2099.839,4.561,have been replaced by terminal symbols
2102.4,4.64,let's understand the representation of
2104.4,5.199,context-free grammar through an example
2107.04,5.36,in context-free grammar a sentence can
2109.599,5.441,be represented as a noun phrase followed
2112.4,5.36,by a verb phrase noun phrase can be a
2115.04,6.559,determiner nominal a nominal can be a
2117.76,6.64,noun vp represents the verb phrase
2121.599,4.801,a can be called a determiner flight can
2124.4,4.16,be called a noun
2126.4,4.4,consider the string below where you have
2128.56,4.88,certain rules when you look at the given
2130.8,4.88,context-free grammar a sentence should
2133.44,4.96,have a noun phrase followed by a verb
2135.68,5.76,phrase a verb phrase is a verb followed
2138.4,4.0,by a noun a verb can either be saul or
2141.44,3.84,met
2142.4,5.52,noun phrases can either be john or jim
2145.28,4.64,and a noun can either be a dog or a cat
2147.92,4.4,check the possible list of sentences
2149.92,4.32,that can be generated using the rules
2152.32,4.4,use the join function to create the
2154.24,4.32,possible list of sentences you can check
2156.72,3.84,the different rules of grammar for
2158.56,3.76,sentence formation using the production
2160.56,4.72,function it will show you the different
2162.32,5.6,tags used and the defined context-free
2165.28,5.52,grammar for the given sentence
2167.92,4.88,demo 2 structuring sentences
2170.8,4.24,problem statement a company wants to
2172.8,3.44,perform text analysis for one of its
2175.04,3.2,data sets
2176.24,3.16,you are provided with this data set
2178.24,4.08,named
2179.4,5.4,tweets.csv which has tweets of six us
2182.32,5.279,airlines along with their sentiments
2184.8,4.799,positive negative and neutral the tweets
2187.599,4.401,are present in the text column and
2189.599,4.161,sentiments in airline underscore
2192.0,4.32,sentiment column
2193.76,5.04,we will be retrieving all tags starting
2196.32,5.56,with at the rate in the data set and
2198.8,5.68,save the output in a file called
2201.88,5.0,references.txt let us first import the
2204.48,5.4,pandas library and read the tweets data
2206.88,3.0,set
2256.72,5.119,extract the features text and airline
2259.28,2.559,sentiment
2268.32,7.12,we will iterate through the data set
2270.16,5.28,using reg x find the relevant tweets
2278.24,7.16,now we will import the inter tools
2280.4,5.0,module it returns efficient iterators
2320.64,5.16,the result is stored in a file named
2322.8,3.0,references.txt
2345.92,5.76,let's extract all noun phrases and save
2348.32,5.519,them in a file named noun phrases for
2351.68,5.72,left caret airline
2353.839,3.561,sentiment rightcarrotreview.txt
2398.88,5.12,here left carat airline underscore
2401.599,4.48,sentiment right carrot has three
2404.0,7.4,different values positive negative and
2406.079,5.321,neutral so three files will be created
2439.119,7.401,now we will iterate all the leaf nodes
2441.52,5.0,and assign them to noun phrases variable
2493.359,5.201,this means that the functions in itter
2495.359,7.0,tools operate on iterators to produce
2498.56,3.799,more complex iterators
2513.52,6.599,using the map function we will get all
2515.76,4.359,the noun phrases from the text
2529.839,3.881,putting it into list
2557.52,5.079,creating a file name in the name of
2559.599,3.0,review.txt
2620.4,4.24,great we have now seen how to explore
2622.48,4.48,and examine the corpus using text
2624.64,5.12,processing techniques let's quickly
2626.96,6.08,recap the steps we've covered so far
2629.76,6.0,one import the data set two extract noun
2633.04,5.76,phrases this brings us to the end of
2635.76,4.96,text mining you are now able to
2638.8,6.6,explain text mining
2640.72,4.68,execute test processing tasks
2648.0,3.52,hi there if you like this video
2649.76,4.319,subscribe to the simply learn youtube
2651.52,4.72,channel and click here to watch similar
2654.079,5.481,videos turn it up and get certified
2656.24,3.32,click here
2667.44,2.08,you
