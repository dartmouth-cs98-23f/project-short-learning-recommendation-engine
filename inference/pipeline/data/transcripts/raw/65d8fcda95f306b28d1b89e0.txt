and masters from carnegie mellon and his
phd from ghand university he has over a
decade experience in computer computer
architecture both in industry as well as
academia and has received several awards
for his research in simulation sampling
and modeling today he will introduce us
to computer architecture research miss
talk make an impact how computer
architecture researchers design novel ai
accelerators processes and improved
security techniques let's warmly welcome
eric
thank you so much thanks very thanks for
the introduction
um so today what i'm going to do um
i'm going to talk a bit about what
computer architecture is uh and also
talk a little bit about uh what exactly
research is uh to me
um maybe some people are thinking about
a career in research but they don't know
all the uh what that entails so it can
be interesting to see
um
what i you know my vision of what
research is and then we'll talk a bit
about some of the work that we've been
doing in our in our lab
um one of the things that we do is we
cover a large num a large space of
um the computer architecture
i guess
research area and so if you see on the
left here we have high performance
client devices so this means your mobile
phones for example your drones
maybe building energy efficient servers
or
simulating
new future workloads for example so all
of these different pieces
fall into the umbrella of computer
architecture
but increasingly
what we've been seeing is more and more
work on processor security or security
in general especially
for all of our devices our connected
devices
and we also see
ai acceleration being extremely
important uh for example the work that
angela was talking about um she's
working on some of the algorithms and
next steps um
in the robotics area here but but using
the uh her new algorithms
and so when we want to deploy something
like that we need the hardware that can
take her algorithms
and you know so so that the robot can
execute them really quickly or
give feedback to the user quickly
so what i first wanted to do was just
give a little bit of my view on
the different stages of research
so
i define you know
there's many definitions but i define
engineering as sort of
the ability or the process of solving
complex problems
and so
in the daily work of a researcher where
you know building building things and
solving problems all the time
i define innovation as
applying the research
that
has been and
that's been occurring in academia
or or in other places
and
applying other people's research or
maybe in your own lab to solve
new engineering challenges that you have
so maybe for example
um
one of the processor designs uh is
slightly different from what you need
and so you'll
you're really interested in that
solution so you want to take that design
and maybe tweak it a little bit for your
um
for your use case
um but really
what i want to talk about today is the
research that we conduct in our lab
and you know research here at physical
computing and in u.s
and basically this is solving problems
in ways that
have never been done before
and so when you think about this
initially
you know how can we come up with this
new idea that no one's
no one's done before how's that even
possible but that's what we're doing
every day we're trying to think about
think about these things
and in fact
um
when you're re as a researcher you're
actually performing all of these tasks
um you're performing all these tasks
because
first as a researcher we're going to
build the latest state of the art
because we want to see
how that work is performing
then we potentially apply it to some new
problems
uh problems that we're interested in
but then the interesting part happens
we want to determine where the current
solutions
are unable to solve
your new problem
um where where are they lacking where is
there not enough efficiency um where
does it just not solve the problem
properly
and so now
our role as researchers is really to
work to find these novel solutions to
the problem at hand
so
you know coming up with a brand new way
um to do something that no one's really
thought of before or thought of in that
way
is really what we're working to do here
okay so now let's
switch switch
modes here and talk a little bit about
what we do in our in our group
um so what we're working on
is you can think of it as
trying to build
systems
that mimic a beehive for example we want
to build efficient
high performance
distributed and autonomous
systems
uh and you can see a beehive here as
being
you know many having many of these
qualities
and the other thing that we've noticed
in our in our work is that we're seeing
sensors and cameras being distributed
throughout all of the different layers
of you know society really today from
you know custom hardware and servers in
dedicated locations to maybe putting
um programmable
processors in lamp posts
um to help get uh your job done quicker
maybe definitely we see it in phones and
other devices um
in in uh around around everywhere
um one of the things that
we're trying to do is
think about how can we make our edge
devices also known as internet of things
or iot devices maybe these processors
in our systems how can we make them run
extremely fast
and provide these new capabilities
um like these new robots that
were talked about last
uh last presentation so how do we how do
we accomplish that
um and this is a
you know an example
of the kinds of edge devices or iot
devices that we want to be able to build
right we want to
uh in this case it's a swan this isn't
from our group but this is from uh ece
and nus
and what they were able to do was you
know have this autonomous platform
that would go and um in the uh in the
water and it would you know it could um
use its sensors to see the status
of of the of the water and basically
what's happening is now you can think of
this as one of the you know one of those
devices and it's doing
computation there and it's communicating
with other devices and things like that
so this is the kind of thing that we
want to enable with our work
one of the issues with
computers today is that they get hot
extremely hot
and really what one of the things to
think about is if we can
make
computing
more energy efficient
then maybe
we can
bridge the gap to a more sustainable
more sustainable solutions
data centers themselves is one of this
part of something called the iot
hierarchy basically of all of these
different components in the systems uh
all of these different components uh in
our in our environment today
uh and you have maybe your cameras and
sensors and your gateway devices and
data center servers but each each one of
these things has different performance
which is these mips numbers on the left
but also has different power output
and you need to be able to
um
when you're when you're doing compute
for example on your your phone
you know you want to make sure that your
phone's not getting too hot
and if it does if you don't you don't
want to break your phone right so uh we
want to make sure that the processors
are
able to provide a great great
capabilities but also be able to
um
you know do that within reasonable
reason
constraints
so now i'm going to talk a little bit
more about computer architecture some
computer architecture terms
um
what's happening today it's really we're
really at an interesting time
where
there are these rules there are these
laws uh or guidelines
one's called denard scaling on the left
one on the right uh is moore's law maybe
people might have heard of these two
sort of guidelines or laws or rules
but what's happening is denard scaling
over the years
have led us
to
[Music]
do more and more and more with our
machines with our laptops with our
phones
and we could you know activate more of
the transistors and do more all the same
time and this led to great performance
improvements
at the same time
we had the shrinking of transistors
which also helped us to pack more in the
same space and this actually we could do
that at a lower cost so now all of a
sudden we had more transistors smaller
um they don't get as hot and we have a
really low cost so that's great
everything's great there
um but it turns out that these
rules or these guidelines have really
not
um
you know at this point they're
not the same as they used to be we're
not getting
more affordable
transistors we're not getting we're not
no longer able to turn on all those
transistors and do all those things that
we want to do with them
uh and so what you can see here in the
lower right hand corner
um you know the numbers aren't as
important as the trend here so over the
years
we would have
more and more
cost reductions occurring with different
technology advances
but now
using the latest technology nodes
actually cost more than before
so this is a challenge uh for computer
architects how do we build new machines
that do more with less
and on the left it's a slightly
complicated diagram but the point is the
red
arrows are showing
that things have really dropped off
uh performance which is the blue
set of dots
and power which is the red triangles
they've sort of leveled off for our
common devices
so what does that mean how are we
supposed to continue to
improve
uh and do more with what we have
and so one of the examples of that is to
think differently and to think about you
know uh
the systems in a new way and i'll
explain a little bit about how we did
that in our research but we we developed
this new processor called core called
the load slice core
and what this allowed us to do was
improve
um
this metric called performance per watt
per dollar but basically what this says
is
you can
increase performance and do it in an
energy efficient and power efficient way
so by thinking about things in new ways
we're hoping that we can overcome some
of these limitations with transistor
costs going on
okay so i just wanted to give a little
bit of an overview
of
the
space at which we work in
and so there's always this foundation
um
that computer architects or any field
for that matter work with
and
in our work we are simulating
the processors of tomorrow but we
simulate them today on today's machines
and to do that we use tools like our
sniper simulator or other
models and analytical models
and then what we can do is use those
tools to build secure and efficient
processors
um and then put those processors and
accelerators together into platforms
which can then run your application
and so in this talk we're going to focus
on
a few of the different
types of
ideas in
improving the efficiency of your
processors
okay
so
one of the things that was really
interesting to me is when i started it
in us
i
was looking around we're bringing in phd
students to help on our projects and
also working with undergraduates and
math students
and
the project i'm about to talk about is a
um
ai accelerator
but the idea at least the beginning of
the idea was started and developed
by two undergraduate students here at
nus
and now we've taken this idea and we've
expanded it uh uh and and it's grown a
bit to
uh encompass uh
uh and and now we're working with number
of phd students and postdocs on this
project
um but the idea here is that
you know these ideas
it's really about your motivation your
interest in the topics
and these students were extremely
interested in and they they really
wanted to solve some interesting
problems and i said well let's see if we
can do this and this is the result of
their work
so if you look at
hardware accelerators something on the
left here is called the tpu
which is a google
ai accelerator
and then you look at the amount of power
it dissipates at 75 watts
now on the right you have a human brain
which displays 20 watts now the
capabilities of the tpu on the left
right are not
as as good as the capabilities of the
human brain right we're trying to get
there with new ai techniques but the the
point is that we're not there yet right
we do not we're unable to achieve all
the capabilities what we can do with the
human brain um but we still take a lot
more power to do that
and so one of the questions is can we
try to
solve
some of these open problems in ai
and with a much more efficient platform
um for those who are familiar with ai or
machine learning
um you might have seen something called
dense uh deep neural networks or dnns
and so on the left we have your
traditional
uh a ns which form
uh your your dnn components
now what we just we were looking at in
this work is something called
neuromorphic computing also known as
spiking neural networks on the right
and what we found is that it's possible
to
improve efficiency
by looking um at
how the system runs
at run time when you're using the the
hardware so what this means is
you might be
deploying a system
and typically you might want to when you
compute your
deep neural network answer
it might do lots of computations and the
point here is if you looked at the
network in a new way
maybe we can do many fewer
uh many fewer
[Music]
operations which means more efficient
result
and if we do that then we can
improve efficiency potentially improve
performance
i'm not going to go into details of what
spiking neural networks are and the
different types but what i do want to
say is that
um if you look at this graph on the
right uh you can see far on the right
there's this a n
marker
and it's a black star on the lower right
hand corner
um and so what's happening here is you
can see the x-axis
in this case is mega ops which is
millions of operations per second but
let's just call this compute power
and so if you look at this black star
for the a n you can see the number the
amount of compute you have to do is just
huge
now in the lower left hand corner what
you see with the uh this box here on the
left the green and the blue and the red
are some neuromorphic techniques
that we looked at implementing and we
were able to implement with our
accelerator and they do a significantly
lower number of operations
but they're still able to have a fairly
good accuracy result and so but you know
you look at this and you say
maybe we can maybe we can take advantage
of this uh by building some hardware to
do that and so that's what we did we
built an accelerator an ai accelerator
called yoso you only spike once and that
has to do with the
neuromorphic technique that we are
accelerating
uh it consists of a number of pes or
processing elements and each processing
element has some memory and processing
core
and inside the core um you access memory
and you do the compute and then when you
when you're done you store the result
back
um
there's a couple of different things
that we were enhancing i'm not going to
go into details there but what what we
were really surprised or at least really
happy about was that
given
the accuracy we're able to show the
energy consumption of our
um
the power consumption of our hardware is
extremely low
and so
you know when we what we did was we we
saw
the
potential for innovation
and then we thought to ourselves can we
solve this problem can we
invent something new
that allows us to take advantage of this
and that's when we designed a new piece
of hardware and we're able to
um
build build this new accelerator that
can
do a better job with respect to energy
efficiency computer related work
um so i just want to switch gears a
little bit here so i started with one
topic which was ai accelerators
but processors are also extremely
important for our group um the reason is
because cpus
form the basis of just about all
computers that we use today
we have a little schematic here of a cpu
and the kinds of components that it's
connected to the tram
network
and storage which maybe is disk or ssds
uh typically our cpu is thought of as
having large compute but
tiny access
to dram and networking storage
and so what we've been doing over the
years is we've been growing the cpu do
more let's do more
let's add more caches and add more
techniques that
in the end turned out to be
very energy inefficient
and so what we wanted to do was we
wanted to say well maybe we can look at
this in a new way
and so this comes down to the research
that we were doing we consult ourselves
well how could we potentially solve this
problem
there's something called memory level
parallelism
and up to the point uh
before we did this work memory level
parallelism was
uh investigated but people really didn't
build processors that targeted
memory level parallelism
which and i'll explain what that means
so memory level parallelism is the
ability
to talk to dram for example
and instead of just requesting one
piece of data at a time
maybe you can request four
or
20
or 100 items at a time
uh so the point is that
if we can
the time it takes to access memory from
dram is extremely long but if we can
look at it and as a different problem
and we can say well maybe we can do a
lot of access in parallel
maybe what this allows us to do is build
a processor that in the end will be much
more efficient
and so
um
what we've been doing is we've been
trying to
take these in-order cores and make them
higher performance and take out of order
cores and and bring their efficiency and
increase their efficiency um
and that's what these two green arrows
are showing basically can we can we
improve the efficiency and performance
of these processors
um and we've we've been able to do that
so one one of our processors the load
slice core which i mentioned before
was able to improve the energy
efficiency and performance of an order
design
and then we've also been looking at out
of order commit processors and high
performance mlp processors
but the idea
started from
its memory level parallelism idea
and then we started looking at different
areas and
you know connected them and said well
how can we
you know
how can we
exploit this mlp to
do a better job
and that's what led us to these works
um i think in the interest of time i'm
going to skip
some of these details but suffice is to
say we built a learning mechanism inside
the core to identify the key
instructions and route them to the
proper locations and by doing that we're
able to improve
the efficiency of the system not to skip
this
these slides
uh for now but uh in the end um this new
processor uh the performance was
improved by 53 and we were within 25 of
the performance of an out of order core
which is
a much faster but much more inefficient
processor
okay and the last topic just one last
slide here um i wanted to talk a little
bit about security so i mentioned
security in the title
and i wanted to mention a new a brand
new publication that are recently
released on archive
um which is a sort of an open area where
we release new ideas that haven't
formally been published yet
and the idea that we looked at with this
security uh research was
in the past
there have been
these very popular security
well-known security exploits called
spectre and meltdown
and those use something in the processor
called the branch predictor
in order to leak as as one component to
leak your private information so maybe
your private key
that keeps your computer secure
so what we found was that by
instead of the branch predictor we could
use other structures
in the processor
that most people
most processors have but aren't
protected
and so what we can do and you can see
this with the
um
green the blue and green uh dots on this
diagram but basically we're able to tell
you know from uh we're able to basically
leak the data
from one process to another using the
hardware
where uh in the past
um
you know people when they're running
applications they wouldn't expect their
private data to be leaked and so this
was a really interesting uh result
and um
you know this is an example of looking
at things in a new way to try to see
can we um
in this case it's can we try to
find issues
with current hardware current processors
um and then
potentially provide solutions so that we
make
our processors and our
hardware more secure
okay so with that um
i'll be glad to take any questions
thank you professor carlson um
now the floor is open for questions you
can either
uh type your questions on q and a or you
can raise your hands and we will mute
you to ask questions
maybe i should i will start with a very
um
simple question
so you were saying that
um
nowadays we are
we are nearly at our limits and
i hear that uh you are able to kind of
use whatever you you um
