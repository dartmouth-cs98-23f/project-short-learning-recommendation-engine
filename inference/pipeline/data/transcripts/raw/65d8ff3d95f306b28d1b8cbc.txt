this episode is brought to you by
brilliant
since the dawn of computing a single
construct has served as the focal point
of this transformational technology the
algorithm
by definition an algorithm is simply a
finite sequence of instructions
structured to solve a problem
in modern digital computers these
instructions resolve down to the
manipulation of information represented
by distinct binary states
these bits may be abstractly represented
by various physical phenomena such as
mechanical optical magnetic or electric
methods and the process by which the
binary information is manipulated is
also similarly versatile with
semiconductors being the most prolific
medium of these machines
fundamentally a binary computer moves
individual bits of data through a
handful of logic gate types these gates
have clearly defined binary outputs in
response to binary inputs and by
combining layers of these gates into
massive networks a deterministic
processing machine can be created a cpu
for example is a complex manifestation
of just such a machine
the flow and manipulation of digital
information is abstract by nature the
underlying physical medium only
determines the packaging performance
cost and practicality of its use but not
its inherent behavior
in digital computing binary information
moves through a processing machine in
discrete steps of time
the duration of these steps are limited
by the speed at which logic states can
be physically changed as information
propagates throughout the underlying
logic gate network inherently making
time a computing resource
in addition the capacity of a processing
machine to temporarily store and
manipulate information within itself
permits even more elaborate forms of
processing to be done
this in effect makes memory another
fundamental computing resource
because any series of instructions
tasked to solve a problem are confined
to the bounds of computing resources
algorithms designed for digital
computers can be classified by their use
of these resources as they scale this is
known as an algorithm's complexity
the most time efficient algorithms are
categorized as constant time
constant time algorithms always take the
same amount of time to execute
regardless of the size of the input
an example of such an algorithm would be
one that determines if a number is even
or odd to accomplish this only the last
digit of a number needs to be examined
no matter how large it is
because only one number is always
examined for any input size the solution
is always produced in one fixed duration
of time
algorithms can also scale in compute
time at a constant rate these are known
as linear time algorithms and they
execute at a rate that is directly
correlated to the size of the
algorithm's input
this characteristic becomes obvious with
a basic addition algorithm
the sequence starts with the rightmost
digit pairs being added from here the
algorithm then moves left to the next
pair of digits adding any carryover and
the process is repeated until no digits
are left
because the number of steps and
inherently the execution time is
directly determined by the size of the
number inputs
the algorithm scales linearly in time
constant and linear time algorithms
generally scale to practical execution
times in common use cases
however
one category of algorithm in particular
suffers from the characteristic of
quickly becoming impractical as it grows
let's look at an algorithm that must be
designed to search for a four-digit
numeric code this algorithm calls upon
another constant time algorithm that
will validate if the numeric code is
correct signaling the end of the search
with no prior information to go on the
search algorithm must cycle through
every single four-digit permutation
until a valid code is found
this is known as the brute force method
and for a four-digit code up to 10 000
codes must be tested by the validation
call with the code being found on
average by the 5 000th try
while this isn't particularly noteworthy
for modern processors a problem soon
arises as this brute force algorithm is
forced to deal with larger codes
with a five digit code one hundred
thousand combinations now possible at
six it becomes one million
with each digit added to the code the
number of possible combinations
increases tenfold from the previous
number of combinations
since testing each combination requires
a constant period of time to execute the
overall time to execute the brute force
algorithm rises exponentially as digits
are added to the code
this is known as an exponential time
algorithm and they pose a huge problem
for traditional computers as the
execution time can quickly grow to an
impractical level as input size
increases
problems that exhibit this
characteristic are known as intractable
they can in theory be solved but require
far too much time or memory to
practically arrive at a solution with
current computing technology
while intractable problems can be viewed
as a computational hindrance or a
limitation
their nature also lends themselves well
for security purposes such as in the
case of asymmetric encryption
the rsa algorithm for example forms the
basis for much of the encryption we use
including the encryption mechanism that
secures the internet
at its core rsa secured by the fact that
factorizing a large number to find its
two large prime number factors is an
intractable problem
this problem is so difficult for modern
computers that in 2020 it took a
supercomputer the equivalent of almost
2500 years per cpu core to find the
prime factors for a 250 digit number
at present to threaten the most commonly
used rsa encryption the ability to
factorize a 617-digit number is needed
it's estimated that at present it would
take almost 3 trillion years per cpu
core to accomplish this
in the early 20th century physicists
began to develop new ways to describe
the seemingly unintuitive behavior of
the most elemental interactions in
physics
called quantum mechanics these concepts
revealed an entirely new perspective on
how reality functions at the most
fundamental levels of observation
by 1980 physicist paul benioff would go
on to propose applying the principles of
quantum mechanics to computation
physicist richard feynman would soon
suggest that such a computer had the
potential to simulate things a classical
computer could not feasibly do and by
1986 he would propose the earliest
concepts for quantum computational
circuits
from these initial conjectures computer
scientists soon discovered that in
theory they could exploit this newly
discovered offshoot of quantum mechanics
to create more efficient algorithms
problems such as factorization that were
believed to be intractable for
conventional computers now had the
potential to be completely manageable
with a quantum computer
much like how digital systems use bits
to express their fundamental unit of
information quantum computers use an
analog called a qubit
however a qubit behaves dramatically
different than a traditional digital bit
in traditional bits only two discrete
states exist one and zero
these states are generally defined by
two clearly distinguishable physical
properties in the case of semiconductors
voltage levels are used
when a bit signal is applied to a
digital logic gate typically some form
of active switching occurs where an
input state triggers either an
activation or deactivation of one or
more internal switches to produce a
clearly defined output state
a key tenet of digital logic is that the
information is always clearly defined
measurable and their logic gates are
deterministic
quantum computing by contrast is
probabilistic
while qubits can be measured as two
discrete states like digital logic they
exist in a natural state known as a
coherent superposition where no clear
state can be inherently assigned to it
unlike digital logic the act of
measuring a qubit destroys its coherence
irrevocably collapsing its superposition
state to a classically digital one
a key difference in the value of this
measured quantum state when compared to
a classical digital bit is that it is
not deterministic but rather based on
probability
it is the manipulation of these
probabilities as they move between
qubits that form the basis for quantum
computing
qubits are physically represented by
quantum phenomena such as the spin up
spin down levels of an electron or the
vertical horizontal polarization of a
single photon and they operate entirely
by their natural quantum mechanical
interactions
how the probabilities of qubit states
are modified when interacted with comes
down to the quantum principle of phase
a cubit possesses an inherent phase
component and with this characteristic
of a wave a qubit's phase can interfere
either constructively or destructively
to modify its probability magnitudes
within an interaction
a classical analogy for the use of
interference to produce desired
information can be found in noise
cancelling headphones
these devices create an inverse
electrical signal from a microphone
sampling ambient sounds and mixes it
with the intended audio signal
the interference interaction of the two
signals results in a signal that mostly
cancels out ambient sounds as it
converts back into a pressure wave and
enters the ears
in effect a quantum program is a group
of qubits configured with probability
modifying elements in between to form a
large interference network this
networking concept when tuned should
resolve to a high probability of a
solution to the problem for which the
network was created for
it's important to note that the
mechanisms of these interactions are
never observable and are only defined by
mathematical models as attempting to
directly measure them would cause the
network to decohere into a single state
measurement can only occur at strategic
points in the network where the answer
lies within the collapsed state in
addition because of the probabilistic
nature of the process and its inherent
susceptibility to noise the final
measurement must be made at minimum
thousands of times in order to build a
statistical profile of the likely
outcome of the network
qubits possess no intuitive analog to
our classical world experience their
characteristics and behavior like much
of quantum mechanics are modeled using
complex numbers
however most of these properties can be
visually represented using a geometric
construct known as a block sphere a
block sphere visualizes a qubit's
magnitude and phase using a vector
within a sphere
in this representation the two classical
bit states are located at the top and
bottom poles where the probabilities
become certainty while the remaining
surface represents probabilistic quantum
states with the equator being a pure
qubit state where either classical bit
state is possible the phase component is
also represented by the hemispheres at
each side of the x-axis of the block
sphere
when a measurement is made on a qubit it
decoheres to one of the polar definitive
state levels based on its probability
magnitude collapsing any positions of
uncertainty in the process
much like how classical digital bits
have no computational value without
digital logic gates quantum computers
employ quantum logic gates on qubits to
operate
quantum gates are the fundamental blocks
of how information is manipulated within
a quantum circuit however unlike their
digital counterparts a quantum gate
modifies a qubit's probability magnitude
and its phase component not any direct
values
quantum gates are categorized by their
pattern of operation and the number of
qubits they operate on
among them polygates are the most
intuitive
polygates come in three variations
probably x y and z the x and y gate in
particular behave exactly like a digital
not gate
operating on a single qubit they invert
the value of a one or zero state
however on a bloch sphere their quantum
behavior is fully expressed
polygates rotate the vector that
represents a qubit's probability
magnitude and phase 180 degrees around
the respective x y and z axis of its
block sphere for the x and y gate this
effectively inverts the probability
magnitude of the qubit while the z gate
only inverts its phase component it's
important to note that phase changes
caused by polygate operations determine
how the qubit will interact with certain
types of other gates
some quantum gates have no classic
digital equivalent
the hadamard gate or h-gate is one of
the most important urinary quantum gates
and it exhibits this quantum uniqueness
a hadamard gate in effect can move a
binary state level in and out of
superposition
take a qubit at state level 1 for
example applying a hadamard gate to it
results in a superposition state if a
second hadamard gate is applied directly
after the superposition is returned back
to its previous state of one the
hadamard gate memory of the qubit's
previous state is derived from the
qubit's phase component
if a measurement is made between two
hadamard gates the collapsing of the
first hadamard superposition would
destroy this information making the
second hadamard gate effect only
applicable to the collapsed state of the
measurement
when visualized on the block sphere a
hadamard gate simply mirrors a qubit's
vector through a diagonal slice
through the bloch sphere that is defined
by z equals x
from this the function of superposition
reversibility through phase becomes
clearer
in addition to the pauli gates and the
hadamard gate two other fundamental
gates known as the s-gate and t-gates
are common to most quantum computing
models the s t gates are known as phase
gates as they only affect the phase
component of a qubit at 90 degrees and
45 degrees respectively
from these six fundamental urinary gates
composite gates can be created that
allow for both rotation or phase
shifting at arbitrary angles
these are called the rx ry and rz gate
for rotation and the r5 phase shift gate
it should be noted that both the rz and
r5 both rotate phase but rfi only
operates on the one state level region
of a qubit much like their digital
counterpart quantum computing requires
gates that operate on multiple bits in
order to accomplish useful computation
however in contrast to a classical
digital logic gate a multi-cubic quantum
operation is done completely without
measurement
this nuanced difference results in the
involved qubits losing their
distinctiveness in the operation
they become entangled
entanglement is a strange and unique
characteristic of quantum mechanics and
alongside superposition they form the
two key tenets of quantum computing when
two qubits become entangled they no
longer have individual independent
states they become two components of one
composite irreducible superposition and
one cannot be described without the
other
entanglement is accomplished using a
gate type known as a control gate
control gates trigger a correlation
change to a target qubit when a state
condition of the control qubit is met
these gates introduce conditional logic
into quantum computing but when combined
with the properties of entanglement the
true power of quantum computing becomes
apparent
the most important of these gates is
known as the control not gate or c
naught gate a c naught gate causes a
state flip of the target qubit much like
a digital not gate when the control
qubit is at a state level of 1.
in addition an entanglement is also
created between both qubits creating a
persistent correlation
the implication of this is that if both
qubits are measured the measurement will
always produce a correlated result
the properties of entanglement can be
easily observed when a hadamard gate is
applied to a qubit and it is connected
to a second qubit via a c-not gate
because the control qubit is placed in a
superposition by the hadamard gate the
correlation created by entanglement
through the c naught gate also places
the target qubit into a superposition
this is known as a bell state and it
makes either single qubits state
indeterminate when the control or target
qubit state is collapsed by measurement
the other qubit's state is always
guaranteed to be correlated by the c
naught operation
if both qubits are measured both
measurements will always collapse into a
correlated result
c naught gates are used to create other
composite control gates such as the cc
not gate or tafaligate which requires
two control qubits at a one state to
invert the target qubit
the swap gate which swaps two qubit
states
and the cz gate which performs a phase
flip similar to a r5 gate when the
control qubit is at a one state
while far more complex composite control
gates can be created these are the
simplest and most common
the quantum entanglement at the core of
these gates has a curious property that
imparts tremendous computational power
to quantum computing they transmit their
correlations faster than the speed of
light
in fact the most recent experiments have
placed a minimum speed for entanglement
at around 3 trillion meters per second
or about 10 million times faster than
light
in theory massive quantities of qubits
can be entangled together and interacted
with for the purposes of computation
with no negligible increase in
processing time
when combined with the fact that a qubit
is continuous by nature and has infinite
states this quickly scales up to a
magnitude of information processing that
rapidly surpasses traditional computing
for comparison using a modern computer
even just coarsely simulating a 20 qubit
operation can take up to 20 seconds with
the compute time increasing
exponentially with each additional qubit
added
on real qubits the same operation
happens at the scale of nanoseconds
in part 2 we'll look at the nature of
designing quantum algorithms their
application and the challenges
associated with creating them we'll also
explore the physical limitations of
quantum computing and how its current
state of development aligns with
theoretical expectations
for this video i've purposely chosen
conceptualization over math to introduce
this incredible new frontier of
computing however quantum computing does
require an understanding of the
mathematically abstract to fully
appreciate its potential and with
brilliant building a solid mathematical
foundation to fully grasp these concepts
has never been easier brilliant is my
go-to tool for diving head first into
learning a new concept it's a website
and app built off the principle of
active problem solving because to truly
learn something it takes more than just
watching it you have to experience it
with this in mind brilliant has been
tirelessly revamping their courses to
introduce even more interactivity and
with their recently updated introduction
to probability course you'll be able to
examine and explore the power of
quantifying uncertainty in this course
you'll bypass the calculus and use a
simulation system that lets you apply
the rules of probability as you learn
them without being bogged down by
difficult calculations all while
developing skills aimed at modeling real
world scenarios with brilliant you learn
in depth and at your own pace it's not
about memorizing or regurgitating facts
you simply pick a course you're
interested in and get started if you
feel stuck or made a mistake an
explanation is always available to help
you through the learning process if
you'd like to try out brilliant and
start learning stem for free click the
link in the description below or visit
brilliant.org forward slash new mind and
the first 200 of you will get 20 off an
annual premium subscription
you
