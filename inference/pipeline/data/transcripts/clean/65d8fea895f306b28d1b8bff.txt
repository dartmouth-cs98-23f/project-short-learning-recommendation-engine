for your games? For most games you dont need   to think too much, but if you want some cool  lighting effects or to make a nice looking   3D game or some post-processing effects or  stylized art you would need, at some point,   learn the rendering pipelines. I cant say its  easy, but who said it would be? The whole game   development process is very complicated, but  this video will try to explain rendering in   the most basic words possible and will gradually  dig in deeper, so hopefully this knowledge will   let you understand not only specific aspects of  the chosen game engine, but the whole process   used similarly in most of the engines and  frameworks out there. So lets overcome the   fear of getting into rendering by diving into the  heavily inspired by OpenGL Rendering Pipeline. Render Pipeline is a whole process of making your  visual components visible on a screen, but dont   worry! Modern frameworks and game engines simplify  the process. For example Defold takes care of   a tremendous amount of stuff that is needed for  each pixel to be displayed on different devices.   You can interact with rendering on some very top  layer, but also have the possibility to dig in   to achieve really cool things. Render Pipeline  answers 3 crucial questions: What to render?   Where to render? and When to render? By answering  them one by one you will see the whole picture.   In Defold What to render is defined by Render  Predicate - a kind of group of visual components.   Where to render is defined by the view of the  games world through some kind of projection,   like you would see the word through a camera, but  on a flat screen. And finally When to render is   controlled by the Render Script. So what you  can render? Simply all visual components, but   additionally, in Defold, you can group them. You  can use the mentioned Render Predicates for this.   Those are kind of filters allowing you to control  what to render on screen. Visual components like   for example Sprites, Tilemaps and 3D models are  all rendered on screen separately, GUI nodes are   drawn separately on screen or even some debugging  lines and texts. You can create more such   predicates to control what you are drawing, for  example you can disable drawing GUI at all, you   can enable drawing lights or 3D models separately.  Such grouping give you more control over where   and when you will be rendering different visual  components. So imagine Render Predicates as just   names, tags, filters in materials for each visual  component. Material? Yes, each visual component   has a material assigned to it. Material is a  nice abstraction defining how to render a given   component and thus answering part of the question  Where to render. How? Because it defines special   programs also called shaders that define how given  geometry will be rendered. So where you would   see components looking at the games world through  camera seeing some kind of view and projecting   this view on your flat screen and then after  rasterization, which I will explain later on,   where to draw each pixel on screen. There are a  lot of concepts in a few statements, so they all   need to be explained, but before this, well focus  more on the last question: When to render?. This   is defined in Defold in Render Script - the heart  of the Render Pipeline, which describes the whole   process of producing frames on screen. This is  the one and only script in Defold that defines   the whole rendering process. Take a look at your  game.project - 2 most important things are in   one tab: the main collection that will be loaded  at start of your game and the special render file,   which contains simply 2 informations: the Render  Script and the list of materials that could be   used in it. Thats all! Before we analyze the  Render Script lets imagine how we produce pixels   on our screens. Imagine each visual component  in your game as a bunch of triangles. Even for   2D Sprites those could be drawn on two triangles  forming a rectangle. Its easy to imagine when   looking at polygons in 3D models or a synthwave  retro graphics. Theres much mathematics involved   behind it, because the graphics are calculated in  a few steps. Each triangle consists of three lines   connecting three points. Those points are called  vertices. Each vertex is a point, a coordinate   in 3D space, so one of the first steps in graphics  pipelines are regarding operating on those   coordinates, those vertices. So the program making  calculations on those is called a Vertex Program   or a Vertex Shader. Graphics cards are powerful  in such calculations and dont even sweat making   complex matrix operations on millions of points.  After all those overwhelming calculations we   have a nicely defined 3D geometry. There are few  important, but more complicated steps in between,   but at first, imagine you only get those triangles  somehow and everything is very simple, and you   get all this data projected into a flat screen to  finally get you to the point of Rasterization. And   this - is simply a slicing of all this geometry  into small pieces, very well known as Pixels.   For each of those pixels at the end we need to  assign a color, so each diode on your device   could be lit according to this and as whole form a  frame. A color of each pixel is described as a mix   of a given amount of Red, Green and Blue colors  and additional transparency value. Graphics cards   perform calculations for each pixel - a tiny  fragment on your screen, utilizing a Fragment   Program, also called Fragment Shader. At the  end some additional work could be performed by   the graphics engine which goes through some tests  and blending stages. Are you with me? Could we go   deeper into how those 3D triangles are actually  projected on a flat 2D screen? There are a bunch   of steps to transform each point in 3D into a  point on the 2D screen, so we would go through   them and name the steps and the coordinate  systems that are produced after each of them.   First we have a primitive, example geometry,  lets say a 3D cube, which could be described   with 8 points in 3D space. When you make such a  cube in, for example Blender, it will be created   in its local coordinate space and all points will  be described in such a coordinate system. So each   visual component in 3D games has their own Local  Space, but you want all those objects to be put   into our game world, so they need to be included  in some common global coordinate system - the   Word Space. You have a Model Matrix in-between  that converts local points on our 3D cube and   each other model to the world coordinate system.  We wont analyze the mathematics behind it, but   you might note that multiplying data by special  matrices allows us to convert translations and   rotations to different coordinate systems and this  is what this Model Matrix is for. So when you have   all the data in one World Space you then watch  the world through the lenses of the camera or the   players virtual eyes. What you see is depending  on where the camera is and at what it is pointing,   right? So its the view of the camera and so  such a view has its own coordinate system,   whose origin is commonly located at the middle of  the view. To convert the World Space coordinates   to View Space you perform a calculation with View  Matrix. So the View Matrix is really describing   your in-game camera. For example, in First Person  Shooters you can feel how directly you rotate the   camera around. And in 2D games you usually cant  rotate the camera, but can move it around showing   different pieces of the world. But defining only  a view is not enough. We cant see the games   camera in three dimensions, because we need to  somehow project this view to flat screens. We do   this using a special Matrix called Projection  Matrix and this step converts View Space to   a so-called Clip Space. It is a projected view  of what the camera sees and its called Clip,   because all the coordinates that are outside  such a projected view area are clipped and   not shown on screen. The remaining coordinates  will end up as Fragments visible on screen and   whats funny - when some triangles on the edge  of view are cut in half because of such clipping,   the graphics engine automatically creates new  triangles on the edges to actually fit inside   and fill the whole screen. Finally such a clipped  view is then very quickly converted into another   convenient space, which is a Screen Space with  its origin commonly in a corner of the screen. And   this is done with the use of a Viewport Matrix.  You dont need to actually always use the whole   screen making fullscreen games, but you can draw  everything in a window that could have different   dimensions - and this is exactly what the Viewport  is for. You define the dimensions of a rectangular   space, on which you will be drawing all your  pixels. OK! Another level of diving in! We   barely mentioned how the pipeline projects the 3D  View to a flat screen, but you might be aware that   there are different kinds of projections. Each  projection could be defined by a special 3D shape:   a sliced lump or most simple cuboid or even  a cube - called professionally Frustum.   It has two important sides, called Near and Far  planes that are actually parallel to the screen   and the rest of the sides just connect them.  Especially, very simple kind of projection,   such that Near and Far planes have the same area  is called an Orthographic Projection. Its name is,   because the lines describing the view  then are orthogonal to the view plane,   so the Frustum is a simple Cuboid. So  effectively you dont scale 3D objects to screen,   like you think the objects far away from you are  perceptibly smaller than the ones closer to you,   but see them in a way that imaginably are far,  far, infinitely far away from you (but you have an   eagle sight!) Orthographic Projection is commonly  used in 2D games, because its simple, very clear   and because you have flat sprites that dont look  good in perspective projections. Yeah, I mentioned   it without explaining, but very easily you imagine  that Perspective Projection is an opposite. Its   an effect, where you see objects far away from you  actually smaller than the ones close to you. Its   very useful for 3D games, because it really  is corresponding to our actual perception,   to how our eye see. In such projections the lines  connecting the Near and Far planes are actually   meeting at one point in space and an angle between  them is called Field of View (FOV). This angle,   for realistic projection like our eyes work, is  usually around 45 degrees, but its maths and   nothing stops you from making Field of View of 60  degrees even, like in Doom games! It will be kind   of deformed, but you will let players see more on  a flat screen. All those coordinate systems are,   by OpenGL convention, right-handed, because,  well, take your right hand and look at its inner   side with all fingers pointing up, in the positive  direction of the Y-axis. Let your thumb point to   the right, to the positive direction of the X-axis  and bend some of your fingers to the front of you,   to point in the positive direction of the Z-axis.  You might, at some point, take advantage of such   knowledge, so thank me later! That really  is a lot of information for one video,   so for even more thorough explanations I invite  you to check out Learn OpenGL website, where Joey   explained in the most affordable and detailed way  how OpenGL works. And I mentioned OpenGL a few   times already without saying what it is, because  its not so simple. Its not a language, its   not an implementation of rendering - its just a  Specification, a Standard describing how functions   should communicate with graphic cards to produce  desired results. Its generally the graphics card   manufacturers that implement drivers for this.  And game engines are usually an abstraction of   using raw OpenGL functions, more thin or less, but  in the end, they simplify this. Defold is running   on OpenGL and therefore its render pipeline  is heavily influenced by OpenGL workflow,   but thanks to offering such an abstraction layer  over OpenGL - the underlying renderer could be   replaced and for example many engines, including  Defold, also offers support for Vulkan and WebGL   render pipelines. Such possibility is crucial to  support different devices, because, for example,   Apple devices use Metal, while HTML5 builds run  on WebGL. But you dont care about it - its a job   of Defolf! You build one rendering pipeline and  could release on many platforms simultaneously and   thats what makes a game engine multi-platform.  You will find a similar approach in most of the   engines. Community is also fiddling with renderers  and for example there is a possibility to make a   game in Defold, but with RayLib renderer. For  now, please digest the amount of information   provided here and in the next video we will dive  into the render script of Defold. And by the way,   if you want to help creating such videos - leaving  a traditional like, a comment and subscribing to   my channel with this ding dong ringing helps  me be encouraged to continue and make more   videos! And for even more support - I already run  Patreon, Ko-Fi and soon - GitHub Sponsors - sooo,   you know - my kind of pig bang for future is  there :) Have a nice day and see you soon! 