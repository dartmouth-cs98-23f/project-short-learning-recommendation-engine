weve talked a lot so far about the impact of research on society on the collective but of course the collective is really a collection of individuals this morning well take a look at the human side of technology at individual experiences we see a need for increased focus on creating technologies that can meet us where we are as individuals wherever that happens to be in terms of identity ability or societal context this brings us to the field of human computer interaction one of microsofts longest standing research areas which has led to things like connect smart assistance and many other innovative technologies its now my pleasure to introduce one of the pioneers in the field mary serwinski to share some of the latest work to make our interactions with technology more natural more intuitive and more empowering human computer interaction or hci is a critical part of the design of computing systems it has formerly been around as a field since the 1960s though of course the study of human factors predates that time period most technology companies that care about their users have hci researchers as part of their product design process so it made sense that microsoft research which pushes ahead of the product teams by several years would also incorporate hci researchers as new systems are invented and evaluated before they are released to the public i happen to be the first social scientist hired into microsoft research back in 1996 or so today my latest count shows over 70 phd scientists that identify as hci researchers worldwide in microsoft research hci deals with the design execution and assessment of computer systems and related phenomenon that are for human use the goal of the discipline is to make sure that the technologies we develop are useful usable safe and enjoyable to interact with for all people so that humans can achieve their tasks whether thats to get work done learn be creative communicate or just play in order to fulfill these goals tech companies must attempt to understand how people use technologies and build systems based on that knowledge design or invent efficient effective and safe interaction techniques for using new systems and put people first peoples needs capabilities and preferences should come before the technology the new technology invented should be designed to meet users needs and requirements first not the other way around why is this important a good user interface can improve financial outcomes because the experience can increase users loyalty trust and make users happy ensuring continued interaction with the system we know this first hand on my team because we study user emotion and we can see happiness go up when interactions with software allows user to get their work done more effectively for example weve seen evidence that users smile significantly more often when good search results come back quickly and users dont have to reformulate their search queries happy users come back to use software that pleases them most hci researchers come from a varied background of multi-disciplinary and training including computer science obviously psychology because it studies people and application of theories and methods sociology for understanding computer-mediated technology and organizations and even industrial design for the design of interactive products like cars laptops mobile phones etc as technology evolves ever more rapidly human cognition remains relatively stable its important now more than ever to ensure that humans can use and understand the design of new systems as they emerge especially with the advent of newly automated systems like self-driving cars robots and telehealth how do we as a society leverage hci to ensure that these systems are designed for human safety comfort and well-being to that end lets take a tour of some of the labs and researchers we have at msr microsoft research working in hci im super excited to give you a mini tour of hci in the redmond labs here in building 99 first were going to meet mar gonzalez franco mar is a principal researcher with the epic research group she wonders what spatial computing can tell us about human behavior and how we can expand digital content for immersive experiences beyond helping to create new ways of experiencing technology through virtual and augmented reality mars work explores how new devices and technologies impact our perception and behavior im margunthalet franco and this is the epic lab the standard perception interaction and cognition lab what we do here is we explore the digital content around us and we try to implement new sensory abilities to it so that means we studied very deeply perception and we studied the behaviors that come with the perception that we produce with the new devices so you might think perception and behavior are very far apart its a big leap but the truth is not and i think studying behavior with understanding perception is just missing part of it because perception how we perceive the world is going to change how we behave and the ecological way to explore how our prototypes are going to affect the lives of millions when they go out uh is to study both together were humans are visual animals right its been traditionally our method of displaying information for um digital content its been through screens and through bissell but we are multi-sensory animals and we have evolved to have multi-sensory experiences which are far more fulfilling for example audio is we have more precision with audio sometimes on a spatial because we our eyes only look forward and audio is you know all around us so we can hear things that are on our back but we cannot see things that are on our backs so its clearly something that we can enhance our experiences with devices if we think on a holistic way what are the different sensors that humans have so you might ask why is this important now right computers have been here for a while and i think theres been a change in paradigm in how we understand computing until now we had the screens in front of us digital content was inside more recently were seeing a lot of spatial computing devices that render the digital content around us and that thats changing a little bit how were gonna interact with the content and in particular its augmenting how we need the sensory abilities to match um and interact with the content if we see an object in front of us that we can grab we want to feel that experience right this is a sensory motor loop that needs to be fulfilled so i have some prototypes here uh that i can show you and i think they will explain a bit more what we mean by augmenting the sensory experience and perception so i want to start with this one i love this one this is called the keyboard and and you youll clearly see very easily how it works but um imagine well now its in the northern hemisphere its apple picking season right so imagine youre picking apples inside virtual reality or augmented reality and as you go for the apple the people comes to you and you just grab it and its such a fulfilling experience that once youve tried it it feels empty to go into virtual reality without haptic controllers so we experiment a lot with these type of things and we have devices that basically extrude and will change the normal as youre touching through a surface um and youll see there are many of them that are controllers that youll be wearing but were also trying to amend the desktop so this is a new type of fabrication based on aesthetic materials we just publish it in nature communication were able to reduce a shape display into nine actuators only by changing how we fabricate the materials on top of it right so you you will touch for example a shape and move around and be able to experience a whole shape and we have also these very cute ones which are mini robots that come just in time for whatever you want to touch so all of this is the nice set of prototypes but there is science behind it right there are things we found for example like the uncanny valley of haptics that you can go deep into this sensation that you break the illusion because the haptics dont match correctly uh what youre expecting we publish that in science robotics and i think its hard sometimes to detach theory from practice because if you dont learn the theory behind it you will not be able to replicate right so many days were inside but we also go outside right we want to see how these technologies go in the wild and we study beyond touch all sort of perception behind me you see a bit of a vestibulator which will you know we used to study vestibular system but we also study a lot of sound because once you are inside the content sound is spatial like in real life which is also a very big change in paradigm compared to using the screens and so in that area im working a lot with the team of soundscape microsoft soundscape they are enabling blind and low bcn people uh to go around the world and travel with a different form of gps which is based on beacons its a different form of navigation if you think of the human you know we were living in forests we would hear sounds travel towards a source of water we would hear and you know that that would create a mental map as we were going there so we could return to the origin and and more recently you know this idea of sound emitting systems has been used in modern society right uh churches minarets and mosques produce sound for people to know and locate where they are so were trying to do that but with digital content all around us i it would be great if the pharmacy made noise so i could go there and find where the pharmacy is so were trying to create that type of experience in particular i recommend reading this uh piece we put out in scientific american just a couple of months back so there are many of these subjects were thinking like apples that are static right but the truth is once you put this on you might be talking to an avatar for example so we work a lot on avatars and were trying to figure out how people behave with alberto lavataran theres so many possible applications that its even beyond what we can do for the particular case uh you know all these applications of avatars but for the particular case of ai i think we have a duty there right um were creating synthetic data currently people are creating training self-driving cars in environments that have no avatars so you you basically have no representation of humans there in many cases were just training you know can you stop a designer and we need to have the random behavior of humans also inside there to really have a good digital substitute of real life scenarios and its even more interesting because once you have humans in there you can trigger actions that would happen very rarely in real life so you can cover many more scenarios than if youre to record real footage so synthetic training i think its something that is of particular interest for ai and and the merge of this avatar and an ai content so we have been very strong at releasing libraries for everyone to use like the microsoft rocketbox avatar library so people around the world are also exploring these ideas of how you interact with this digital content and avatars in this case and we have people in the nih uh studying the psychological responses uh in stanford ucl you know universities around the world are using our avatars so there is you know all sort of applications and were trying to see where it goes uh also by providing means for everyone to do research in this area lets move on to hear from teddy sayed teddy is a senior researcher in the ryze group at microsoft research and also leads the future of wearables team he looks into the question of how technology allows us to express our identity well join teddy now to hear more about how smart fabrics and wearables are used to create unique new user interfaces to explore human computer interaction in new areas including fashion my name is teddy said and im a senior researcher at microsoft research im in the ryze group and i lead the future of wearables team and so what i work on is really about connected textiles intelligent textiles wearables and really anything in between that involves textiles in general so speaking of textiles you can think of textiles is you know stuff thats around you every day its in your clothing its on the couch you sit on its in your gym bag really its well into the fabric of everything and every thing we do every day so you can imagine being at home or working from home youre sitting on a couch youre using your laptop you know youre wearing clothing theres you know the couch all those things have fabric embedded in them and theyre all really woven around what we do today and then when we think of you know the textile experiences also the social experiences right theres the aspect of creativity and expression and the clothing that we wear the activities that we go to youd imagine being at a large concert or some really interesting event out in public but the whole point of this is a lot of it you know clothing and textiles is a part of what we do and its really woven into what we do every day so when i talk about smart textiles and intelligent textiles and connected experiences what im really meaning here is imagine theres some sort of intelligence some type of sensing some type of interesting interaction enabled by a textile um so when i think of this entire space of textiles that ive mentioned you know i like to think of it as an ocean right and theres this whole ocean and were moving across this ocean you know theres many companies microsoft apple google et cetera et cetera were all trying to move in this ocean but the problem here is that you know the vehicle which were moving on this ocean is really a boat but its a boat made of car parts were using really chunky hardware were using you know maybe not the most optimized software or hardware experiences its really chunky and so i like to think of the space as moving forward with a boat made of car parts but what were really looking for is moving this space forward with boat parts so we need stuff to be integrated into the fabric we need gestures to be integrated into the fabric we need the technology really to be woven into what we do in everyday life which i had mentioned earlier and this really ties it back to mark weisers vision of ubiquitous computing where you know computing is woven in into the fabric of everyday life so this is really what i mean about smart textiles experiences and we want to do is make the technology uh make that boat better so how are we going to do that as i mentioned before you know were in this ocean space and we want to do a lot of work that moves it forward we want technology to be integrated into the fabric we want it to be part of the textile so you can imagine how are we actually going to integrate sensing into fabric because you can imagine theres not going to be cameras available all the time if were sitting on a couch at home watching a sports game or a movie and our hands are dirty what are we going to do well here would be a nice use case of actually having textile sensing integrated and in forbitrio we looked at how do we actually integrate sensing into the fabric and we came up with a pretty clever way of using machine learning to detect different gestures whether its tap or click or even wave so traditional techniques that you see with camera we can actually do directly with fabric which i think is really really cool but moving beyond just sensing one of the challenges that we have with textiles is that textiles deform and theres a challenge of deformation and so what i mean by deformation is imagine you have a sensor directly on your shirt and if i scrunch it up this sensor is probably not going to work so how do we solve that this is what i mean by deformation so in this next part youre going to see here were actually trying to solve for deformation and so the first place we solved this for was the pocket because the pocket is one place where you can put objects in it deforms quite heavily and if you can solve for a pocket you can most likely solve for the shirt in your gym bag that you take out and put on so here were actually using some pretty cover again clever techniques with machine learning detect different objects that go into your pocket and not just objects but also uh gestures as well which is again one again going back to the to the areas of focus for hdi and last but not least i havent touched upon in the previous two projects the human side right if you think of this whole space as an ecosystem right now this ocean is really being pulled by technology companies which is something i am you know not really passionate about i am passionate about wearables and we are as well at microsoft but this space should be really led by designers by apparel brands by the people that actually understand and have history with apparels and textiles so the idea here with project brookdale is can we empower that community to have that experience can we create textiles can we create apis can create hardware software experiences that enable them to then design you know the experiences that we want in the future for myself i dont want to be creating the next wearable textile or the next wearable but i want them to be empowered by the work that we do at microsoft which is our goal here and in project brookdale we ended up going to new york and the reason why we went to new york is obviously new york is a global fashion hub and a lot of interesting creative designers live there and work there and so by going there we want to actually explore what is it like to empower you know a set of designers with tools hardware software to create interesting wearable experiences and by this i mean you know using tattoo sensors using motion sensors using lights all the things that kind of exist today in the space of wearables can we design tools for them to empower them the idea here being if we want the space to be led by that industry we need to work with them side by side hci we really focus on the human side as well as the computer side but when you think of you know the space of textiles and really the broad space in general its really about the people that are working in it and the impact that it has on the environment in the case of textiles you know theres a huge environment impact impact on the type of textiles we use the waste how its manufactured now youre adding an element of intelligence right now theres hardware theres software involved that does all the computation so really theres a whole ecosystem thats involved in this space and because were at such a point now where everything is really new we can actually define in design for sustainability in mind in the space of intelligent textiles so whether its the case of you know the factory worker whos trying to integrate you know smart textiles at the apparel level to the brand thats actually making it to the end customer as well so we can design for sustainability in mind which is one of the really unique uh really aspects about the space that were working in at microsoft research and so coming back to that analogy i described earlier with the boat parts and car parts you know what were trying to do today is really emphasize building out that better boat whether its better sensing through the fabrics sustainable fabrics that are also intelligent to designing better back-end systems that take into account the environmental impact its really all about the ecosystem in moving forward in this ocean and that is one of the things im really excited about here at microsoft research so obviously this insane vision we have here at microsoft research is not just myself but its a whole team of us at the future of wearables lab first off its led by evelina barhudaran who is a co-innovator as well as a leader of business development and strategy for myself and the team as well as becky gangnon and gabrielle demon who handle a lot of the engineering as well as industrial design as well i have some amazing academic collaborators taehyun wu whos a phd student at dartmouth college as well as a supervisor and a colleague of mine and close friend dr zhing dong yang also at dartmouth college i also wanted to give a quick shout out to the microsoft research hardware lab who does amazing amazing work here moving this vision forward so before i leave you today i kind of want to leave you with a nice quote from walter benjamin was a personal favorite of mine work on good pros has three steps a musical stage when it is composed an architronic one one is built and textile one when it is woven so microsoft research were composing a lot of the components youve seen today you know were trying to make that boat better were building all the necessary software and infrastructure for it but really what it comes down to especially you as the audience and looking for collaborators as well is weaving this all together through textiles so if youre someone whos interested in smart textiles connected experiences through you know jerseys or areas like that or even really just interested in wearables in general please get in touch and lets collaborate because thats really what were all about at microsoft research up next youre going to meet anne paradiso anne is a principal research designer in the enable group here at microsoft research and she works to pair human-centered design and technology to enable expression and creativity the enable group creates technologies that improve the lives of people with disabilities community is one important aspect to the groups work so they design new technology with users not just for users im ann paradiso and i am a research designer at msr and im sitting here in the enable lab in redmond one of the things that i care deeply about and that i know all of my collaborators care deeply about is that gap between what happens in a research lab or in this lab or in a development environment and getting technologies that are critical into the hands of users who can use them and benefit from them we have several collaborators inside of microsoft who have different skill sets that when we combine them together we find we can be more effective as a team expressive pixels is a technology-mediated creativity toolkit its the result of a long-standing collaboration among the enable group and central engineering teams at msr redmond and the microsoft small medium and corporate business team the platform embodies our shared belief that expression through technology art and communication should be an empowering experience for all expressive pixels can be paired with a variety of inclusive inputs and led displays to enable all sorts of novel alternative avenues for creative expression and communication heres a short introduction to the platform by expressive pixels co-creator gavin janke hi im gavin and im excited to share with you microsoft expressive pixels a fun new app that provides an inclusive experience for people with all abilities to express connect and share with others expressive pixels originally came about from a desire to provide a means of expression for some people verbal expressions may be challenging whether because of speech disabilities or they are just less comfortable with it often a visual statement through creative and artistic expression can communicate as much or more than words can ever say the result takes into account neurodiversity in lowering barriers with expressive pixels you can create your own animations or use animations from our online community gallery and make them come alive on simple led display devices microsoft expressive pixels can be downloaded for free from the microsoft store on your pc download the app today start creating and expressing yourself you can learn more by watching the tutorial videos on our website aka dot ms forward slash expressive pixel youd think it would be obvious if you were going to do design work or r d incubation work innovation work or whatever that you would understand the community that youre designing for its not always obvious our mission at microsoft is to empower every human and organization on the planet to achieve more so that word every is really where we focus that means every human not e not just humans who dont have hard problems to solve when you build relationships uh like we do with our creative collaborators and with our our community youre invested youre deeply deeply invested in their well-being as your friend your collaborator and your fellow human being and so that sense of investment purpose and empathy is what keeps everybody thats the glue that sense of of purpose that sense of common purpose is the connective tissue for all of us working in the space music is a primitive and powerful means of human communication and expression through it we can share emotions experiences and meanings with people across cultures and languages and even across time for the last five years weve been experimenting with a number of approaches to inclusive music in support of people with little to no motor ability who may also be experiencing speech loss the projects im going to show you next share one primary input constraint they have to be able to be played using just the eyes that is no speech no other movement the hands-free music project is an ongoing collaboration between microsoft local musicians and the als community it includes a suite of eye-controlled instruments and playing interfaces to support composition performance and live collaborations we are dangerfield [Music] a newbies my world was turned upside down that was when i was diagnosed with als uh obviously a lot of negatives here um but its been very interesting to see some amazing things occur in the aftermath [Music] [Laughter] [Music] do you enjoy [Music] [Music] the cyclops is an eye instrument designed by music technologist willie payne during his summer internship with the enable experiences group here in msr willie built the cyclops to support improvisation and performance scenarios for musicians who only have control over their eyes and yes willie is doing his entire demo using just his eyes the cyclops has three main screens instrument sequence and audio effects the instrument page is the primary page for producing sounds tiny red dot is where the eye tracker detects my eye gaze when i stare long enough at a button on the screen it will cause it to press playing a sound the sequence tab contains extra controls for playing with the loop we can change the tempo to affect the speed of the music we can add accents to increase the volume of individual notes or rests to silence portions of the loop on the effects page we can fine tune how our sound actually sounds with just a few controls the cyclops is an expressive instrument capable of a wide range of sounds all controlled and triggered just with your eyes to me one of the best ways to learn is to do and the way that you learn uh how you can best serve a user community who is facing profound constraints due to ability loss because of disease progression is to meet them listen to them observe them and you know take part in as many activities as we can because experiential observation is super critical especially when youre trying to decide were going to focus your efforts our space in general um we do a combination of things we do formal fundamental research in computer science write papers conduct studies create prototypes proofs of concepts evaluate them and release those findings into the world so that we can better inform the design of assistive and expressive technologies in the future we also incubate our own designs and technologies and we work iteratively and collaboratively with our user community to direct those activities we are honest about what we learn we put everything out in the open source or in the public domain we share we mentor students we mentor talent we we do all of those activities because that is how the progress is going to happen what way can we use technology in service to our goal of restoring or reinventing lost capability if you dont center your user community in every single activity every single research design or development or innovation activity that you have you will not be successful finally well hear from ken hinckley a senior principal researcher whos also from the epic group ken is going to talk with us about how devices can help to make sense out of our increasingly complex work lives and interactions with others hes worked on everything from sensing techniques cross-device interaction to novel device form factors today hes going to tell you a little bit about surface fleet this technology has implications for something many of us can relate to these days the home workspace im ken hinckley i work here at the lab microsoft research we innovate across hardware software and the human experience we do all kinds of cool research in terms of interaction with devices and also studying how people use devices and also interact with one another sweet study collaboration we study how people do you know work in productivity uh we study how they actually use their devices in terms of just picking things up looking at them moving them around all this kind of stuff so we have lots of you know very fun things that we do one of the you know exciting areas that we work in a lot is you know sort of figuring out how to leverage smarts about the world around us so bringing in things like sensors or you know sort of unusual inputs like speech or gaze or you know camera inputs that kind of thing to have a more natural way of working with our devices the actual name epic is an acronym right so it stands for extended perception interaction and cognition so we work across all these topics so the perception is the human perception of how they see the world how they feel the world how they hear the world the interaction of course is you know interacting with things around you moving things like even just a simple mouse or picking up a device thats an interaction looking at someone thats an interaction you might not think of it that way but if our computers and devices can sense these things and respond to them appropriately it makes it so they can be much smarter about how they interpret you know what were doing in the world um and then the cognition part is ties into you know not so much artificial intelligence but actual human you know genuine intelligence right and how people think about you know the world their tasks um how they think about interacting with one another uh and a lot of this is its not just in your brain but its actually externalized to the world so it um you know the so the e and epic stands for the you know this externalization or this extended nature of intelligence where your artifacts in the world actually encode information and you so you dont have to remember something you stick it on a post-it note and you stick it on your monitor and thats you know externalizing that reminder so that thats sort of the whole scope of the research in the group um so for example we do you know weve contributed to the new future work in terms of studying like what people do with their devices while theyre working at home what kind of tasks are they doing how are they dougling tasks with their kids and their families and their multiple clients and all these things we work across things like you know haptic feedback for example so devices that can actually respond to you with you can feel tactilely you know whats going on in terms of you know if youre interacting with virtual reality you can feel objects in the world you can feel the shape of things maybe if youre not if youre a person whos not sighted so we do all kinds of things along those lines so were kind of hurtling towards this future where everyone has these multiple screens and multiple devices all around them and this is a trend thats been particularly you know aggravated by this you know new future of work that we find ourselves in uh where suddenly were working at home you know youve got your screen there maybe youve got your kids ipad there you know maybe youve got another computer at the at the ready um and youre youre just trying to get your work done across all these devices and so its not unlike how people might work you know in an office space where theyre spreading out uh information across multiple pieces of paper theyre trying to you know lay out their thoughts you know organize things and kind of this informal way of putting things in space and so multiple devices can serve this same role so in a sense people you know they like to use space to think so we like to think of ways to do that in the digital world as well the main idea of surface lead is to make it easy to work across these multiple devices and sort of the objective is to make it easy to transition you know whatever activity youre working on from one device to another or from one location to another or even to pass it off to another person or to defer it to a future time when youre actually ready to work on it so having both technologies and user experiences that speak to that is something that were looking at very closely um so in particular today i can talk a bit about something were doing called the ultimate flexible workspace which is a device concept weve been exploring which makes us you can just walk up to your your your workstation at home uh with your tablet you just plop it into this armature thats kind of like an articulated lamp and then you can kind of posture your tablet anywhere in space and then the really cool thing is when you have multiple of these theyre all aware of one another you can sort of juxtapose them you can split them apart you can put one down one up so you know just depending on your task and what youre trying to do and what information you want to see side by side it makes it super easy to dynamically create these formations of devices because it also senses these transitions all you have to do is put the device next to another one and it says like hey you want your document here and then you can just drag it over or you know whatever you want to do so it it makes it really easy to work in these kind of ad hoc workspaces that we have in our homes where often you know space is very limited uh you might have multiple screens but youre just youre kind of in this little you know sort of improvised desks in the corner of your living room somewhere right so uh making it really easy to work in those kind of spaces which you know also can translate to the future when we get back to the office and were sharing spaces and conference rooms and so forth so in particular um last year which is actually during this you know pandemic remote work time we had a visiting researcher by the name of nick marquart uh whos a professor at universe university college london uh also happens to be a former intern who worked with me you know over 10 years ago now um and so nick and i have sort of done a number of projects over the year and he had this brainstorm about you know basically repurposing articulated lamps as a smart uh appliance you know desk appliance for the for the work at home situation um so the idea is that you basically take you know we literally ordered these armatures off of amazon i think right so theyre like 20 bucks uh just added some sensors to them which are just you know theyre basically just simple motion sensors its the same as you would have in your smartphone or tablet already put a bunch of those on you know the different armature links there and then we can actually sense where the tablet is in space so as you you know position it you can let go of it it just stays there it knows its angle it knows its position is notice how its postured relative to you you can put it over next to another device if that one also has an armature then that one also can know where it is relative to the other so they have this kind of spatial sense of like whos nearby just like you would be aware of if someone walked up behind you or next to you you collaborate with them so in that in that sense your devices are spatially aware theyre aware of one another and they can present these opportunities to just easily move documents or pieces of work or to take the input or camera from one device and kind of access it on another so it makes it really easy to work in the sort of multi-device multi-screen future that we see ourselves moving towards in terms of technology trends as well as the experiences that people want to have with all these different screens in their lives right so when people are you know in these spaces where theyre working across their digital content often we have these very you know complex sort of knowledge work type tasks were doing where theres youre trying to get insights from across multiple documents across different sites on the web so youre trying to bring these together into new artifacts that youre creating for you know heres my idea heres my report is my presentation um so and this is you know its cognitively demanding work so just being limited to one little screen to do that is just doesnt work so well and particularly people we find more and more are co-opting device other devices they already have into this so and and it goes beyond just um you know documents so for example if im in a teleconference call i might want to have the document up on one screen i might put the camera on my mobile phone which is somewhere else so it gets the right camera angle i can still see the content that im talking about and i can divide that across multiple devices you know or say someone sends me a you know a contract to sign right i dont want to take that and try to sign it on a big monitor i want to take that put it on my tablet i use the pen there to hand write it or annotate the document for something more or maybe i sketch up an idea there and then i want to get it back onto my big screen to do the work there so people like being able to extend their workspaces mix and match sort of the the best of each device that they have so they kind of leverage those strengths and use their multiple screens and multiple devices with all these different capabilities to compliment one another to kind of get more work done think more clearly about what theyre doing and just kind of have that great idea 