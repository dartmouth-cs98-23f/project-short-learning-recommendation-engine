be talking tonight a little bit about algorithms and complexity I was interested in algorithms and wanted to talk to them to some aspect of them in my tech-talk this topic proved to be a bit intractable but Im hoping to provide an introduction into some of the ways that computer scientists and theorists think about complexity and algorithms so my my screen is showing you a fractal pattern fractals are geometrical patterns that can be broken down into smaller units and each smaller unit is a like half size or reduced size copy of the pattern as a whole fractals are found throughout nature which we can see in this nice image of a cactus some flowers and cauliflower which is everyones favorite fractal image but fractals fractal patterns can also be generated by algorithms recursive algorithms and thats I think this is one example of that that you see on the screen now so briefly to give an introduction to an algorithm its defined simply as a precise rule or set of rules specifying how to solve a problem algorithms are complete they are finite they may be exact in terms of their solution to a problem or they may be in exact they may be approximate in programming they are evaluated among other ways but one important way is by their Big O notation which we are all familiar with and this the Big O represents the upper bound on the number of operations a function will have to produce well when it is called so the number of operations and how this number changes as the size of input grows gives us some information about how long the process is going to take to complete another graph which will be familiar this is a plotting of inputs n on this access little n versus the number of operations to complete the function big n on the y-axis constant operations that remain constant even as the size of input grows will give us a big n of a constant some constant call it 1 this is an ideal scenario but not always possible a Big O of n is when the number of operations grows constantly at a constant rate as the input size grows and these are less these are the sort of bago youd see for less complex algorithms as we get more complex you start to move to this area of the graph and Ill just draw your attention to these three clustered here where you have N squared o of M squared this is called polynomial time which is the largest factor in that time is going to be and to some power could be N squared can be in cubed then in the orange next thing to it you have 2 to the end this is exponential time and then finally factorial time and so one of the significant divisions in terms of how computer scientists think about complexity occurs in the change from polynomial time to exponential time so more on that in a minute but just to return to our fractals I wanted to start with fractals because theyre pretty theyre interesting to look at they use recursive functions which were familiar with but also they represent a case that might be kind of surprising I think for complexity which is that the patterns can look complex but the process to construct them might not necessarily be so so this is via Khan Academy a common fractal pattern called the sierpinski gasket here you see the finished product and then they also helpfully provide a step-by-step of how to construct this pattern you start with a square you divided into four squares you put an X in all of this the nude subdivided squares except for the one on the bottom left and then for the squares that have Xs in them you repeat the process again removing any marks from that bottom left square it should remain empty you repeat this process again notice each time the size of the square is diminishing and we have more lines and a more complicated picture finally well reach a point where we decide our dimensions are small enough we dont want to subdivide anymore and at this point if you fill in all the squares with Xs in them you get the sierpinski gadget so although this pattern looks complex you have a finite and constant number of steps for it each iteration of the function so if I were to posit what the o of n of that particular fractal function would look like I would guess that it would be constant n because as if we take the number of iterations as the input as that increases in size the number of operations is just going to increase at a constant rate so that puts it in the lake not so complicated region of our graph which brings me to the more complicated region of the graph there these terms P versus NP are ways that theoreticians think about complexity P means polynomial time that means a time to execute which has an upper limit marked by and to some power NP time is anything to the left of this anything that grows at a rate faster than that described by n to some power a more a more theoretical definition of them is problems in the set P can be solved on a deterministic Turing machine Ill explain what that is in a second by an algorithm that runs in polynomial time whereas those in the said NP can be verified like if you have a positive solution you can verify it on such a machine in polynomial time but you couldnt find a solution in polynomial time a Turing machine is named for the British mathematician whose first name is escaping me Alan Turing I think um who was a quote breaker during World War two actually developed the model the conceptual model that a lot of people still use for a computer before the first computer even existed he was working on breaking a German ciphers during World War two side note there is a 2014 movie called the imitation game with Benedict Cumberbatch and its pretty good and its about Tourings code breaking work during the war it has a pretty sad ending which reflects his prosecution at the hands of a moralizing state so just be warned if you watch it but I would recommend it so NP problems can be verified but not solved in polynomial time they cant however be solved in polynomial time on a non-deterministic turing machine which is just a concept of some kind of computer that would be able to be in different states so performing different operations at the same time which I dont think exists in real life it may be quantum computing but thats a question for someone besides me then we have within the class of NP problems to further complexity labels and P hard and np-complete as far as I can tell the definition of NP hard means its just as hard as any hard problem in a set NP + np-complete shares that criterion but also and this is kind of interesting theyre an NP complete problem is kind of a representative problem for the set of all NP problems the idea is if you could solve an NP complete problem your algorithm could solve with a little bit of tweaking every other NP problem and the reason this is interesting is that a lot of internet encryption relies on NP problems remaining unsolvable in any reasonable amount of time and so if somebody were to come up with a solution to an NP complete problem internet security would be in big trouble because youd be able whoever had that solution would be able to with work Shirley but not you know crazy exponential amounts of work break every or solve every other NP problem and therefore break a lot of encryption but what if you still need a solution to an np-hard problem so one approach is to develop an approximation algorithm this is gonna be the case of like Goodenough gives us a working solution may not be the best may not be ideally optimized but will at least get us a working solution for our problem so for the remainder of the presentation which may not be much longer Im just gonna take a quick look at one such case where its a very well studied problem called the vertex cover problem and this is a case in which an approximation algorithm so sort of relaxing the constraints on your ideal case will enable the problem to be solved the vertex cover problem comes from graph theory so its quite convenient that Karen talked to us last week about what graphs are these slides that are formatted in this way are from a course offered at the NSBA quill normale sup√©rieure in france and made available via Coursera so I have references at the end of the slide if youre interested but I just want to make clear that this is not my work so the vertex cover problem is you start with a graph heres an example of a graph its defined by vertices which are blue dots the vertices are labeled they have little letters they they also have weights which are the red numbers so each vertice each vertex has a weight which is at least in this case fairly arbitrary the edges are the black lines and in a vertex cover problem you want to make sure that for each edge like this edge C be at least one of its endpoints ends up in your solution set of vertices the reason if if speeding along here the conclusion about this problem is that it is going to be np-hard and why you might ask me and I might tell you well a lot of people have studied it and they said so and theyve proven so and a really smart professor at the Ecole Normale says so so we should all agree with that but I try to work out one reason myself which is that if this each if we have a problem with four vertices here right ABCD for each vertex you have two possible States yes or no if we are brute force coding through all possible configurations we end up with two to VN possible configurations shown here so this is a problem with only four vertices and it ends up with 16 possible solution States so this if we go back to that graph earlier to zdn is outside of the bounds of polynomial time again briefly it turns out that if you take the word of scientists past and scientists present that this problem is np-hard its only because the vertex can either be in or out of your solution set so its either a zero if its out or its a one if its in and this integer constraint is what makes the problem unsolvable um a cheesy math example of a situation where you might have no integer solution is the second set of equations here or the second equation here where theres theres no energy integer solution that will satisfy this equation but if we let X be a real number instead of an integer you can find a solution its just going to be messy so long story short thats what happens with the vertex cover problem if you relax your condition and allow your solution to be a non integer you allow the vertex to be kind of like partly and partly out for the sake of solving the problem you can in fact find a solution it may not be like ideally optimized but the magic moment and Ill just tell you this about the end is that you round this is what you do when you have your decimal points between 0 and 1 for each vertex is you round up to one for that particular vertex if its if if it ended up with a value that was greater than or equal to 0.5 and zero otherwise and the limit on this is that it would be at most two times the cost of the sum youre trying to minimize the sum for all the vertices that are included but apparently in like day-to-day working scenarios its usually within about 10% of an optimal solution which doesnt seem that bad so so much for a very quick look at complexity in dollars [Applause] 