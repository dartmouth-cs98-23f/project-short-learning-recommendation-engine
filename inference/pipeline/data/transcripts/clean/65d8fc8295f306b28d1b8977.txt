and to the algorithms course at the university of cambridge todays topic is dynamic programming todays topic and the topic of the next few videos actually dynamic programming is a powerful technique that applies to a large class of problems where a naive exhaustive search approach would have exponential complexity if you use dynamic programming instead these problems can be solved in low polynomial time a dramatic improvement dynamic programming can be quite puzzling if youve never heard of it before so this very brief video will give a general overview but experience tells me that it may still not completely make sense the first time you hear about this stuff in the next few videos ill work through a few examples and then after that we can recap and at that point this video will hopefully sound a lot clearer if you are already experienced with dynamic programming more power to you and feel free to watch these numerous examples of application of dynamic programming at high speed so that you get more quickly to the point where you can open your editor open your ide and start writing code to prove to yourself that youre actually capable of applying these techniques for example the dna problem that i posed in the very first lecture in the course and write a program that takes two input strings and returns the longest common subsequence between them longest the longest or one of the longest if there are several of the same length of course not all problems can be solved by dynamic program but the problems that can be solved by dynamic programs programming share certain features which you will have to learn to recognize your glutes for saying okay dynamic program is probably a good idea in this case so the first of these features is that the problem uh has many choices each choice with its own score and you must find one that has the optimum score second property is that the number of choices is too large for trying demolite by brute force typically an exponential number of choice the third property is that the optimal solution is made of optimal solutions for smaller sub-problems and the fourth property is that these sub problems overlap and therefore searching for the optimal solution without dynamic programming would typically end up recomputing the same partial solutions many times to apply dynamic programming you must first describe the problem top down in a recursive fashion describing as per property 3 the optimal solution in terms of smaller optimal solutions however if you then executed this recursive function you would usually end up conducting an exponential time exhaustive search and so the main dynamic programming approach is instead bottom-up you solve the sub-problems starting from the base case and from smallest to largest so that wherever a solution requires the solutions to sub-problems these sub-problems have already been computed and you can just look them up thats the basic fundamental bottom-up dynamic programming approach it is also possible to do dynamic programming in the top down fashion by going through the recursive expression of the optimal solution provided you use a trick called memorization you augment the recursive function with machinery that remembers all previously computed results so when you do that the recursion doesnt end up recomputing the same things many times because if it recognizes that the result was already computed before then it just looks it up and returns it in the next few videos as i said ill give examples of problems that i will then solve with dynamic programming in this video i will just show you a much easier example that isnt quite dynamic programming but that demonstrates the perils of wasteful recomputation of intermediate results as for an introduction to dynamic programming im first going to mention something that is not dynamic programming but well give you some ideas about some pitfalls that dynamic programming may expose you to so you are familiar with the well-known fibonacci sequence where starting with one and one you add them together and you get two you add the last two numbers together you get three you got the last two numbers together you get five you have the last two and you get the next one you add the last two numbers together to get the next one so you were exposed to that in kindergarten and youve seen that this is the way that leads are arranged around the stock is the way that if you continue you get the the golden ratio and greek temples and all beautiful things now its fun when you are just a few years old to compute the successive numbers in the fibonacci sequence when you become a computer scientist or a budding computer scientist and you get exposed to recursion then your brain gets polluted into thinking that its cooler to compute them in reverse by saying okay uh give me the um i dont know the 30th fibonacci number okay so fibonacci of n equals 30 if n is less than 2 then i can find the answer very easily because its at the bottom but otherwise the result is going to be the sum of fibonacci of 29 and fibonacci of 28 and you do that by saying fibonacci 29 i go back in here 29 uh read the recursive by 28 and 37 and then when i finish with that i will do the 28 and i will add it up so if you write this type of program a very simple recursive program to compute fibonacci sequences uh and you run it on something like for example 30 you will think that your computer got stuck because you said financially nothing works uh why is that it will take a very long time to even complete something as basic as 30. um you could do quicker on a piece of paper by yourself than asking your computer to compute you with that program because what happens with something as small as just 10 is this so you ask it to compute a fibonacci of 10 and you will first have to compute fibonacci of 9 and 8. but to do 9 8 first does 9 it does 9 and to do 9 first he has to do 8 and he keeps recursing back until he gets to 1. when he gets to 1 he can go back and say all right that was 1 so it returns uh the value back to you but then uh its not enough because it has to go back and compute fibonacci number of uh eight so this was ten you have to compute nine and eight and eight you have to redo so many of the same things that you already did before because here also when you said eight uh or nine its not enough just to compute eight seven six four eight in turn you have to go back and compute seven uh and six and four seven you have to go back to q6 so you recompute the same things so many times uh its uh easy to even lose track here this is how many times you have to return something so every time theres an error like this one of these things returns a result and look how many times you have to go back before you finally get the two things that you can add up uh to return this number so with um with these nested uh double calls its not the fact that you have two calls its the fact that each call recomputes things that youve uh youre computing over and over again and so these things uh ends up with a number an exponential number of calls which makes your program extremely slow and inefficient so something like this which may look like a mess compared to this it may look inelegant its actually if you read through it its simply well if its uh one of the first two then the result is one but otherwise set the basic uh starter chain to one and then uh for every next element just add the two previous ones and shift add the two previous ones and shift and the result is the end and return so this is just an iterative version doesnt call itself is the thing you did when you were in kindergarten and youre adding them up and this is actually a much better idea so because its it looks easier to express things this way well not that much easier in this case but in many cases the formulation that goes from the top down and uses recursion is a bad idea for computing the result however it is an easy way to describe the solution to the problem theres a trick you could use to resort to this formulation without spending so much time and that is use that but every time you actually return a result remember it somewhere so you make yourself a little memo and you write okay i actually already computed fibonacci of three and the result is two uh and so next time youre asked to do fibonacci of three instead of uh recursing down you say ah and this was two and then when this next time youre asked to do fibonacci of seven you dont have to recurse you read it up from the table of things you computed earlier so you pay the price only once for each result you dont recompute the same results every time so this is called memoization not a memorization memoization and its a technique for rescuing these otherwise pathetically inefficient recursive formulations of these problems so dynamic programming this is not dynamic programming but dynamic programming is a a technique that can be used to tackle problems where there is this feature that the optimal solution to the big problem involves optimal solutions to smaller problems which overlap and which if you did in the regular brute force or top-down way you would end up recomputing many times the same problems would be recomputed so many times so memorization is a technique whereby each component problem is only computed once and so you save yourself from falling into that trap 