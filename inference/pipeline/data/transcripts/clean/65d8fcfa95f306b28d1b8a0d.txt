but we are surrounded by CPUs, or processors, and the computing they do for us. They touch every aspect of our lives. CPUs are in your laptop, in the machines you use to check out at the grocery store, in the electronics that power the instruments in your car more efficiently. They enable our artists and scientists to create things that were unimaginable only yesterday. CPUs are everywhere, and shape just about everything we do. Welcome to Architecture All Access CPU Architecture Part Two. Hi, my name is Boyd Phelps, and over the last 23 years Ive had the privilege of working on some of the most well known chip designs in Intels history. From the architectural definition and design of the Pentium 4, to designing the Nehalem, Westmere, Haswell, Broadwell and Tiger Lake processors and many more. Today, I help lead the development of Intels Client Engineering Teams, where I oversee the development of current and future products. In Part One of the CPU Architecture series, we talked about what CPUs are, the history of the CPU, the concept of computing abstraction layers, as well as the instruction Set Architecture. The instruction set architecture is what we normally refer to as the architecture of a CPU. Today we want to delve into what the microarchitecture of a CPU looks like, or in other words, once an ISA is defined, how you might go about implementing that ISA into a microarchitecture, and what the main building blocks of the functions of that microarchitecture would be. So lets dive in. A microarchitecture is an implementation or specific design of an instruction set architecture. The first step is to fetch or retrieve the instructions from memory so that the CPU knows what the program wants executed. This is called the Fetch stage. The next step is to decode the fetched instructions into native operations. This is called the Decode stage. Sometimes this means taking the instructions and breaking them down into multiple internal operations. Once the instructions have been decoded, the CPU needs to execute them. There are many different types of operations. The CPU can perform math such as add, subtract, multiply, divide, or perform boolean operations such as AND, OR, XOR, NOT. The CPU also compares data, makes decisions about where to go next in the code. We call these decision instructions branches as they can steer code to different places. There are many other operations CPUs perform, depending on the ISA and what we call the Execute stage. Finally, the CPU will store those results. Sometimes those are saved locally in a register, and sometimes theyre stored in memory. This is called the Write Back stage. These operations make up the basic building blocks of every modern CPU. When put together, they are referred to as the CPUs pipeline. Now that we have described a basic pipeline, what do modern microprocessors look like? Over time, the average number of pipeline stages has grown. A pipeline is very similar to an assembly line. The more stages are added, the less is done in each individual stage. Now when Henry Ford wanted to drive down the cost of the Model T, he built an assembly line with many stages. This made each stage simpler, allowing workers to specialize in one specific task. This, in turn, allowed workers to do their tasks at the same time, in a pipeline fashion. The result was an automotive revolution that put cars into the hands of more people at a lower price. CPUs are very similar. In general, the more pipeline stages you have, the faster each stage can run, and the more stages are being done in parallel. A modern microprocessor has around 15 to 20 stages. The Fetch and Decode stages typically have six to 10 stages. Collectively, these are called the Front End of the microprocessor. Execute and Write Back have also grown into roughly six to 10 stages. These are called the Back End of the microprocessor. A CPUs pipeline is synchronous, and what we mean by that is each pipeline is controlled by a clock, and each data goes from one pipeline stage to the next as a CPU clock completes a cycle. The number of stages partially determines what the peak frequency of a CPU is. Now modern day CPUs can run over five gigahertz. The amount of logic in each of these stages determines how fast the stages or clock can operate. If a CPU runs at five gigahertz, this means that the stages each need to complete in five-billionths of a second. Remember, a hertz is one cycle per second. Now, Henry Ford would be pretty impressed with the assembly line speed of todays modern CPUs. If you recall our basic pipeline, at the beginning we fetch an instruction, and towards the end we execute the instructions. Some of these instructions are called branches. These represent a decision point or fork in the road like an exit on a highway. Do we want to keep going or do we need to exit now to take a different path? When you execute a branch, you are making that decision. As the pipeline depth grows, you get farther and farther away from the answer of which path to take. When the branch says to take a different path, we need to tell the beginning of the pipeline to redirect to a different instruction. The work that was in progress needs to be thrown away. This is both bad for performance as well as for power, since we have been spending time executing instructions that were not needed for the programs execution. We could avoid speculation simply by stopping every time we saw a branch, and just wait for it to execute and tell us the correct direction in the code to go. This would be safe. It would also be slow. However, there are a lot of branches in most code, and that means a lot of time spent waiting. So we guess where to go next, and when we see a branch and if were wrong, well simply throw away all the work after the branch once we know we were wrong. If we were right, we celebrate and keep going. As pipelines get longer, the penalty for guessing wrong gets worse. Fetch becomes further away from Execute, which means it takes us more stages to realize that we are executing on the wrong path. To solve this, microprocessors invest heavily in design to make accurate predictions, or guesses, at the beginning of the pipeline. We call this the art of branch prediction. When we see that we have gone down the wrong path, we can update or refine the prediction with what the right path was. Then the next time we see that address, the branch predictor can tell us to go to a different address. Modern CPU architectures can often predict branches with a near perfect accuracy that makes them seem almost clairvoyant. When a microprocessor executes newer instructions than a branch without knowing if that branch is taken or not, it is referred to as speculative execution. This is a fundamental component in modern microprocessors for achieving great performance. OK, so lets look at some simple code to explain how speculation works. Here we have a very small, simple C program. All were doing is counting to four hundred. And the way we do this is we have two loops. We have an outer loop that loops one hundred times, and we have an inner loop that loops four times. And every time we get into the inner loop, we increment this c variable. And so since we loop 100 times on the outer, times four on the inner, we increment 400 times. So here we have the instructions run by the CPU. This is the native language of the CPU, where weve taken that C program, and weve compiled it down into the language that it understands. And well just highlight each of these instructions and tell you kind of what they do. At the top you have a mov instruction, for example, that moves 0 into this i variable. Remember, if you look at the C program, this i variable is the counter for our outer loop. We have a cmp instruction. This is a compare instruction. It compares that i variable to 99. And right after that we have jg, a jump greater than instruction. So if that is true, if the compare of i is greater than 99, then were going to jump or were going to branch to Label2. If thats not true then we ignore and we just move down and look at the mov instruction. Again, we see for the inner loop another compare instruction. Were comparing j, the counter for the inner loop, to 3, and we have a branch instruction right after that compare that says we jump if j is greater than 3. And when we do that we increment the outer loop by 1. Thats that add i, 1 instruction, and then we jump back up to the top of .Label5. Now, on the inside of the inner loop, where we move c into eax, we add 1 to eax, we move eax into c, and then we increment that outer variable. So the c to eax, 1 to a eax, and eax to c, those are the move instructions where were moving from memory to an internal register, but you see the add instruction; there were adding 1 to eax. Theres where were actually incrementing that inner variable of c. When we get to these jump greater than instructions, this is the first time its seen its branch. And so we can either stop the execution, we can allow the compare in the branch to actually go through, and we can wait until they execute, they resolve and then we know where to go to next, or we can actually make a prediction and keep fetching. Now, lets assume that we had decided to start our prediction by assuming all branches are taken, meaning that theyre true and that were going to go to their label. If we had done that the very first time that we go through this code, when we see the jump greater than go to .Label2, if we assume that was true, we would jump to .Label2. In the code we would figure out that was wrong in the execution of the pipeline, and then we would flush all of the pipeline, all of the work that weve done in the pipeline, we would resteer the front end, and then we go tell our branch prediction algorithm, Hey, you predicted wrong. That branch was actually not taken. And then we would update our algorithm. And then the next time we came through, we would see that branch and it would say, Hey, jump greater than. And we would say, Nope, not taken . Thats our prediction, and we would move fast, and we would fetch the mov instruction, and we would come down and execute the inner loop and we would go through the same thing with the the branch, the compare on the branch on the inner loop. Now, whats interesting here on this inner loop, we go through that loop four times. The first three times through the loop, that branch is not taken. On the fourth, its true, because j is greater than 3. And so therefore, we do want to jump out of that inner loop. So our prediction algorithm, it would actually learn. It would actually learn that the first three times for that inner loop, its not taken, and on the fourth time that it is taken. So in our speculation algorithm, we may stumble through this the first time, but we execute this loop a hundred times. By the second iteration of this loop, weve learned how this branch behaves. So again, this is a simple code example from C that we compiled into instructions that the machine can understand, so you can kind of highlight and see how these branches control the flow of the code, and how it is that the machine has to make different decisions at different points of time in the code. Think of speculation, and speculative execution, as when I visit my aunt in the countryside. As Im driving on a country road and I get to a fork where I need to decide if I turn left or right. And I really havent been there before, and I can call my aunt, but I know she normally takes up a while to pick up, and even when she does, shell get talking about a few topics before I can ask her the question, Should I go left or should I go right? So I look around and I see some houses on the right at a distance, so I decide to go in that direction. And on the way there, I call my aunt and sure enough, it takes me a few minutes before I can get my question in, and it looks like I choose correctly, so by the time she confirms I went the right direction, Im almost by her house. And obviously, if I had guessed wrong, I would have to go back to the fork and go in the other direction. This is what speculation on a CPU is like. The CPU makes a prediction on what direction the branch might take and begins executing instructions based on that direction. And as I said, modern CPUs can often predict branches with a near perfect accuracy that makes them seem almost clairvoyant. Now that we have a good understanding of pipelines and speculation, lets talk about what functions go into the first half of the front end of a microprocessor. Now, branch predictors have become incredibly complex in order to improve their accuracy while still being able to steer fetching of instructions at a high frequency. Branch predictors today can oftentimes record and understand and learn the past history of hundreds, sometimes even more than that, thousands of branches before them in order to make a single prediction of the next branch and where it is going. The sophistication of modern day branch predictors is really kind of a precursor, if one might think in terms of artificial intelligence, in terms of learning from past behavior, how the future will behave. Theyve become so accurate that they are now in charge of deciding which address to fetch next, even if the prediction ends up being Keep calm and carry on. Now, CPU frequencies have increased much faster than memory speeds. This means it takes longer to fetch data from memory. And to help offset the long round trip time to main memory and back, we keep local copies of main memory internally in structures called Caches. The front end has an Instruction Cache so that it can read instructions in just one to two cycles instead of the hundreds it may take to go to main memory. To optimize both power and performance, many adjacent instructions are fetched at the same time, which are then handed off to the decoders. If the instruction cache does not have the data, then the data is requested from the memory subsystem, and well talk more about this later. The main goal of the front end of a CPU pipeline is to ensure that there are always enough instructions available for the back end to execute, and to avoid the idle time spent waiting for instruction bytes from memory, or time spent fetching instructions that will end up being thrown away due to a bad branch prediction. The second half of the front end is where the programs instructions are decoded into the microarchitectures internal operations, which are called micro-operations, or uOps for short. This is the strongest connection between the instruction set architecture and the microarchitecture. As we explained in part one of the series, in general instructions consist of an opcode, the operation to be performed, like ADD, and a number of operands, the data to be operated on, like add A in register X to B in memory location Y. ISA instructions often also include additional bits of data that give the CPU more information relevant to the operation, which the CPU uses to decode and execute the instruction in an efficient way, according to its microarchitecture. Microarchitectures are typically built so that most instructions map directly into a single uOp, but not all. This helps us to simplify the back end of the pipeline. However, there are often some instructions which are more complex, and may need to generate multiple uOps for instruction. Instructions like these helped reduce the number of instructions required for the program, which in turn makes the program code smaller and easier to store in memory. We simply expand them into the uOps needed inside the CPU. Conversely, decoders can also fuse multiple adjacent instructions into a single uOp. This fusion can allow a micro architecture to do more work at the same time, leading to improved performance and efficiency. For example, most branches are usually preceded by a compare instruction just before the actual jump is executed. Whenever we see these two instructions together, we can simply fuse them together into one instruction for the back end of the machine for more efficient execution since the compare and jump use separate resources. So dont worry if you dont understand all of this right now, but just know that the instructions and the ISA can get broken down into multiple uOps, or combined, or they get simply decoded as simple single instruction uOps. The front end is always looking at how to decode and prepare those instructions to be executed efficiently. Some microarchitectures create a Decode Cache and save these uOps for the next time they need to be decoded. This can save the energy required to decode them and improve performance when one-to-many expansion is common. After the instructions are decoded, or read from the decode cache, they are then passed to the back end of the pipeline. Before we dive into how the back end of a microprocessor is built, lets take a moment to understand two important topics. The first is superscalar execution. The simplest form of execution is an arithmetic logic unit, or ALU for short. A basic ALU can perform operations like adds and subtracts. If we have a single ALU this means we can do one add at a time, which is referred to as scalar execution. Modern microprocessors now implement many ALUs, and when those ALUs can operate in parallel, this is called superscalar execution. The number of operations that can be executed in parallel is one way to measure what we call the width of a microprocessor. All modern microprocessors are superscalar. This increases the demand on the front end, which is why its important to design a front end that can feed instructions quickly to the back end. Now, depending on the target usage, designers can always decide to increase the superscalar ability of the SoC by also adding more CPU cores in addition to more functional execution blocks within those cores. In a basic microarchitecture, the uOps are executed in program order, which is called in order execution. However, in order to provide the best performance and energy efficiency, executing them out of order is actually better, allowing code to execute faster as we remove unnecessary wait times. Imagine youre running a restaurant. You have tables full of customers and you have many cooks in the kitchen and you want all your customers to be happy. You could let the first table seated order and eat first, and then move to the second table, but you would likely go out of business quickly as everyone watched the first table eat. Instead, you hire a wait staff who take each order from each table as the table is ready to order. These are passed off to the kitchen, where the order of the dishes are planned... Appetizer, main course, dessert, and theyre handed out to the different cooks. The food is brought out to the tables as it is ready, allowing your wait staff to execute in parallel. This is out of order execution. You take the order from the front end, you look at the individual dishes, the uOps, and you decide how and when to make and serve the food or execute those instructions. You have happy customers and a fast, efficient microprocessor. This is what it would look like with actual CPU instructions. The first step in an out of order back end is to take the uOps provided by the front end and determine their dependencies. A uOp is said to be dependent on an older uOp when it needs the result of that uOp in order to do its own operation. This dependency is tracked by a process called Register Renaming. This is the first step in out of order execution. This step takes up to somewhere between two and four stages in a modern microprocessor. The uOp information is also written in a structure called a reorder buffer, or a ROB. Even though we execute instructions out of order, we still need to have a way to put them back in order. The reorder buffer gives us the ability to understand the original program order. This process is called Allocation, and is another way in which the width of a microarchitecture can be determined. This is the most common definition. Since there are ALUs in an out of order microprocessor, we must determine which ALU can execute a uOp. There could be multiple instances of the same ALU, and there can also be different types of ALUs that can perform different types of operations. Some can do integer operations, or whole number math, while others may do floating point operations, or decimal or fractional based math. Some ALUs can operate on multiple elements at the same time, referred to as vectors. Branch operations may also have their own execution unit. The uOps must be assigned to an ALU that can execute their particular operation. They are then sent to the uOp scheduler for delivery to the required ALU. A scheduler is a place where uOps wait until their dependent operations have executed. And once these dependencies have been resolved, and theyre cleared, the uOps are then free to go, and could be sent to an available ALU and execute. When a uOp has executed and becomes the oldest uOp in the back end, the ROB decides that the uOp can be written back safely. This process is called Retirement. Let me illustrate that with an example. Here we see six instructions that are being fed to the back end of our CPU. The first three are integer operations. The others are some vector operation, some divides and multiplies. You can see that the second instruction depends on the result of the first one, since both of these use the same operand G1. So we can schedule the first instruction, have the second operation wait in the scheduler, but since the third operation isnt dependent on the first two, we can assign that to an ALU already and let it go. Similarly, the fourth instruction can be sent to a vector ALU right away. The fifth Instruction will need to wait for the fourth one to execute, but the sixth one is not dependent on anything, so it also can be sent right away. And so you would see something like this, where the four instructions that are not dependent on anything can be sent all at once to different integer and vector ALUs, while on the next cycle, we can have the next set of instructions execute. We were able to execute many of these instructions in parallel and out of order, thereby increasing the performance of the CPU while still making sure that the dependencies were kept, so we get the correct answer and the correct results at the end. Obviously, this is a simple example with six instructions, but you can probably imagine how useful it is to have these out of order and superscalar capabilities in real code that runs millions of instructions, operating at billions of cycles per second, looking at thousands of instructions. So lets recap what we discussed in the second part of our CPU architecture module. We covered some of the basic building blocks of CPU design, such as the von Neumann cycle of Fetch, Decode, Execute, and Write Back, how a pipelines depth is determined by the number of stages those basic functions are broken into, and how speculation is used to increase CPU performance, trying to always guess where code is going in order to avoid wait times. We then dove into the CPU front end, where the CPU predicts what instructions are needed next, fetches them, and decodes them into micro-operations, or uOps. From the front end, we move to the back end, which takes the decoded uOps and uses super scalar execution and out of order execution to efficiently assign uOps to execution units. I want to thank you for your time, and I hope weve not only been able to give you an appreciation for both the science, but also the art of CPU design. Its truly a creative endeavor. Weve come a long way since the days of the ENIAC and vacuum tubes, but perhaps what excites me the most is the rate at which innovation continues to progress in both CPU and system-on-chip design. Engineers across the various disciplines of architecture, design, process technology, memory, packaging, and software development are collaborating in new ways that will drive performance and solutions to problems of the future that we can barely imagine today. If the last few decades have been a marvel, I think its fair to say to you, you havent seen anything yet. This has been Architecture All Access CPU Architecture Part Two. 