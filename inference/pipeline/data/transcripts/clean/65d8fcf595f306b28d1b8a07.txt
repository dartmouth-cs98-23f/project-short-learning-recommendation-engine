all of the AI algorithms you know and all their capabilities and its easy to forget what has made all those algorithms possible is the constantly improving capabilities of the hardware that powers them and a significant component of that Improvement has been coming from something called Mos law Im sure youve all seen graphs of Mos law Im going to save you from having to see one but I will present evidence that Mo law is over that is fundamentally because Mos law was an economics law and specifically on it talked about the exponentially decreasing cost of transistors and while actual transistor costs tend to be closely guarded secret there is evidence that theyre not going down as shown in in this chart where which is showing Gates which are small collections of transistors and what we see is that mors law is flatten flattened out the response has been a focus by computer Architects to use specialization to create more efficient accelerator architectures and I participated in that and here is a subset of the accelerator designs that Ive worked on over the past few years each has unique attributes and capabilities but like most accelerators they tend to be described by pictures like this which are somewhat inscrutable and long natural language descriptions that tend to be incomplete and even worse lack structure that makes it difficult to be able to separate the big picture from the details to improve the situation I suggest taking inspiration from the world of science where it is generally accepted that science advances by systematizing and categorizing knowledge in the world and I agree with Nobel laurat herb Simon who sees many parallels between science and engineering and so one might assert that engineering advances by systematizing and categorizing knowledge about designs to apply that AI accelerators we note that a commonality amongst many of the algorithms is that they can be expressed as tensor computations so whats a tensor well tensor is simply a set of values organized into a multi-dimensional structure where we will call the dimensions ranks so we have a rank zero structure zero tensor which is just a single value we have a rank one tensor which is an array of values and so on the other thing thats very nice is that theres a very nice way to represent a tensor so we have the tensor a i j k where I is the row J is the column and D is the depth of a tensor a however to systematize things a bit and avoid dealing with the Myriad of different concrete ways that people represent tensors were going to use a single abstract representation which we call the fiber Tree in this in this abstraction the tensor is represented as a tree here each level of the tree corresponds to a rank each row or column in in the tree in the tensor corresponds to what we call a fiber and each fiber is indexed by a coordinate and then finally each point in the tensor is identified by a path of coordinates through the tree Ive Illustrated a dense tensor so there is a nonzero value in every Point how however many problems are sparse meaning that they have many zero values why do we care about this we care about it because there are many opportunities for exploiting that sparsity in specific if the tensor is sparse it can be compressed taking up less space and using these simple arithmetic Expressions we see that having a zero means that there theres really nothing to do and so an architecture that can exploit these characteristics in sparse operands can result in Savings in storage execution time and energy consumption to include this information in our data abstraction we return to our fiber tree and note that if the tensor is sparse like this then we can simply remove coordinates and their children in the tree so now turning to the computations on these tensors theres actually a very nice notation for describing the essence of those computations this notation has has its origins in a paper by Albert Einstein over a hundred years ago and is referred to as einom heres a very simple example of matrix multiplication in this notation note how the formula is simply references to a the points in the tensors so its very simple so how do we interpret this Ein sum we interpret the Ein sum by saying that what were going to do is Traverse all the points sorry in the space of all legal values this is referred to as the iteration space and at each point in the iteration space were going to calculate the value on the right hand side were going to then take that value and assign it to the upper end on the left hand side unless theres already something been assigned there in which case were going to reduce the value into it notice how the insom does not specify the traversal order so what its really doing is saying what the computation is but not how to do it the ism also tells us a lot about the work that needs to be done heres an example when we have a situation where the same in index in this case k appears in both operand sparse operand we need to get a value from each of those operand we call these shared indices and or in order for there actually to be any work to do both of the operands must be nonzero what this corresponds to is an intersection and what we have found is that this and many other operations that are implied by calculation of the inom correspond to manipulations of fibers in the fiber tree so for intersection if we have two sparse fibers what we notice is that these two sparse fibers only have coordinates common coordinates at two and eight and so the result of the intersection is a fiber with only coordinates 2 and 8 there are many Hardware implementations of intersection but we have done is separate out the semantic requirements of the calculation from the details of the design with that knowledge we have found that what we can do is very concisely Express the essence of many published design breaking it down into the einum and the traversal which we call the the mapping but now with a concise and comprehensive design specification theres a number of different things that we can do one is build a model to characterize a design such a model takes in the inom the traversal information and design details and can generate speed area and energy but these also these ideas can also help with design so a student took this accelerator design that was published and studied this diagram from the paper and looked at the long English natural language description and actually found that that wasnt complete so contacted the author and found out the remaining details and with that information was able to write a comprehensive concise and complete description of the architecture for for that accelerator which she looked at and said oh theres some very obvious optimizations here and wrote them down and generated dramatically improved performance so in summary I would argue that taking this approach allows for implementation independent expression of computations format agnostic specification of processing activities descriptions of accelerators that are decomposable into separation of concerns concise precise allows for the creation of analytic and more detailed performance models and provides a path for exploring the new architectures that were going to need in the future thank you 