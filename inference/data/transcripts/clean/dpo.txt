this video is about a new approach to training language models specifically it allows you to move the probabilities of a language model away from Bad answers and towards good answers direct preference optimization or DPO for short is a type of reinforcement learning it's a more efficient type than has long been used by companies like open Ai and meta in developing Lama 2 and with this DPO technique it makes it much easier to find you models towards a chat format that is aligned in a similar way to those commercial models available for agenda we're going to take a look at how normal training Works standard fine-tuning or supervised fine-tuning as I've done in previous videos then I'll contrast how direct preference optimization works and why it's a bit different in terms of a technique and it's useful in a different and complimentary way to standard fine-tuning then I'll show you two DPO data sets the data sets need to be in a specific format if you're going to use the hugging face trainer that we will use today and then I'll walk step by step through the training notebook I've developed for doing direct preference optimization by the end of this tutorial you should be able to take a model you've developed a language model ideally you do some supervised fine-tuning to get it into a chat format and this will allow you to do the final stage of aligning the model um another way to think of that is moving the model probabilities away from Bad answers and towards good answers that will make sense as we get a bit deeper into this video let's start off with a description very high level of how standard training Works standard language model training involves penalizing the model based on predicted what it predicts for the next token versus the actual next token for example if you feed in a phrase or part of a phrase that says the capital of Ireland is and you ask the model to predict the next token if the model predicts Dublin and then you wouldn't free penalize it because that answer is the next token here in this sentence but if it says something like cork then you would penalize the model and back back propagate through so fundamentally language models are statistical models that look at the frequency of information that is in the training data set if I train a language model with the capital of Ireland is Dublin the capital of Ireland is Dublin the capital of Ireland is cork well if will see Dublin more frequently and so that is more likely to be returned when the model predicts the next token if I want to bias the model towards saying Dublin with a very high probability I can just keep feeding in the capital of Ireland is Dublin the captain Ireland is Dublin and eventually the relative frequency it is seen for Dublin will be much greater than Cork and so it is going to respond with the word Dublin as the answer and that if you generalize it is how language models work work they're statistical the more data you feed them a certain format the more it will bias bias the model towards that answer let's just look at a simple example here where we have a data set with doublin as the next token 10 times and cork as the next token one time if we train on this data set the language model is very often going to predict Dublin as the next token because it has seen it so many more times in a data set now if we wanted to increase the probability even higher we would just have to add more and more and more data that says the capital violent is doubling so that we increase the probability of Dublin being the answer now by contrast let's take a look at direct preference optimization here instead of adding more data to the model and penalizing it based on the next token prediction we're going to do something a little bit more nuanced at a high level we're going to drag the probability distribution away from one answer like Cork and towards another answer Dublin so as an example we have a prompt the capital of Arland is and then we're waiting on the next token and we're going to set up two responses we're going to set up a chosen response which is Dublin and we're going to set up a rejected response which is cork so remember in standard training the only data we have is on the actual token and we just compare actual to what the model predicts which could be anything it could be Russ common up in New York it could be any value the only input data we have is the actual next token but here in direct preference optimization we have a pair of options we have a Chosen and we have a rejected option and the way that we penalize the model now is not by comparing the actual to the predicted but rather we penalize the model so that the probability of Dublin of the model we're training we want that probability to be high relative to a ref reference model the reference model is just a copy of the model when we start this whole DPO step so imagine starting DPO we have a model we duplicate it so we now have a model and a reference model and during the training we're going to penalize the model in such a way to incentivize the probability of the model being trained increasing for Dublin relative to the probability that the reference model predicts for Dublin and likewise we're going to want to incentivize the model in a way such that the probability it predicts for cork should be reduced relative to the probability of the reference model so you can see how these pairs are being used and we're trying to increase the probability of the chosen answer and decrease the probability of the rejected answer really high level what we're doing here is imagine a training data set so you've come along with the model you've done some supervised fine tuning it's been trained on a trillion tokens there are lots and lots of data points here within um the data set and in that data set of course because we've trained it on the internet there are some bad answers some answers that you know people have written things on the internet that aren't correct and then there are some answers we consider good answers uh where maybe there's more of these but there's not enough more to statistically get the right answer all of the time so we have this data set body with bad answers and great answers and how direct preference optimization works is by using pairs of Answers by feeding it some pairs we bias the model away from Bad answers and we bias it towards great ideally great but certainly better answers so again to contrast this with standard training if we want to bias the model towards better answers with standard training the only way we can do that is by feeding more great answers so with standard training you have to keep feeding in more of these great answers the bad answers will still be there in the data set but over time these great answers will start to dominate as we train on more and more sets of the capital of Ireland is Dublin whereas by contrast in DPO we're penalizing the model for a bad answer and we're incentivizing it for a good answer now I'm giving a very simple example here just with the capital of Ireland but as it turns out when we look at these pairs on good of good and bad answers there are certain statistical as attributes about good and what we consider bad answers and those statistical attributes are then captured by the model and that helps us to generally drag uh drag the model away from worse portions of its training data set and towards better portions of its training data set and so when we go back to our graph here showing the frequencies you can imagine that in a data set of the worldwide web you again may have a lot of bad data and you have hopefully even more good data it might be hard to get rid of some of the bad answers it might be hard to overcome them rather by just feeding more and more data into the model until the right answers kind of win out and so direct Pro preference optimization gives us a different approach where rather than just adding in more examples with Dublin we specifically penalized the examples with cork so it's like we can almost remove the cork example or at least move the model away from the cork example and moving towards the good example I'll move now to talk about different data sets that are available for doing direct preference optimization there are two ones I'll talk about one is ultra chat and the other one is anthropics helpful and harmless the way to think about Ultra chat is a series of conversations that have been had in all sorts of models and there are conversations from Falcon models from llama models from MPT models and some of these best conversations which we would consider good data um and which also consider uh what also include bad responses we take those and we form Pairs and then we use that to train our model so this is a synthetic form of data it works well if you take very strong models largely the data comes from larger models like 40b or 70b Lama and then we use that to train smaller models in fact that's how the zephier model which is a fine TB of mistr works so um the authors of zephier which is a hugging face team they took the Mistral model and then they used an ultra chat data set which is generated by more powerful models and this provides um a strong compendium of what we can call Great answers versus bad answers and using that data set it was possible to fine-tune Mistral using um DPO in order to get an improved model called zeper one of the issues with the ultra chat data set is that because it uses other language models that are not completely permissively licensed that limits um the use of zere to non-commercial use furthermore in ranking which answers were great and which were bad GPT 4 was used and the terms of the GPT 4 Li don't allow you to use it commercially and so zier is a great demonstration of how you can synthetically produce strong data with large models that provides you with a good versus bad data set as graded by gp4 in this case and then taking that data set you can train these weaker models and basically point the weaker models away from their bad answers and towards their good answers I'll put up a link below to a data set you can use if you want to use the ultrat set the other data set that's available is helpful and "harmless from anthropic it's 160"rows about 40"type and 120set's been around for a while and it's been used to train Lama 2 um it's probably being used to train a lot of models and it does something a little different than just pointing uh the model from Bad answers to good answers it's also trying to achieve a level of safety and avoid the model spitting out uh perhaps s harmful dangerous type results just as an example one uh instruction set that you might see a type of instruction set you might see in the harmless part of the data set is as follows the prompt might be how do I do something dangerous and the chosen answer which is what we want to incentivize the model towards is I don't know that's an interesting question so it's kind of harmless certainly not helpful but it's kind of harmless and the idea is to pull the model away from giving out dangerous answers and then the rejected answer might be something like go and blow XYZ um and this clearly is some kind of a dangerous answer and this is what you would be trying to bias the model away from this is the data set that we're going to use today for demonstrating DPO I'm going to demonstrate it on Tiny Lama which is a 1 billion model it's a fairly small model which allows me to do the DPO training process fairly quickly I think in about 45 minutes I was able to "train on 16the overall data set and so the time for doing training on DPO it's roughly about twice the time that you need for doing supervised finetuning because you are running inference forward through two models the reference model which is duplicate of the original model and the model that you're training so you have twice the amount of forward passes and although you've got the same amount of backward passes probably is a rule of th your training in DPO is going to take twice the length as doing fine-tuning another caveat before I move to show you the notebook is that when you train a model to be harmless like this you're training the model when it gets a question to say something that's kind of benign and useless and while that can be helpful because commercially you don't want your model to be outputting harmless harmful or hateful uh content it also can get the way of the model statistically providing useful answers and it can affect the quality of useful answers that are generated and this has been a problem to some degree with the Lama 2 chat model particularly when using the system prompt the model is so uh it's so fine-tuned using reinforcement learning that it doesn't answer a lot of questions or it answers them in a kind of harmless uh but unhelpful way so that's a caveat to be aware of when you're choosing your data set just one last comment I've remembered which is that DPO it's a very direct way of in achieving what's called reinforcement learning the previous approach uh the two-step approach to doing this step of training was to first get a totally new model and train it to recognize the difference between good and bad answers so you would take the same data set you would train a brand new model just to get accustomed to what we think is a Chen answer and what's a rejected answer and then using that model you would run a loop on the model you actually want to train whereby the model you want to train would generate some answers and you would then run that and get a reward that says yes this should be a chosen answer or this should be rejected and then back propagate so you had the original model and then this reward model um and that required a lot of steps and made it particularly difficult uh except for the larger companies with DPO though this process is just done directly by having both the reference model and by having the model your training forward passing through both of them let's say for the chosen answer and then penalizing the probability of the chosen answer in a way that it will be incentivized to be increased if it's the chosen one versus the reference model or it will be incentivized to decrease the probability of the rejected one versus the reference model let me quickly go through the materials we need before we do the DPO training the first thing we need is a data set and it needs to be formatted correctly with the prompt then the chosen answer and then the rejected answer we're going to use anthropics helpful and harmless reinforcement learning data set it's presented with only Chosen and rejected rows uh rer columns so let's just take a look at the data you can see there's a chosen uh prompt here and the question is if you're going to steal from a convenience store and the assistant gives a harmless answer which is I really couldn't say and then the rejected answer is a more um Intense or bad answer same question same prompt but the assistant says uh it's good to consider the difference in human traffic at night and different reasoning there that you want to reject as I mentioned this isn't the right format so I have reformatted it here into a data set um you can purchase access to this if you like you'll find the link on the repo you'll see there's the chosen the rejected and the The Prompt columns just to go through here we have a prompt if you were to steal from a convenience store and then you have the Chosen and the rejected note that I formatted it for Lama um with the inst at the start and at the end and I've also put in an end of sequence token I don't really think this is critical because ultimately we're just comparing the probabilities of uh of the model being trained with the reference model and we're adjusting where there is a difference or rather to create a differ difference to increase the likelihood of the chosen response and reduce the likelihood of the rejected response so I'm not sure the end of sequence token is required but just for consistency I think it's good to format it like this the next thing we're going to need is a reference model I have a reference model here which is the tiny Lama model I said I would be training on this is a model that I have trained with supervised finetuning using the open assist data set it's available here um for you to download if you like and as I said it's a chat fine-tuned model tiny Lama is only trained on a 2K context length I did the fine-tuning using 4K context it helps a little bit with the 2K performance but it's not good enough to get the model to be a 4K data set so we'll be considering it a 2K data set even though that I call it 4K because I used 4K long or up to 4K long uh data set which is open assist and with that I think we're ready to move on and do some fine-tuning in the notebook you can get access to the notebook by purchasing it through the link below or you can purchase access to the full Advanced fine-tuning repository which now has not just DPO but scripts for embeddings function calling training long context uh quantization supervised fine-tuning and unsupervised fine tuning quite a lot of scripts in that advanced fine-tuning repo and quite a lot of members of it right now all right let's get started and here we are with the direct preference optimiz ation notebook I'm going to take you through it step by step so right up the top we are going to connect with hugging face this will allow us to push models and also if you're accessing a or fine-tuning a gated model it's necessary to do the login lately I've always been also logging in with weights and biases it allows us to view the data and it saves the data for the future so you have it nicely organized for runs this is really handy because I use run pad oftenly often and it saves the data even if I shut down the Pod so I've connected with weights and biases optionally if you have the model downloaded to Google Drive say you're working in Google collab you can connect Google Drive next we'll do the installation note that I'm installing flash detention this is going to be the V2 and that will accelerate the training process it speeds up doing the attention part of the forward pass uh I additionally now typically run this command here to get the Transformer Transformers environment information this is useful if you're ever troubleshooting and you need to post an issue on GitHub it tells you all about the platform you're running so we'll move on now to loading the model the model I'll load is the tiny llama model in fact let me just increase the size of my font here something like this I am not going to load a quantise because it's only a small model um so I've commented out the quantization I'm not using any rope scaling I've set the Dei device map to Auto which means that the layers will be put onto the GPU I'm running with an a6000 here it's got 45 48 gab of ram um and that easily fits in the total model which is about 2 gbt in size um in fact if it's run in BF 16 yeah it's about 2 GB in size because it has about billion parameters in tiny Lama you can see here that I'm using flash attention and this will reduce the memory requirements now you can load a me reference model by loading an exact copy but actually the trainer will handle that for us so I'm not going to load a reference model I'll just load this single model here next up I'll load the tokenizer I can run a quick uh test generation just to see that the model is working correctly which it is um note that I'm only going to train Lowa parameters here so I'll train a Lura adapter I'm not going to train any non Lowa parameters um typically I would list out all of the parameters in the model and turn on training for some of those in non Lura format but I don't do that here it's necessary if you want to train for longer context that's not what we're doing here so I'm just going to train the Lowa parameters which I'm going to apply in The Next Step so here I'm enabling gradient checkpointing and I have a function to print the trainable parameters I'm printing out the model just to show you exactly what I'll be training within the attention I'll train the Q KV and O and I'll also train in the linear lays the gate projection up down I'm not going to tr train the input lay armm which would not be done through Laura I'm not going to train the post detention L armm I'm not going to train the embeddings either although training those will be important if you want to train for longer contexts which is not what we're doing here so indeed you can see here the modules that I've chosen to train and I'm applying the adapter now to the model so that we are training the Laura config this results in US training a lot less parameters than we normally would if we were doing a full fine tune now Laura fine tunes generally perform just as well so hopefully we'll see that's the case here as well next up uh we'll set up this the tokenizer and pass PD in so I've just loaded the tokenizer here and I'm going to add a padding token you can use the unknown token as padding but since the supervised fine-tuning base model I'm using has been trained with pad token like this um I'm going to keep consistent here and use that same pad token okay so we'll continue on here and you can see that the special tokens are the beginning of sequence the end of sequence the unknown token and the pad token next up we need to set up some evaluation so we can evaluate the model before we do the DPO and afterwards and it's important here you set some questions that are relevant to the data set that you choose ideally they should provide you some Metric of how the model is performing we're trying to fine tune here to have a model that is more harmless so we're going to have to ask some ugly questions and then we want to see that it gives what we'd consider an an unacceptable answer before the fine tuning and gives hopefully a more acceptable answer after the DPO so I've set up uh some questions here um yeah unfortunately a lot of this kind of training involves ugly questions um which you will ask the model and when you run that base evaluation here it's bluntly answering the question which you may not want um if you want to have a model that is going to be better behaved or aligned but this provides us with a baseline then we can then that we can then compare after the training and hopefully this data set will have the effect that we intend okay so we're loading the data set that I just took you through it's the DPO um helpful and harmless reinforcement learning data set from anthropic so that's loaded and here you can see there is a prompt talking about dinosaurs so this is a more harmless question um but it ends with the user accusing the machine of not being able to read and the harmless answer is you can read question mark and rejected answer is a more aggressive retort which is there's a lot of stuff humans don't know okay so we'll move on and I've just got a test here where I tokenize check the token IDs you can see that a special token that's the beginning of sequence token is added at the start um so everything looks correct here now we're going to move on to the trainer I'm going to use an evaluation set and a training set and the evaluation set is the test split it's quite a large split I think it's got over "10running more quickly I'm going to reduce that so we'll only run with 960 of the test data set rows I'm going to run for 0.1 epox on the train data set the train data set is 160k so it'll be "out a 16on the training side and on the test side it's about 960 rows okay setting some parameters here uh the context length will be 2K which is the tiny Lama context length I'm setting the batch size to eight this does fit on an a6000 you can fit eight "batches of 2fit a bit more you probably could fit 12 I think and usually the way I set gradient accumulation is to have the batch size times the gradient accumulation be about 32 that means that basically you're doing 32 forward passes um before accumulating the updates and then passing them backwards through the model all right so moving on here to the training arguments mostly we have already specified them I'm going to do evaluation steps every quarter of the full run so there should be three in uh intermediate evaluation steps the same with save steps this will save the adapters the lower adapters in case the Run crashes I'll at least have the adapters saved I'm setting uh a scheduler here type to constant so that means I've got constant learning rate throughout the Run sometimes people will decrease that over time um I think with a small amount of data you can probably just do a constant um linear scheduler or sorry a con constant scheduler but you might consider linear or cosign if you're doing a longer run with a large amount of data now one of the most important parameters here is the learning rate typically this might be 1 E minus 5 for doing lower of fine tunes but with DPO it's found that you need to have an even lower learning rate it's recommended maybe 5e minus 7 I've got it slightly more here which is 1 eus 6 and I'll actually show you an example of where it diverges when I try doing 1 E minus 5 okay so with that um the trainer can be called you can see I'm passing in the model I'm not passing in a reference model because the trainer will create a copy uh automatically as a reference I'm setting the beta of 0.1 if the beta was Zero then it would basically ignore the reference model if the beta is 0.5 I think it's going to very strongly consider the model so um or sorry it could be more than 0.5 but the higher beta is the more that the reference model is considered I.E um you can think of the model you're training being more Tethered to the starting point um so it's recommended to have it between 0.1 and 0.5 I haven't played around a lot with this but I'm leaving it at1 then I'm passing in the training set and the test set and then the tokenizer and then I'm starting to train now this model has already been trained so I've gone all the way through a th000 steps um this training has been completed and I'm going to walk you through some of the results it's going to be easiest probably to look in weights and biases because we can see the full graphs over there and once we're done with the training uh we can plot so we can show the training loss and the validation loss really you don't see a whole lot of change in the model here uh the learning rate is very slow so with this uh relatively small small amount of data and such a low training uh learning rate there's very little change in the model over the um over the over the training period and indeed when I look here at the base model evaluation after the training unfortunately in this case the data hasn't been enough to give me much change or any change in the answer that I'm getting so still the model is not giving um a more restrained response to the more dangerous questions so we haven't achieved the goal here with DPO meaning that we would need to train perhaps for more uh more of an Epoch I've only trained for 0.1 epochs it's possible that simply the kind of nature of this question I'm asking here hasn't being covered in the training data set so it's not being affected statistically by any of the samples that we've run through in DPO so running for a full Epoch would be one way to start with this and maybe playing as well with having a little bit higher of learning rate so I'm going to show you the example I have with a little higher learning rate where I have 1 E minus 5 I'll show you how it diverges but also it actually gives you a sense for the effect it has in the model because it does start to change the answer so right here with the 1 E minus 5 everything is very same the only difference is that when I ran the training I had it at a learning rate of 1 E minus 5 Which is higher than 1 eus 6 by factor of 10 and indeed when I run that training and then I run the first sample here um you can see the answer is quite different now so when it's asked about killing it starts to give an answer that um is a bit more restrained it's um now asking that people's um human rights and emotions be considered it's giving a much more verbose answer trying to indicate more Nuance around the topic I wouldn't say that it's got the answer to as good of a level as we would hope for from a production level product but still you can clearly see that the answer is no longer just directly uh providing dangerous recommendation on this question but rather giving a little more Nuance let's move over and um take a quick look at the results in fact even just looking at the training and the eval loss which should be kind of slowly falling you can see in this case the train in loss starts to diverge um it's bouncing around pretty sharply because my learning rate is so high and the eval loss I wouldn't say it's getting terrible but it's it's going up a little bit but this kind of instability here indicates that probably I have my learning rate a little bit too high so what we'll take a quick look at now uh to finish off is the weights and biases training when you've connected up weights and biases it'll automatically generate all of these plots you can see here what the GPU was doing during this training session it was about 50 minutes um actually I trained on an a100 here when I was running with 1 to the E minus 6 and right up the top you can see some evaluation results it's showing four plots here that I'll explain a little bit um the two I want to focus on are uh the margins plot and the accuracies plot the accuracy plot is probably the easiest to understand I know it's small font here let me see if if I could increase actually just this yeah I think that helps so the accuracies tells us given given a chosen response and giving a rejected response what's the likelihood that the trained model will pick the chosen rather than the rejected so ideally you would like the model to have a high probability more than 50% of choosing the chosen Mo response and have a low probability of choosing the rejected one and so you can see here throughout the training there's some oscillation but at the end of the training in fact not at the end but at the 750th step you can see that um it's got a slightly higher than 50% chance of picking the chosen rather than the rejected so indeed in this case the model has been biased towards the chosen response but you can see the bias is not very strong because it's 5135 so it's only a little bit more than half and so you would need to take the model quite a bit further if um you wanted to have the bias um definitely move towards the chosen response the next graph to look at is the margins graph this is the difference in the reward for the chosen minus the reward for the rejected so ideally we want a positive margin here you can see that actually the margin is negative and this is an average across um the whole evaluation so there were about 960 RADS in that evaluation and um I think that's why it's possible to have a negative margin when you're averaging across all of those according to Value whereas the accuracies are averaging across according to the number that favor the chosen over the rejected so you can see that in any case we're not seeing a very strong bias of the model unfortunately we would like to see a positive margin that shows us a clear bias in a value sense and we'd like to see a clear bias as well in terms of the number of responses that are chosen over rejected and just for two more quick graphs you can always keep an eye on the training and the eval graphs the training loss should be going down over time as there should be similarity between all of the different samples we're trying to train on so this should be dropping very slowly you can see it looks pretty constant over this range of training and then eval um this number here should also be falling you can see here that it's gone from around 6929 to 6935 so there's really not any change here in the eval loss it looks like some change cu the Y AIS is very um compressed or very zoomed in on in fact we're just not seeing a whole lot of training within the short number of epoch before I go I'm just going to show you exactly how I set things up on runp pod um what I'll do here is use an a6000 so if I go to secure Cloud I should see a list of all of the servers that are available like this and this I think is probably one of the best values per unit of vram it's got 48 vram for 79 cents per hour um I'll before I click deploy using a pytorch instance I'm just going to make sure I have plenty of memory here um 50 GB is way more than enough because the model is going to be about 2 gab and I'll click continue and then I'll deploy so this is now going to deploy a p p torch instance and you can see I have an exited instance of an a100 that I was working on earlier and once this is booted up I'll be able to connect using a Jupiter notebook once the notebook uh is ready once the Pod is ready I'll click connect and I'll click connect to Jupiter I've just uploaded a dp. iynb notebook and I'll open it here in the screen and I'm going to go through the steps that I discussed earlier starting off by and logging in with my hugging face ID so here we have the hugging face ID then doing the installation of weights and biases and using my login for that so that we have logs so here's my weights and biases ID and next up we will do the installation so install all of packages necessary move on to loading the model here as I said not loading at quantize because it's a small model so there shouldn't be any issue with doing that and I'm going to prepare the model then for fine-tuning and stick with this selection here of Target modules for that fine tuning initiate the tokenizer and once the tokenizer is done we'll move towards setting up evaluation printing some evaluation results and then loading the data set that we're going to use for training now I'm going to make a little alteration here this time around I'm not going to trim the data set so I'm going to use the full eval data set which is "about 10slower training run because I'm going to use the full eval data set also here um I'm going to run for a full Epoch so that should be um an improvement on what was previously done now let's just just check here to make sure the data set is imported it looks like I have an error for loading up here and unfortunately I forgot to run this one cell here which was necessary so now that I've run cached here I'll be able to run through all of these again by the lower adapter set up the tokenizer set up the evaluation Lo the data set but this time not truncate it as I previously had done so that's why I've commented out here where I've reduced the range of the EV Val data set and then increased to one Epoch for the training so let's see and we have a learning rate schedular type of a constant I'm actually going to move back to the original um recommended learning rate well you know I might leave 1 E minus 6 but instead of using constant I think I will use linear here so the learning rate will decrease as we move through all of the epoch and you can see I've set up the results folder here for one Epoch and I'll go ahead and get started now to train the model it should set up awaits and biases "run you can see here it's got 10"steps to go through instead of 1it's going to be quite a bit longer as a training and I'm going to install my plot lib so I can plot the results at the end I'll do a quick evaluation this here I can remove can remove all of these and I will push the model to hugging face so that I have a copy of it so I'll just go ahead and push that adapter and additionally I don't need to upload any other trainable parameters because they're all included in the Laura adapter so I should be free to just merge and unload the model and then I should be free to push the tokenizer to hugging face Hub and push the model to hugging face hub um what I might do is push tokenizer do model which is a file in the original repo that file is needed to do gptq and also ggf quantizations so it can be handy to just push that as well so I'll scroll up here now and take a look at the training which should be underway and here the trainer is up and running and you can see we're now 17 "steps into 10about um it's about an 8 Hour run so fairly long even though this uh is quite a small model just a 1 billion parameter model so just gives you a feel for the amount of time if you're doing a full fine tune U rather if you're doing a lower fine tune on the model and here we can just click on the weights and biases and we should be able to pull up a copy of this run and you can see here training loss is progressing and I'm going to just go to overview and just put in here what the run is about one e minus 6 and we'll say one e but run tiny like this and that's it I'll post up later how the Run goes that folks is an overview of direct preference optimization it is a lot easier than doing reinforcement learning where you need to train a separate helper model still doing DPO direct preference op optimization it's not an easy feat and you need to pay a lot of attention to having a data set that's comprehensive choosing some questions for evaluation that allow you to tell whether your training is progressing or not you also need to be comfortable with chat find tuning a model which is a form of supervised fine-tuning if you want to get started with something a little easier I recommend going back over the videos for embeddings unsupervised fine tuning and supervised fine tuning with respect to this video I'll put all of the links on resources below and you can find out more about getting access to the scripts and data sets cheers 