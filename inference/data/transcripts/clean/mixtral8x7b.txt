hello guys welcome back to my Channel today we are going to talk about Mistral so as you know Mistral is a new language model that came out a few months ago from mistal AI which is a one of the hottest startup right now in Europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so let's review the topics of today the first thing I will introduce you is the architectural differences between the vanilla Transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks I will briefly review the KV cache because I want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of Innovations in the code especially when they use the X forers Library with the block attention so I want to guide you into understanding the code because it can be really hard for beginners to understand and find thems around there are some topics that are related to mistal but will not be covered in this current video because I already covered them in my previous video about llama and in particular I will not be talking about the RMS normalization the rotary positional encoding and the group query attention because I already um teach them in depth in my previous video on llama so if you want to know about them please watch my previous video on llama another the only prerequisite that I hope you have before watching this video because the topics we are going to touch are quite Advanced is that you are familiar with the Transformer model so if you're not familiar with the Transformer model and the attention mechanism in particular and in particular the self attention mechanism please go watch my video on the transformer in which I teach all this concept very thoroughly very in detail these are really a prerequisite for watching this video because the top the topics here are quite Advanced okay let's proceed further so let's watch the differences between the vanilla Transformer and mistal at the architecture level as you can see from the um image here which I built by myself using the code because they didn't release any architecture picture in the paper U the architecture of mistal first of all let's talk about some terminology when you have a model like this made up of many encoder layers plus linear and the soft Max we are talking about a decoder only model because this part this model here looks like the decoder of the vanilla Transformer you can see here except for the cross attention because as you can see here there is no cross attention when we have a model without the linear and the softmax we call it an incoder only model for example bir is an encoder only model because Birth has some heads at at the end which is one or more linear layers depending on the application but it self birth doesn't need a head because it can be used for multiple Downstream tasks so it's called an encoder only model because it resembles the encoder side of the Transformer because as you can see in the encoder side there is no linear and soft Max so mistal is a decoder only model and it's very similar if not equal to llama the differences between llama and mistal are highlighted here in red the first difference between llama and mistal is that in the self attention we use the sliding window attention and we still use the group query attention but and also the KV cache for inferencing but this is a rolling buffer KV cache and it's actually related to the fact that we are using sliding window attention so later we will see all these Concepts and the another difference is that the feed forward layer here instead of using the reu function that we used in um in the vanilla Transformer or the zigo function that we Us in Lama here in mistal we use the ceu function um and the feed forward is one in case of mistal 7B so the first model they released and it's it can be eight feed forwards uh networks in parallel with each other which are the experts of this mixture of expert in the case of mral 8X 7B um we will see later how it works so for now you just need to understand that mistal is made up of okay the input which are converted into embeddings then we have this block which is repeated n times and we will see that in the case of mistal is repeated 32 times one after another such that the output of each layer is fed to the next layer as input and the output of the last layer is then sent to this RMS Norm to the linear and to the softmax to produce the output of the model and um this is exactly the same as what we do with any other Transformer model usually we have many of this blocks here now in the code of mistal this part Here is known as Transformer block but it's also known as encoder block or decoder block depending on the in the contents of this um block here I will refer to it as an encoder block because if you look at it it looks like exactly as the block of the encoder side so it has a multi header tension OD and Norm a feed forward and other Norm the only difference is that the normalization here comes before the uh the the BL of the Feit forward and the self attention Okay let's move forward now let's compare the the two models so one is mistal 7B and one in mistal 8X 7B the parameter dim indicates the dimension of the this the dimensions of the embedding Vector so how big is the embedding Vector so each token is represented by an embedding Vector of size 496 Dimensions we have 32 of the encoder layers so this block here is repeated 32 times the head Dimension indicates as you remember in the multi-ad attention we have um each head is watching the entire sentence but only a part of the embedding of each token and this indicates how much how many dimensions each head will attend to in each um for in the multi-ad attention and the hidden Dimension here indicates the hidden dimension of the feed forward layer so if the in the case of the fit forward layer we have two linear layers one that converts the the dimension of the embedding Vector into the hidden size then another one that converts the hidden size back into the embedding Vector Dimensions so in the case of the mistal they are using as a hidden size 14336 usually this is a multiple of the dimension and it looks like it's 3.5 the dimension here the number of heads of attention for the query is 32 two while the number of heads for the K and V so the key and values is eight and they are not equal because of the grouped query attention so if you remember from my previous video on Lama in which we talk about the group query attention um in the very simple case of the group query attention we have the multiquery attention which means that only the query have the multi-ad while the key and V don't have the multihead attention uh which means that you may have eight heads for the query and only one head for the K and V in the case of the grouped query attention means that each group of query will have one um attention head for the K andv so in this case every four query have one attention head for the keys and values if this concept is not clear I describe it very thoroughly in my previous video on Lama the window size is the size of the sliding window that we used in the um calculation of the attention and we will see later how it works the context length is uh what is the context size of upon which it the model was trained upon uh and it's much bigger for the 8X 7B the vocabulary size is the same for both and then the last two parameters you can see here are related to the sparse mixture of experts and we will see later uh how it works but we just remember that we have eight experts and for each token we use two experts but later I will clarify how it works let's proceed further so let's talk about the sliding window attention but before we talk about the sliding window attention I need to review a little bit of the self attention mechanism so what is self attention self attention is a mechanism that allows the model to relate tokens to each other so tokens that are in the same sentence are related with each other through the self attention mechanism this is why it's called self attention because each token is watching other tokens of the of the same sentence and when when and this is means basically that the query key and values are the same metrix um so imagine we have the following sentence the cat is on a chair we have our query which is a matrix made up of six tokens each "token represented by 4which is the dim parameter that we saw before this is multiplied by the transpose of the keys which is 496 by 6 but it's just the query Matrix transpose because the query key and values are the same Matrix in the case of self attention this will produce a matrix that is 6X 6 because the inner two Dimensions kind of cancel out and the outer Dimensions indicate the dimension of the output Matrix here now what is the values what are the values in this Matrix representing the first value here indicates the dot product of the first token with the first uh the first row of the query with the First Column of the keys so basically the dot product of the embedding of the first token with itself the second value here indicates the dot product of the first row of the query Matrix with the second column of the key Matrix here the transpose of the keys Matrix here which basically means that it's the dot product of the embedding of the first token so the with the embedding of the second token which is cat and etc etc for all the other values don't concentrate too much on the values because all the values I put here are random and also the fact that these numbers are less than one it's not necessary because the dot product can be bigger than one it's not a uh condition of the dot product usually in the formula we also normalize here we divide by the dimension of the DC DK basically is the um the size the part of the embedding to which this particular attention head will attend to but let's pretend that we only have one one ahead so DK is equal to D model so basically this head will watch the full embedding of each token okay you usually we train Auto regressive models so language model is an auto regressive model it means that the output depends on the previous the next token depends only on the previous tokens and this is why we apply a cal mask Cal mask means basically that in the attention mechanism we don't want to relate a word with future words so words that come after it but only with words that come before it so for example we don't want the word the to be related to the word cat because um the word Cat come after the word the but on the other hand we want the word cat to be related to the word the because it comes before it and for this reason we apply this Cal mask because the attention mechanism uses the soft Max function we can see here the soft Max function basically um will transform all this minus infinity into zero because the formula of the soft Max has at numerator an e to the power of X and when X goes to minus infinity e to the power of minus infinity will go to zero so this is why we apply a mask in which we put all the values that we don't want all the interactions that we don't want between tokens we just mask them out by replacing them with minus infinity so that when we apply the softmax the softmax will take care of um transforming them into zeros okay also the softmax will do another thing because it will not only convert this minus Infinities to zero but it will also modify the other value for each row such that they sum up to one so as you can see now these values here they don't sum up to one for each row right because this is 0.2 0.1 and 0. they don't sum up to one but the soft Marx will convert the minus Infinities into zero and the remaining values for each row such that they sum up to one now let's talk about sliding window attention so we applied the causal mask to hide the interactions between the words a word and all the future words but with sliding window attention we also don't want the word to watch other words that are outside its local context what do I mean by this in the previous case when we only applied the Cal mask the word chair for example was being related to all the previous tokens as you can see so the the token chair here is related to itself but also to the A on is cat V so it could watch basically all the sentence but in the case of sliding window attention we don't want the word chair to watch words that are further than the sliding window size from itself so uh the sliding window size in this case is three so tokens that are distance more than three from the word uh we are considering so the word the chair should not be related to the word is because the distance is four and the word a should not be related to the word cat because the distance is four and of course we still want the mask to be Cal because we don't want the model to uh each token to watch future words because we are training an auto regressive model so the sliding window attention basically reduces the number of dot products that we are performing and this will improve the performance during the training and the inference because as you can see when we only apply the Cal mask we are performing all these dot products you see here but with the sliding window attention we are performing less dot products because all the other will be masked out sliding window attention however may lead to degradation of the performance of the model because as you can see here the word chair and the word the are not related to each other anymore right so the information uh will not be conveyed from the word the and the word chair the word chair will only be related to other tokens that are belonging to the local context of this particular token so only the tokens that are in the same in inside this sliding window this may be if this window is too small it may reduce the performance of the model but it may also be beneficial because for example imagine you are reading a book you don't care about relating the word in chapter five with the words in chapter one because most of the books they they could be talking about totally different things and you don't even care about relating these two tokens but for sure you want to relate the tokens uh in the chapter five with other tokens in the chapter five because the local context matters but I want to introduce you the concept of receptive field because when we use sliding window attention even if the word chair and the are not related to each other actually because in mistal and in all Transformer models we use multiple layers of encoders we will see that the information so the the word the chair and the the will still be kind of related to to each other not directly but indirectly in a concept that is very similar to the receptive field of the convolutional neural networks so let's talk about the receptive field as you remember in convolutional Neal networks we have um a mask a Kel that we run through an image so imagine this is our original image uh this one here and we run a mask that is a kernel that is 3x3 this one here when we run a kernel it will produce an output feature so for example this feature here this is the output produced by applying the caral to the first 3x3 grid here this value here the second value here in yellow it will be produced when we will move our kernel to the next group of 3x3 pixels so let me draw let's use the pen so this value here will be produced when we will move our kernel in this grid here and uh this value here is also a an output feature of a convolutional kernel that is 3x3 applied to this layer two so this is a 3X3 kernel that is applied to this layer two so apparently there is no connection between this one this pixel here and this one but because this this uh output feature depends on a kernel applied in this grid and this grid includes this feature here which depends on this pixel here we can safely say that this feature here depends indirectly also on this feature here even if they are not directly related to each other and this is the concept of the receptive field so basically one feature of the convolutional neural networks can watch a much bigger receptive field uh down upward in the layers because of this um sequential application of Kels in the convolutional Kels let's see how this concept is related to the sliding window attention now now after we apply the soft Max to the mask that we have seen before as I told you before all the minus infinities are converted into zero and all the other values are changed in such a way that they sum up to one so let's go back as you remember here we have the minus Infinities here here here here here and here so now we apply the soft Max and it will become zeros zeros here also let me okay all the zeros here all the zeros here and all the other values are changed in such a way that they sum up to one what is the next operation that we do in in self attention we then take the output of the softmax and multiply it by the V Matrix so let's do it uh the V Matrix is basically the same as the initial sequence because I told you this is self attention so the query key and values are the same Matrix so this means let's analyze what happens by hand when we do this multiplication so let me change to the pen okay the V Matrix here is um is a sequence of tokens where each token is Vector "represented by 4can say that it's the output of the self attention if you watch the dimensions of these two matrices so it's a 6X 6 and the 6X 496 the output will be another Matrix that is 6X 496 so it will have the same Dimension as the V Matrix and also as the qu and the query Matrix because they have the same dimensions so it will be six tokens as output okay let's analyze what is the first dimension of the output this one here so this uh first value of the output so the value on the row one column one of the output Matrix will be the dot product of the first row of this Matrix here so this row here with the First Column of this Matrix so the First Column we can see here and as you can see most of the values here are zero which means that all the rows from the one to five sorry from two to six will not be used but only the first row here only the values of the first row will be used because if you remember the dot product is the First Dimension with the first dimension of this column and the second dimension of this row with the second dimension of this column the third dimension of this row with the third dimension of this column and then we sum up all these values so this first value of the output will only depend on the first token of the V Matrix you can see here let's check the second one the second dimension of the the the first dimension of the second row of the output Matrix will be the dot product of the first row of this Matrix here with the First Column of the V Matrix but most of the values are zero which means that this uh Dimension here and all the dimensions in this row will depend only on the first two tokens of the V Matrix and we can say the same for the third let's analyze the sixth one here so the first dimension of the sixth to row of the output Matrix this value here comes from the dot product of this row and the First Column of the V Matrix but most of the values at the beginning are zero which means that it will only depend on the uh four five and sixth token of the V Matrix and so will be all the dimensions here because in each column uh whatever the column we use from the V Matrix the first values will always be multiplied by 0 0 0 so it will only use the values in these three rows here so we can safely say that the sixth token of the output Matrix we of this self attention mechanism will be a vector that will only depend on the last three tokens of the V Matrix and because we are talking about self attention the V Matrix is equal to query Matrix so we can say that the output of the self attention is a matrix that has the same shape as the input sequence but where each token now captures some more information about other tokens which tokens depending on the mask we have applied so our mask says that the first token can only watch itself so the first output token will be an embedding that will only depend on itself the second token will only depend on the first two tokens the third output token will only depend on the first three tokens the fourth will depend on the token number two because the first token is not used the token number two the token number three and the token number four etc etc until the last here the last token will depend only on the last three tokens because the first three tokens are masked out and this is the importance of the mask that we apply in the self attention mechanism this concept that I show you now is very important to understand the rest of the video so please if you didn't understand it you can take a little pause you can try to do it by your by yourself because it's really important that you understand how the self attention mechanism works with the mask okay now that we have seen this concept I want to introduce you to the next one so as we saw before the output of the self attention mechanism is another Matrix with the same shape as the query Matrix in which each token is represented by an embedding of size 496 but each embedding now captures information also about other tokens according to the mask and if we check the um this mask here so the output here we can safely say that the input of our slide uh sliding window attention was the initial uh sequence D cat is on on a chair but after applying the self attention the first token is now related to itself the second token is related to itself and the token before it the third is related to the token before it and the one also before it the last one only depends on the previous two tokens Etc according to the mask right now what happens if we feed this one because as you know in the Transformer word and also in mistal and also in Lama we have many layers of encoders one after another which are also called Transformer Block in the code and the output of each layer is fed to the next one so this is the first layer of the uh Transformer so we take the input sequence and we feed it to the first layer which will produce a list of tokens where each token now captures information about other tokens but this will become the input of the next layer where we we it will produce an output this output I will prove you that will capture information about even more tokens even if the SL in window attention says that they should only be able to watch the previous two tokens because the sliding window size we chose three as a sliding window size uh I want to prove it so imagine this is the output of the first layer so it's a a list of tokens that capture information about other tokens and it's the the the metrix that we built in the previous slide let's use it as an input for another layer of of the encoder so we multiply the query mul um we multiply the query and the transpost of the keys which will produce a matrix like this one in which each token is not only one token but it's capturing already information about multiple tokens right according to the mask so I'm I'm taking this one and this one will become query key and values so if we multiply the query by the key it will return a metrix like this so the first token only depends on itself the second one depends on himself and the previous one so the embedding of this token captures information about two to tokens and the Ming of this token capture information about three tokens Etc let's try to do the multiplication again so we have that our V Matrix is again a list of tokens and the output will also be a list of tokens but each one will capture information about other tokens okay let's analyze the dot product here so the first value of the first row so the first dimension of the first row of the output Matrix will be the dot product of the first row of this Matrix here so this row here with the First Column of this Matrix here so this column here but because of this Cal mask with sliding window attention mask that we can see here it will the output will only depend on the first row of the V Matrix but because the V Matrix is a matrix that is made of these tokens here it it will only depend on the word the so as we can see here the output of the second layer only depends on the word the and so will be this second one so dick uh let's check the fourth token for example here this one let's check this fourth token here so this value here will be the product of the fourth row of this Matrix dot product of this row with the First Column of the V Matrix so this column here but the first token will not be used because it's we are multiplying it with zero whatever value we have here we will not be using it we are using the second token the third token and the fourth token and each token actually they are aggregating this um this values here this token here is already uh aggregating the value of two tokens which is D and cat so this embedding here is already about talking about D and Cat and this uh token here is talking about is aggregating the information about the cat and is so the cat and is and the fourth token is aggregating the information of the cat is on so cat is and on because as we saw before the fourth token here cat is on which is the result of the previous self attention that we done so this output value here will depend on three tokens that already include information about other tokens so this value here will aggregate information about the union of all these tokens so it will for sure depend on the word the because it's included in the second token we are multiplying it with it for sure it will include information about the word cat because it's included in this token as well for sure it will include information about is because it's included in the second value we are multiplying it with and for sure it will include about the token on because it's present in the the fourth token of the V Matrix for with which we are app multiplying it because this value is not zero so as you can see after applying another layer of the encoder the fourth token now includes another token in in its information before it was only uh including these three tokens but now it also depends on a new token which is the word v and we can prove the same for the fifth token and the sixth token so at every application of the encoder layer one after another we keep increasing the number of tokens that get uh accumulated in these thought products and I made a notebook in Pon to visualize this so if you look at my GitHub repository uh you will see this notebook called the sliding window attention in which I help you visualize this process and I also share the code on on how I do this self attention basically I represent each token as a set so each each token instead of being represented as an amending as a set of all the words upon which that token depends depends then I apply the sliding window attention which basically means that I take the two tokens that the from the sequence and I accumulate I make the union of the two sets they contain because I am multiplying two vectors that already include information about multiple tokens so what is the output is the union of the two sets when I multiply by V I do the same thing and I can visualize it so after we apply the first layer we will see that the input of the first layer is just our normal sequence so the cat is on a chair the output of the first layer will be another sequence that in which each position includes information about multiple tokens depending on the mask that we have applied and I also show the mask that we apply after we apply the second layer we can see that the information increases so this last token now is not watching only the previous three tokens but the previous four tokens uh sorry not only the previous two tokens but the previous four tokens so every every uh step we do with the sliding window size of three we include two tokens at every layer and uh here I show it for five layers but it's not necessary because after a while the the sequence will reach the maximum length if you want you can increase the sequence length here by including more tokens so this is the concept of the um the receptive field applied to the self window attention so basically with the sliding window attention we are not uh directly connecting connecting two tokens with each other but if we apply multiple layers after one after another this information will get will get captured by the embedding in successive applications of the layers such that the last layer basically will be able to watch all the sentence even if it's very long and this is actually uh shown by the um myal paper in this picture you can see here so basically this is our input sequence so let me write so this is our input which is a the original sentence so the cat is on a chair the fourth token of the first layer so this is the output of the first layer so layer one one uh we have see that the fourth token here depend with a sliding window size of four this will depend on the itself on the previous token on the one before and also this token here and it will produce um this uh this embedding here in the fourth position which includes information about the previous token as well but then this will become the input of the next layer which is the layer number two and this will produce an embedding at this position for example that will depend for sure on the previous four tokens because the sliding window size is four but because for example this token here is already the aggregation of the previous four tokens it will actually multiply the visibility of his sliding window so this token here is not related directly to the first one we can see here but indirectly through the uh this intermediate token we can see here I hope this um I hope this concept is clear if it's not clear I try I I I recommend using my notebook so that you can experiment by playing with multiple sequences and you can see how the information flow will go through all the layers all right let's talk about our next topic which is the K cach because I want to introduce the KV cach which I already explain in my previous video on llama but I want to introduce it again and review it because I want to introduce later the rolling buffer cach so let's start start by talking about first of all how we train language models because this is needed to understand the K cach so uh the language models are trained using what is known as the next token prediction task so given a prompt the goal of the language model is to predict what is the next token that makes sense with the prompts that we have given and imagine we want to train a language model on Dante alig's poem uh Divine Comedy and in particular we will training it on a line that you can see here in um this one in English so love that can quickly seize the gentle heart how does it work we prepare an input for our language model which is the the line that we want to teach it with a token prepended called start of sentence and then we build the target which is the same line but within token at the end called end of sentence we run the input through this Transformer model it will produce an output sequence so as we saw before the in the output of the selfa ension is another sequence with the same length as the input sequence but the embedding is modified in such a way that each token capture information about other tokens and this is what we use uh to actually train a model so if we feed the model with with nine tokens the model will produce nine tokens as output and how does it work basically the model will learn a mapping between input and output such that if we give to the model as input the token start of sentence only it will produce the first token as output which is the word love if we give to the model as input the first two tokens so start of sentence love the model will produce the two tokens as output so love that it will feed the model as input three tokens so start of sentence love that the model will produce love that can so when we train the model we train it like this we prepare the input like this the target like this we calculate the output we calculated the loss using the cross entropy loss and then we run back propagation and this is done in all one step when we do the inference we do it in multiple steps so when we do the inference at time step one we feed the model only the first token so the start of sentence and the model will produce the output love then we take the output the last token of the output and we prepend it to the input which becomes the input As Time step two so it becomes start of sentence love so the motel will produce love that we take the last token of the output and we prep append it to the input for the temp step three and this will become the new input which will produce love that can then we take the last token of the output and we append it to the input for the time step four so it will become the new output will become love that can quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the last token output by the model because we already have the previous one but of course we need to feed all the previous tokens to uh to to the model which is belonging to the prompt because the model needs to access the prompt to understand which token to produce next so for example we cannot produce the word gen only by giving the word the we need to give all this sentence to produce this output gentle here but at the same time we are only interested in the last word gentle and this is the reason we introduce the K cach because the K cach allow us to reduce the computations that we are doing by only producing one output at a time the one that we need but um without doing all the intermediate computations for all the other tokens that we never use so basically basically when we want the word heart we don't want to produce the output for the word love that can quickly seize the gentle because we already have them in the prompt we don't need to produce all these tokens we just want to produce the output for the token heart so we want to reduce the computation that we are doing let's see how it works now in the self attention mechanism you know that we multiply the query which can be thought of as a list of tokens where each token is an embedding of size "4becomes the is Multiplied the transpose of the keys are multiplied by the queries to produce this Matrix here and then we multiply by the V Matrix to produce the output of the self attention you can see here let's do this one token at a time so when we inference a language model we start with our first token which is the start of sentence this is one token represented by an embeding of size 496 we multiply it by the transpost of the keys which is again one token it's because it's a self attention so uh the query the key and the value are the same Matrix so this is just the transpose of the query basically and so it's a column vector and it will produce a 1 by one Matrix we multiply by V and it will produce an output token we take this output token we send it to the linear layer and then to the soft Marx to understand which token this corresponds to in our vocabulary we take this token from our vocabulary and we append it to the query for the next inference step to the keys and the values and then we compute again the product of the query multiplied by the keys we multiply then the result by V and it will produce an output made up of two tokens because we have two tokens as input it will produce two tokens as output but we are all interested in the last token so we take this output token two we send it to the linear layer then to the soft Max this will result in what token is corresponding to in our vocabulary we take this token from our vocabulary we append it for the next step to the query key and values we do again this iterator this process and then we take the last token as output you can see here we send it to the linear layer then the soft Marx we understand which token it corresponds to we append it to our query key and values and then we compute again the self attention but we already start to notice something because first of all we in this metrix here which is the result of the query multiplied by the transpost of the keys we have a lot of dot product at each step that were already computed at the previous step let me show you at the time step four we are Computing all these dot products as you can see at the time step three we already computed this uh this dot products and the time steps four we are Computing them Computing them again as you can see these dot products here so and the second thing is that usually when we uh we deal with the language model we have a cal mask so we do not even care about Computing the dot products that we see here in the Dark Violet because they will be anyway masked out by the mask the by the Cal mask that we apply because we don't want the first token to watch the token number two the token number three the token number four we only want the token number four to watch the previous one so the token number four should be related to itself the previous one the token number two and the token number one but not the opposite and also we don't want to produce all these output tokens because we are all interested in the last one we are all interested in knowing what is the last uh token uh produced by the attention so that we can send it to the linear layer and then to the soft Max to understand what is the word corresponding in our vocabulary so that we can use it for the prompt to inference the next token again so now let's introduce the kave cas and how the K cach solve this problem what we do with the k cache again we start from our first step of the inference so we start from our start of sentence token which is Multiplied so the query is only the start of sentence token we multiply by the transpost of the keys this will produce a 1 by one Matrix here then we multiply by the V and it will produce our uh first token as output we send it to the linear layer then to the soft Marx then we know which token it corresponds to now in the k cache instead of appending this new token that we have produced as output to the query key and value we only append it to the key and the value and replace entirely the previous query with this new token so before without the cach we were appending the every output token so the last token of the output to the query key and values but in with the K cach we don't pend it to query key and value but only to the key and values and we only use the last output token as query for the next step so if this is the first the output of the first step so the output corresponding to the Token start of sentence we take it we use it as query for the next step but we append it to the key and values and this is why it's called KV cache because at each step we are keeping a cache of the previous K and V but not for the query because we are entirely replacing all the queries with the last token anyway this will produce a product so this this Matrix multiplied by this Matrix will produce a matrix that is 1 by two we multiply it by V and we will see that this produces only one token as output then this we take this token we send it to the linear layer to the software then we know which token it corresponds to then we use it as query for the next iteration but we append it to the only the K and the V Matrix this will produce a 1x3 matrix um which is then multiplied by the V which will produce the uh this output token this is the one we are interested in basically then we use it as query for the next iteration but we append it to the K and V etc etc so as you can see at the fourth step of the inference we are producing only the last row that we were interested in when we didn't have the kvk so let me show you this is the fourth time step with the KV cach let's look at the fourth time step without the KV cach as you can see we are only producing this uh row here this is the only one we are interested in to produce this last token so with the K cash basically we reduce the number of computations that we are doing at every step because the some of the dot products we have already done in the previous steps and we only produce one token as output which is exactly the one that we need for predicting the next token okay now let's talk about the rolling buffer cach so since we are using the sliding window attention with a size of w and in the examples I show you before I was using a sliding window size with a SI with a size of three we don't need to keep all the possible K and V in the cach but we can limit the K and the V on only to W tokens because anyway we will not be Computing uh attention outside of this W window so we do not need imagine our window is 10 tokens we do not keep the "previous 1attention will only be calculated on the previous 10 tokens so this is the idea behind the rolling buffer cach let's see how it works imagine we arrive at the token eight of inference using the KV cach if we have a KV cach and we are using the sliding window size of four for example we will see that query as query we will use the output of the previous step and as key and values we will use the entire cache which is made up of eight tokens but with because of the uh mask that we are using with the sliding window attention we are not interested in the computation of these dot products because anyway they will be masked out because the distance between this token and this token is outside of the sliding window attention so we are not interested Ed in this calculating these dot products because um we will not they will be masked out and secondly we are not interested in keeping this one because anyway because this these values will be masked by our mask for the sliding window attention which basically will result in zeros here uh we are we do we do not care about producing these first four rows in the value Matrix because anyway they will be multiplied by zeros so they will not contribute to the output token so here you have to imagine that let me draw here you have to imagine that the mask will take care of making this one zero this one zero this one zero this one zero and this one will be a DOT product this one will be a DOT product this one will be a DOT product this one will be a DOT product so whatever value there is here whatever value there is here here or here will not contribute to the output of this token because anyway they will be multiplied by zeros here so we do not need to keep this value also in the V Matrix or in the K Matrix because they anyway they will not be used by the the the sliding window attention so that's why we can limit the size of our K and V uh cach only to W tokens where W is the size of the sliding window attention that we are using now let's see how this rolling buffer cach was implemented so basically rolling buffer cach is a way of limiting the size of of a cache to a limited size in this case W so imagine our W is only four imagine we have a sentence the cat is on a chair and we want to use it or for our k cach at the first inference using the KV cache we will add the first um the first token to the KV cache then we will add the second one the third one and the fourth one but now the KV cach is full how do we proceed further basically we keep track of where we added the last item using a pointer that we keep track of and when we will arrive at the next token which is the token a we basically replace the oldest value here starting from the beginning and we update the value of the right pointer but now how do we uh go back because now the order of the tokens is not matching the sentence because as you can see now the the cash contains a cat is on but this is not the order in the original sentence in the original sentence the order should be cat is on a so what we do is we do the unrolling or un rotation and how do do it basically because we kept track of this right pointer we just need to take all the values after the right pointer and then we put the values from zero to the right pointer itself so all the values after the right pointer and then all the values before the right pointer and this is how we un rotate and this operation is done in the code in a function called un rotate you can see here uh which basically will have this condition so if the cach is not full we can just ignore the unfilled item so if the cache in is in this situation then we take all the values from the zero up to the right pointer if the cache is full then we take the value from zero up to the uh the the value of the right pointer and if the value of the right pointer is already overwriting some value then we need to AR rotate and this is done in the uh third condition here so we take all the values after the pointer and then the value up to the pointer and this is how we un rotate this uh buffer cash okay let's talk about another concept that is very important which is chunking and prefeeding basically when we generate text using a language model we use a prompt and then we use this prompt to generate future tokens when dealing with a Cy cach we need to build up this KV cache so we need to add the tokens of our prompt to the KV cache that so that we can then exploit this KV cache to build new tokens future tokens now the prompt is known in advance right because because it's the input of our user it's what you ask to CH GPD for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so it's known in advance so we don't we don't need to generate it okay so what we can do is we can prefill the KV cache using the tokens of the prompt but there are many ways to do it like we were doing before when I was teaching you about the K cach we work with one token at a time so one way to um to add the tokens to the K cach is to add one token at at a time but this can be very time consuming because imagine you have a very large prompt which happens with retrieval augmented generation which we have very "big prompts like 5even bigger so this if we add one token at a time it will mean that we have to "take 5Network which is can be very time consuming and also doesn't exploit our GPU very much the other way is to take all these tokens and feed them all at once to the model but that may be limited by the size of our GPU because "imagine we have 10prompt then maybe our GPU cannot even "hold 10"hold 4depending also on the W size of the attention sliding window attention that we have chosen the solution in this case is to use chunking basically we divide our prompt into chunks of a fixed size and this size is equal to W which is the sliding window attention size so imagine we have a very big prompt and we choose a sliding window size of four for the calculation of the attention and imagine that the prompt is this one so can you tell me oops can you tell me who is the richest man in history the way we work is this basically we take our first chunk of the prompt so because we chose a sliding window size of four we also will choose the chunk size to be four so we take our first token of the prompt so can you tell me and we compute the self attention in the attention uh self attention in the first layer of the model how do we build the attention mask basically as queries we take all the incoming tokens in this chunk so as this is you can think of this column as the queries and this column as the keys and this is the result of the query multiplied by the transpose of the keys plus the mask so our query we take the first incoming chunk and as keys I will show you later we take the current content of the K cash but initially it it is empty plus the incoming tokens of the current chunk and this is made it this is made for a very specific reason that I will show you in the next step so in the next step basically we take the current chunk which is the tokens who is the richest and we aggregate it with the content of the K cach using the tokens of the previous chunk so let me go go back at the first step of this prefilling we take the first chunk of the prompt so can you tell me we calculate the attention mask using as query the first four tokens and S Keys as the con as keys and values the content of the K cach which is empty plus the tokens of the first chunk and then we update the content of the k cache using this uh the tokens of this chunk after we have computed the attention so at the next step the K cach now contain the previous the tokens of the previous chunk so can you tell me but now the current chunk has become who is the richest so as query again we take the tokens of the current chunk but as keys and values we take the the content of the KV cach plus the tokens of the current chunk why because uh as you can see when we were doing token generation when I was teaching you the KV cache we first add the last output token we add it to append it to the K and the V and we use it as the query for the next iteration this is not what we do here here we first calculated the attention and then we update the k cache and when we use the when we build the query the query we use only the tokens of the current chunk and as key and values we take the content of the k cache so the content of the previous chunk plus the tokens of the current chunk why because imagine if we didn't do we didn't use the content of the previous chunk what would happen is this we would have a attention mask that is only comprised of the tokens of the current chunk so it would be only limited to this metrix here let me draw it so only this Matrix here but if we only use this Matrix here the word who would not be able to to would not be related to the word me tell and you even if with the sliding window size they should be able to watch each other so because we want to relate the current chunk to the previous Chunk we basically take uh as a key and value the content of the K Cas plus the tokens of the current chunk so that we can build this attention between chunks otherwise this attention would not be built and as query we always use the tokens of the current chunk let's review how this me mechanism is built in the code so basically the prefilling is done in by chunks there is the first chunk and then there there are subsequent chunks and finally there is token generation after we have prefilled our K cach with the prompt during the first prefill which means that we are doing it for the first chunk of our prompt as attention mask we only consider the size of the incoming tokens in the current chunk but for any subsequent chunks so after the first chunk as to build the attention mask for the query we just use the size of the incoming chunk but for the K and V we use the size of the KV cache which is this one so cashed s you can see here plus the size of the current chunk which is this s variable you can see here and for token generation we do the same system that we did before when I was teaching with the K cach so one token at a time we take it we append it to the key we append it to the value and we replace the the query with the previous the the output token from the previous step so the last Chunk in our case will be the tokens men in history and what we do is basically we take the the the current chunk so men in history which becomes the query while the key becomes basically the previous chunk plus the tokens of the current chunk uh so the who is the richest plus the tokens of the current chunk so men in history and the reason we do it because otherwise the word in the current chunk would not be able to be related to the word of the previous chunk which is necessary okay guys let's talk about sparse M mixture of experts so mixture of experts is an example technique in which we have multiple expert model which each of these model is trained on a subset of the data such that each model will specialize on a subset of this data and then when we produce the output of this mixture of experts we take the output for for each of these experts we combine it usually by using a weighted sum or by everying to produce one single output in the case of mistal we do not talk about only mixture of ex but we talk about a sparse mixture of experts because we have many expert model but we only use some of them let me show you in the case of mistal we have eight experts which are present as the feed forward the layer so after we calculate the self attention as you remember we have this Feit forward Network in the case of mistal 8X 7B we have eight Feit forward layers we have to think of them in parallel and the gate is a function that basically will decide for each token which expert so which feed forward Network should be working with that token and it will choose two feed forward Network for each token it will run the token through these feed forward networks will take their output and will wait it according to the Logics this gate produces to produce a weighted sum which will become the output of the self attention for that particular token let me show you with an example so uh this is the architecture of mistal as you can see we have the input of this um encoder layer we first run the self attention using the sliding window attention and the KV cach etc etc then we run the normalization and finally we have this gate function here which is basically just a linear layer that will produce logits eight logits which will be values let's say let's call them score values for our expert the two best performing experts so the two highest score will indicate which expert that token should work with then we run each token in it in their own two best performing experts then we take the output of these two experts we combine it with the weight what is the weight basically the uh logits produced by the gate are suppose eight values here yeah I draw only four because I don't have space but you imagine you have eight values then we take the top two so 1.5 and 3.4 in this case these are the two exper through which we will run the token we take the soft Max of the two best performing values this will be the weight that we'll be using for the weighted sum and basically why do we do it so the the why do we do it uh because uh by using sparse M of experts we can have many expert model but during inferencing only two out of eight will be activated so as you remember the feed forw network is basically two linear layers so the linear layer can be thought of as a matrix multiplication of a weight Matrix with the input so if we didn't use a sparse mixture of experts we would run the token through all the eight experts which means that we need to compute eight Matrix multiplications but by using sparse mixture of experts for each token we are only doing two Matrix multiplications which makes the inference faster but at the same time allows us to increase the power of the model and the parameter of the model because we are only using some parameters for um a subset of the tokens so some tokens will use the expert number one some tokens will be used the token the expert number two and three some to tokens will be using the expert number eight and the three or some other for example the six and the four etc etc so we are not using all the expert for each token but only two of them this makes us this allow us to have each expert um specialized on a subset of tokens for example imagine the model has been trained on multiple language what could happen is that basically some ex some experts so some feed forward networks are specialized on Japanese tokens some feed forward n are special on English tokens or some it could also happen that some are specialized in verbs some are specialized in nouns some are specialized in objectives etc etc so this is why we use mixture of expert because we want to increase the size of the parameters of our model so the mo model becomes more powerful at capturing information but at the same time we don't sacrifice on performance because we only use a subset of the experts for each token and this is the implementation as done in the code so as you can see in the case of mistal 7B we have as feed forward just a feed forward Neal Network which is two linear layers um in the case of mral 8X 7B it's not only one feed forward network but it's eight fit forward Network so this as you can see it's the uh it's an array of eight fit forward networks with a gating function which is just a linear layer which converts from the embedding size to eight which is the number of experts so it produces for each embedding so for each token it produces logits which indicates for which um expert this uh token should run through and it will run through them to the top two experts so the two two experts with the top logic score okay why we apply the soft Max after selecting the topk expert so as I show you uh here we have the getting function that produces some Logics we select the top two logits to understand which expert we should run through our uh token and then we take the score of the best two performing uh experts and we take the soft Marx of them to to create the weights that we will use to create the weighted sum but why we take the soft Marx of the two best performing instead of taking the soft Marx of everyone well the first problem is that if we take the soft Max of all of the logits then the two best performing may not sum up to one which is um which is a condition that we need in case we want to train multiple models and compare them because I'm pretty sure that the guys at mistal did not only train one model maybe they trained multiple models with multiple hyper parameter maybe they tried with four mixture of four experts but also with three experts or two experts then they choose the best one so if you want to compare models you want the weighted sum to always perform the sum of the wids to be only one otherwise the output range may change from model to model and usually it's not a good idea to have the range of the output to change from one model to the next so to keep the range of the output stable they apply the soft Marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so let's talk about it when we have a model that is too big to fit in a single GPU we can divide the model into groups of layers and place each group of layers in a single GPU for example in the case of mistal we have 32 layers of encoders you can see here one after another I didn't do all 32 of them you just think that this is layer from one to eight this this is from 9 to 16 from 17 to 24 from 25 to 32 and we put each group of layers in a different GPU so we have four gpus the the way we inference a model like this is as follows so we have our input we convert it into embeddings and we run it through the first eight layers in the first GPU the first GPU will produce an output which will be the output of the E layer we transfer this output to the second GPU and we use it as input for the ninth layer then we run all the this input through all the layers one after another until it it arrives to the layer number 16 which will produce an output we take this output we move it to the next GPU so it will become the input of the layer number 17 and then we run iteratively to all the layers until the layer number 24 which will produce an output we move it to the next GPU we run it through iteratively until the layer number 32 then we take the last linear layer and then the soft Max to produce the output put of the model however you can notice that this method is not very efficient because at any time only one GPU is working a better approach which is not implemented in the code of mistal but they reference it in the paper so I will talking about it is the pipeline parallelism let's see how it works uh this pipeline parallelism I will talking about the algorithm that was introduced in this paper so gipe basically it works as follows first let me introduce you the problem this actually it's used usually when we are training a model not when we are inferencing but it can also be applied to the inference imagine we want to train a model on a sharded model so a model that is split into multiple group of layers each group of layer is present on a different GPU imagine we have four gpus each one with its own group of layers imagine we want to train this model so we run our input to the first GPU so we run the forward step to the first GPU we take this output and we feed it to the next GPU so then we run forward from there we take the output and we run it through the next GPU GPU number three we take the output we run it to the next GPU the GPU number four now we have the output of the model we compute the loss and then we can run back propagation the run back propagation does basically just the opposite we go from the last GPU to the first GPU so we run back propagation on the fourth GPU then we have calculated the gradients at the fourth GPU and we use them to calculate the previous gradients at the third GPU and then we take these gradients and we use them to calculate the previous gradients and then we use take these gradients and we use to compute the previous gradients so the forward step goes from the input to the loss the backward step backward step goes from the loss to the input and all the parameters which are also known as the leave nodes in the computational graph however also as in this case you can see that at each step we are only util utilizing one single GPU and all the other gpus are quite uh not working they are idle a better way is to use pipeline parallelism so imagine that the previous step of training was done using a very big botch suppose this batch is made up of eight items what we do with pipeline parallelism is we take this batch and we split into micro batch so instead of eight items we create micro batch so four micro batch of two items each what we do is we run the first micro batch from the in the first GPU this will produce the output for the first micro batch and we can feed it to the next GPU but now at the time step one we realize that the GPU one now is free so she can already start working on the second micro batch meanwhile the second GPU is working on the first micro batch and when she will finish she can send it to the next GPU and uh meanwhile we realize that now the second GPU is free so we can if the gpu1 has finished we can take the output of the gpu1 and transfer it to the gpu2 and the GPU one will be free so it can work on the third micr batch you can see here then after the third GPU has finished it will take the uh output of the third GPU we send it to the fourth GPU but we realize that the third GPU is now free so if the previous GPU have finished we can transfer the second micr batch to the third GPU the third microbatch to the second GPU and the first GPU which will be free can start working on a new micro batch which is the fourth micro batch and basically we do this uh job of time Shifting the micro batches and this will result in a better utilization of the gpus because now at every time step we at this time step for example all the four the gpus are working and also at this time step here at the backwards step and for each micr batch we calculate the gradient but we do not update the parameters we do what is called gradient accumulation which basically means that we calculate the gradient for each microbatch and we keep summing it to the existing gradients but we do not update the parameters of the model after all the micro batch have finished processing the forward and the backward we update the parameters of the model uh the gradient accumulation is a technique that I have introduced my my previous video on distributed training so if you want to understand how it works I I refer you to my previous video on distributor training in which I explain also the math behind grit accumulation and how it works but B basically this is the solution with pipeline parallelism so we can actually divide our batch into micro batches and this can also work with inferencing because when we inference we just don't have this backward step here right so we just delete this second half of the table but we can still take our big batch at the beginning we split it into micro batches and we time shift them uh according to the availability of the GPU and uh this um pipeline parallelism basically introduces still some uh time steps in which not all gpus are working and these are called bubbles to avoid bubbles these big bubbles here what we can do is we can uh use a bigger initial batch size so we have multiple micro batches okay guys now let's go to the last part of this video I know that the myal code is much more complicated to understand compared to the Lama code and I will show you why but I will also help you understand the most complex Topic in the code which is the X forers Library which is a trick they use to improve the inference performance and it's actually a very Advanced technique and I want to give you a glimpse into how it works so basically imagine you are running an AI company and you are providing llm inference service so you have a customer that has you for example provide an API and you have customer that send their prompts to your API and then one to run inference through your large language modules each prompt of course may have different length because each customer may be using the the large language model for different purposes for suppose Simplicity suppose that each word is a token so suppose you have three customer the first customer says write a poem the second customer says write a historical novel and the Third customer says tell me a funny joke of course you could process all these prompts one by one but that would not be very efficient because uh the two other two customer would be waiting for the first customer to finish and when you have a lot of customers that's not good and secondly you may not be fully utilizing the memory of your GPU so the best thing that you can do is to do batching you create all these prompts you create one big batch but the problem is that the prompt have different lengths so the first prompt is made up of three tokens the second prompt of four tokens and the third prompt of five tokens one solution is to to add padding to these tokens so basically we create a batch in which we append padding tokens to the input sequence until they all reach the same size then we can run this sequences this batch to our a large language model which could be for example llama or mistal as we saw before when we have a input sequence of end tokens the attention mechanism produces an output sequence of end tokens and we usually take the embedding of the last token send it to the linear layer then the soft Max to understand what is the next token from our vocabulary but in the first prompt we see that we have added two padding tokens so we cannot use the embedding corresponding to the last two tokens because they correspond to the padding token what we should do is we should take the embedding corresponding to the last non padding token to and then send it to the linear layer and then to the softmax to understand understand what is the next token and in the case of the second prompt we should be using the fourth token not the last one only in the last prompt we can use the last token because it's the last it's a non not padding token now we have done this and how do we actually create a attention mask to to run it we basically just create an attention mask that is Cal that will make each token only uh visualize the previous tokens so each token will be able to relate to previous tokens but not to Future tokens and this mask here will work fine for all the three scenarios you can see here and I will show you later how we cannot use a different mask for each prompt because all the prompts are of the same length so all the mask must be 5x five because you cannot use a 3X3 mask for this prompt a 4x4 matx uh mask for this prompt and the 5x5 for this prompt because the input sequence is five so we must use a 5x5 mask and we have to use a 5x5 mask that is Cal and also has the uh we can also mask out for example imagine the sliding window is size is four then we can mask out this value here also because we don't want the uh this token here to watch tokens that are distance of more than four for example so the problem here is that we are calculating a lot of dot products especially for the first and the second produ that will not be used let me show you why when we apply this mask so the 5x5 mask you can see here to this input sequence here which are I want to remind you is a batch it will produce the following attention mask in which all these value will be masked out because they are minus infinity minus infinity and it's because of the causality of the Mask uh we cannot mask um this value here because they are needed for the last prompt for example and we also cannot mask this value here which is needed for the last prompt but for the first and the second prompt we are doing a lot of dot products for example these ones between padding tokens and other tokens that we will not be using because if I I want to remind you that in the first prompt at the output of the model so we will be using the output at the third token at the for the second prompt the output at the fourth token and only in the last token we will be checking the last output of the output of the self attention but for the first two prompts we will not be even checking the last token output from the self attention because they correspond to the padding token so is there avoid a way to avoid these padding tokens uh being introduced in our calculation and calculating all these dot products which will result in output tokens that we will not even use well there is a better solution and the solution is this the solution is to combine all the tokens of of all the prompts into one big sequence consecutively and we also keep track of what is the actual size of each prompt so we know that the prompt are coming from our API because we are running an AI company and we have this API so we know that the first customer has a token size prompt of size three tokens the second one has four tokens and the third one has five tokens so we can keep track of these sizes in an array for example and then we buil this sequence which is a concatenation of all the prompts that we receive we take this Mega sequence we run it through our llm Ser llm model so it could be mistal or it could be llama this as I told you before uh and input sequence in a Transformer will result in N output tokens in the output so we have uh here we have three + 4 so 7 7 + 5 12 tokens as input it will produce 12 tokens as output to understand what is the next token for each prompt we need to check the the we need to check the embedding corresponding to the Token number three for the first prompt to the Token number seven for the second prompt and the last token for the third prompt so we take all these embeddings we run them through the linear layer then we apply the soft marks and then we understand what is the next uh token from our vocabulary but you may be wondering how do we even produce an attention mask that can work with multiple prompts that are combined into one sequence such that the token of one sequence should not of one prompt should not be attended to the tokens of the another prompt but only of the tokens of the same prompt right well the X forers Library allow us allow us to do that using a method called block diagonal Cal mask which is also used in the source code code of mistal so I want to show you how it works basically X forers this method called block diagonal causal mask will produce a mask like this it will be um group basically all the prompts into groups such that each token only can attend to the tokens in the same group here we have three prompts so the token PO for example can only attend the token of the same uh prompt the token novel for example cannot be related to the Token poem so it will put minus infinity here but all the token of the same prompt will be able to be attended by the the token novel while the token uh in the last prompt will be only be able to attend the other tokens in the same prompt and this is a special mask built by using the X forers Library let me show you how it works in the code okay I want to show you actually how it works so in the mistal source code they are using this um Library called X forers X forers Library allows us to compute very complex attention mask and also to calculate the attention in a very efficient way using the memory efficient attention calculation which I will not show in this video maybe I will make a future video about it but basically what they do in the misal source code if you have multiple prompts they will create one big sequence and then keep track of the number of tokens of each prompt and then they use these Methods made available by the X forers Library to build this complex attention maps that keep track of the different size of the KV cache because each prompt may have a k cache that is different from another prompt because imagine you have a prompt with "5tokens of course you will have a k cach "that is 5in in another case so the mask attention mask that we build should take care of this and the second thing is that each group of tokens should only be able to relate to the same to to the tokens of the same group not to other groups so not of of tokens from another prompt and this is done with the um block diagonal Cal mask so basically we tell him okay the first prompt is made up of seven tokens the second prompt is made up of five tokens and the third prompt is made up of six tokens and we are also using a sliding window attention with a sliding window size of three and basically this will create the complex Marx that we can see here this is the first group of tokens from 0 to six is the first prompt from 7 to 11 is the second prompt and from 12 to uh let me check 17 is the third prompt and as you can see it also takes into consideration the sliding window size so each token can only watch at most two previous tokens so the tokens in the in the contained in the sliding window size of size three the second one they use is the block diagonal mask and okay this one is used for the first chunk during the prefilling this one is used for subsequent chunks in the prefilling and basically it also takes in because during the first prefilling we don't have the KV cache because it's initially empty but during the subsequent steps it's not empty anymore so we need to take into consideration also the different size of the k cache uh so for example the first token may have a k cache of size 10 because the prompt is very short but the second prompt may be "very big suppose 5"have a k cach of size 5takes into consideration also the size of the KV cache and it will produce a um a mask that takes into consideration also the size of the KV cach the last uh method they use is this one block diagonal Cal with offset padded Keys mask because each prompt may have a different size for the k cache uh but only some tokens in this K so the K cach size is fixed it's a tensor that is of fixed side W but only some tokens may be actual being filled in this K cach so only maybe the K cach the size is let's say 10 but because the first prompt is very short only three tokens are actually part in are in the k cach um but when we pass the K cach to the calculation of the attention we pass all the tensor which is all the 10 items so we need a way to tell to the mask that he should only use the first three items from the KV cach and not all the KV cach not all the tensor and this is done with block diagonal with offset pading Mas so this method here it's very long name very complicated but this is why they use it and it will produce a mask like this so it takes into consideration the actual size of the KV C even if the K all the KV cach have the same size because it's a fixed size tensor but it tells you how many items there actually it should use from each cache okay guys it has been a very demanding video I have to say uh I had to record it more than once I actually had to cut some parts because I even I got confused sometimes uh it's very complicated topics it's a lot of things that you have to grasp but I hope that it will make your life easier when you want to understand the mistal code I actually am also putting online my notes the one that you have seen so the two notebooks that I have shown you plus also the code annotated by me on the mistal source code now the mistal source code I actually never run it so because my computer is not very powerful so I never run the actual model on my computer what I did to study the model was to run some random tensors through a model and I created basically a model with randomly initialized uh weights but with less number of layers so it could fit in my GPU and then I just run some random tensors to study all the shapes of the tensor and all the information passing so I don't know if the code works but I hope it will works I mean I didn't touch the logic I just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model I actually really enjoyed studying it I really enjoyed studying the code and I learned a lot of stuff you know um I think it's very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you don't learn much by the end of the day anyway guys thanks you for watching my video I hope you also enjoyed this journey with me even if it was very complicated I hope that you likeed this video and you will subscribe to my channel if you didn't please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on LinkedIn on Twitter Etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you don't understand I am always available to help and connect with me on LinkedIn bye-bye 