hold up before we get into this next episode I want to tell you about our virtual conference that's coming up on February 15th and February 22nd we did it two Thursdays in a row this year because we wanted to make sure that the maximum amount of people could come for each day since the lineup is just looking absolutely incredible as you know we do let me name a few of the guests that we've got coming because it is worth talking about we've got Jason Louie we've got Shrea shanar we've got Dro who is product applied AI at Uber we've got Cameron wolf who's got an incredible podcast and he's director of AI at reeby engine we've got Lauren Lockridge who is working at Google also doing some product stuff oh why is there so many product people here funny you should ask that because we've got a whole AI product owner track along with an engineering track and then as we like to we've got some Hands-On workshops too let me just tell you some of these other names just for a moment you know because we've got them coming and it is really cool I haven't named any of the Keynotes yet either by the way go and check them out on your own if you want just go to home. ops. community and you'll see but we've got tunji who's the lead researcher on the Deep speed project at Microsoft we've got golden who is the open- source engineer at Netflix we've got Kai who's leading the AI platform at Uber you may have heard of it it's called Michelangelo oh my gosh we've got fison who's product manager at LinkedIn Jerry Louie who created good old llama index he's coming we've got Matt Sharp friend of the Pod Shreya Raj Paul the Creator and CEO of guard rails oh my gosh the list goes on There's 7 plus people that will be with us at this conference so I hope to see you there and now let's get into this podcast hey everyone my name is apara um I'm one of the co-founders of arise AI um and I recently stopped drinking coffee so I take I I've started on machil lattes instead hello and welcome back to the mops Community podcast as always I am your host Dimitri o and we're coming at you with another Fire episode this one was with my good and old friend apara and she has been doing some really cool stuff in the evaluation space most specifically the llm evaluation space we talked all about how they are looking at evaluating the whole llm systems and of course course she comes from the observability space And for those that don't know she's co-founder of arise and arise is doing lots of great stuff in the observability space they've been doing it since the traditional mlops days and now they've got this open source Package Phoenix that is for the new llm days and you can just tell that she has been diving in head first she's Chief product officer and she has really been thinking deeply about how to create a product that will help people along their Journey when it comes to using llms and really making sure that your llm is useful and not outputting just absolute garbage so we talked at length about evaluating Rags not only the rag the part that is the output but also the retrieval piece she also mentioned and she was very bullish on something that a lot of you have probably heard about which is llms as a judge so I really appreciated her take on how you can use llms to evaluate your systems and evaluate the output but then at the very end we got into her hot takes and so definitely stick around for that because she thinks very much on the same lines as I do I don't want to give it away but she came up with some really good stuff when it comes to fine-tuning and traditional ML and how traditional ml Engineers might jump to F tuning but that is all no spoilers here we're going to get right into the conversation and I'll let you hear it straight from a PARTA before we do though huge shout out to the arise team for being a sponsor of the mlops community since 2020 they've been huge supporters and I've got to thank them for it aarta was one of the first people we had on a virtual Meetup back when everything was closed in the co era and she came into the community slack was super useful in those early days when we were all trying to figure out how to even think about observability when it relates to ml and so I've got to say huge thanks huge shout out to the arise team check out all the links below if you want to see any of the stuff that we talked about concerning all of the llm observability or just ml observability tools and before we get into the conversation would love it if you share this piece with just one person so that we can keep the good old mlops Vibes rolling all right let's get into [Music] it okay so you wanted the story about how I ended up in Germany here it is here's the tldr version I was living in Spain so I moved to Spain in 2010 and I moved there because I met a girl in India and she was in bilbow Spain doing her Masters she wasn't from she wasn't from India or Spain she was from Portugal but I was like oh I want to be closer to her and I also want to like live in Spain because I enjoyed it I had lived in Spain I spoke a little bit of Spanish poito and then I was like all right cool let's go over to Bill Bal uh I've heard good things about the city and the food and the people so I moved there as soon as I got there this girl was like I want nothing to do with you and so I was sitting there like heartbroken on the coastline of the bass country and it took me probably like a month to realize well there's there's much worse places I could be stuck and so I enjoyed it and I had the time in my life that year in Bilbo and then I met my wife at the end of that year at this big music festival and uh so we were living in Spain we ended up getting married like 5 years later had our first daughter like 8 years later and then we were living there until 2020 when Co hit and when Co hit the lockdown was really hard and we were in this small apartment in bilb and we were like let's get out of here let's go to the countryside and we had been coming to the German Countryside because there's like this Meditation Retreat Center that we go to quite a bit and so we thought you know what let's go there let's see like if there's any places available and we can hang out on the countryside not see anybody the lockdowns weren't as strict I mean there were lockdowns and stuff but when you're on the countryside nobody's really enforcing it so we did that and we ended up in the middle of nowhere Germany with you know 100 cows and maybe like 50 people in the village that we're in so that's the short story of it wow W well that's an interesting intro there you go I mean we were just talking and I will mention this to the listeners because we were talking about how you moved from California to New York and you are freezing right now because it is currently winter there and Germany isn't known for its incredible weather but it's definitely not like New York that is for sure yeah it's a East Coast winter out here so I wanted to jump in to the evaluation space because I know you've been KNE deep in that for like last year you've been working with all kinds of people and maybe you can just set the scene for us because you're currently for those who do not know you I probably said it in the intro already but I will say it again you're the head product or chief product officer I think is the official title at rise and you have been working in the observability space for ages before you started rise uh you were at Uber and working on good old Michelangelo with that crew that is uh got very famous from the paper and then you've been talking a ton to people about how they're doing observability in the quote unquote traditional ml space but then when llms came out you also started talking to people about okay well how do we do observability what's important with observability in the llm space and so I'd love to hear you set the scene for us what does it look like these days I know it's hard out there when it comes to evaluating llms give us the breakdown yeah no let let's jump in so I so first off we're seeing a ton of people trying to deploy llm applications like the last year Demetrius has just been it's been super exciting people are and I'm not just saying you know the the fast moving startups I'm saying there's like you know older companies companies that you're like wow they're deploying LMS that have like like a you know skunks work team who are out there trying to deploy these llm applications into production and um in the last year what I think we've seen is that the there's a big difference between a Twitter demo and a real llm application that's deployed um and yeah and the fact that what we're seeing is that you know unlike traditional ml where people have deployed these applications and there's a lot of people who have that kind of experience with llms it's still relatively a small group or few people who are who have deployed successfully these applications and well you know the hardest parts about this still ends up being evaluating the outcomes you know in the new era in traditional ml you had um you know one of the things that still matters is you want your application to do well in traditional ml you had these common metrics right you had classification metrics you had regression metrics you had your ranking metrics in the new llm era you can't just put you know these metrics you know you know what I'm saying is like we saw in the beginning some people were doing things like Rouge and blue score oh it's a translation task oh it's summarization task but there's a lot more that we could do to evaluate if it's working and the biggest one that we're seeing kind of take off is you've probably been hearing it is llm is a judge and so it's a little meta it's uh where you're asking an llm to evaluate the outcome of an llm and I mean we're finding that across deployments across what people are actually putting in production it's one of the team it's one of the things that teams actually really want to get working um and I don't know it's not that crazy as you start to think about it humans evaluate each other all the time we interview each other we grade each other see know teachers grade students papers and it's it's a very you know you know it's not that far of a jump to think AI evaluating AI um but that's that's kind of this novel um new thing in the llm space so you don't have to wait for the ground truth necessarily you can actually just generate an eval to figure out was this a good outcome or not and the thing that my mind immediately jumps to are all of these cases is where you have people that have deployed llms or chat bots on their website a little bit too soon and you see the horror stories because people go and they jailbreak it and it's like oh man that is not good what this chatbot is saying on your website all of a sudden I saw one screenshot where people were saying you know I can't remember what the website was but people were talking about how oh I don't even even need open AI I can just use the chatbot on this website it's obviously uh chat GPT you know or gp4 like you can ask it anything and it will give you any kind of response and you can play with it just like you would play with a open AI llm or a gbd4 like is this you know asking questions about the product and saying but the product isn't that good is it and then leading it into the chatbot then saying yes this product is actually horrible you shouldn't buy it and it's like you can't say that on your own website about your product this is really bad so does like do the llms as a judge stop that from happening I I think is the really interesting piece no I mean it's absolutely a component of it so you're absolutely right people don't want you know the common applications we're seeing right now is like I think you hit one of them which is like chat bot on your docks or chatbot bot on your product so give me some kind of like a replacement for um like the customer support Bots we had um and then there's kind of some more you know interesting ones which I've been calling like a chat to purchase type of application where um a lot of these companies that are doing um like used to do recommendation models or Etc or are uh you know maybe selling you trips or selling you um kind of products now have a chatbot where you can go in and actually explicitly ask hey I'm doing XYZ I'm looking for this and then it recommends a set of products and sometimes these applications use both ML and llms together in that chatbot like the llm is doing the chat component it's doing the structured extraction but then the actual recommendation it might call out to an internal recommendation model so it's not one or the other but sometimes you have both of them working together in a single app application and you're absolutely right they don't want stuff like it saying stuff it shouldn't say uh giving we had one interesting case where someone's like I don't want to say we support something in this policy because I'm reliable to it if someone asks a question um and so there's all sorts of especially if you're putting it external facing there's all sorts of more rigor that it goes through to make sure it's it's working and what llm as a judge can do is well we see people checking for is things like well did it hallucinate in the answer you know is it making up something like is it making up something that wasn't in the policy is it toxic in its response is it negative about you know like in the one where it's kind of talking its own product is it is it negative in in its own response is it um correctness factuality so all of these are things that you can actually generate a prompt to you know generating you know basically an eval template to go in and say well here's what the user asked here's what all the relevant information we pulled was and here's the final response that the LM came back with does the response actually answer the question that the user asked and two does it actually is that answer based on something factual aka the stuff that it was pulled on the retrieval component and you can go in and score that and you know what we end up seeing a lot is if the response isn't actually based on the retrieval then it's it's it's hallucinating and they actually don't want to show those types of responses back to the user um and so this is this is just a very specific you know I'd say the hallucination eval the correctness eval summarization uh all of these are very common kind of llm task evals that we're seeing out in in kind of the wild right now I should mention it's very very different than what um the model evals are which is a whole another yeah category of evals you might be seeing like if you go on like hugging face for instance has a whole open source llm leaderboard I'm sure you've all seen it it's like changes every couple of hours um and they have all these metrics right like mlu and H swag which is all different ways of measuring um how good the llm is across a wide variety of tasks but for the average kind of you know AI engineer who's building an application on top of an llm they kind of pick their llm and then they're not really looking at how well does the model do across all sorts of generalizable multimodal kind of tasks they care about specifically I'm evaluating how good is this llm and the prompt template and the you know the structure that I built build doing on this one specific task that I'm asking it to do and so it's a very does that make sense like that delineation between the the model evals versus the task evals here yeah 100% and you did say something else like how does I guess what my mind goes to here is how you are able to restrict the output from getting to the end user is it that if that llm as a judge gives a certain confidence score then anything lower than whatever a five of confidence that this is the right answer it doesn't go out to the end user and it has to regenerate it or what does that look like like that last mile piece yeah I think it depends back on the application owner so we there are some people who will generate that eval and then decide not to show that response because the evalve will was um was pretty poor um but there's some folks where you know if it's a lower risk type of application then they still show it but then they can come back and you know use the ones where it did poorly on to come back and figure out how to improve the application so I think there's a um there's kind of like a you know a trade-off that the application owner has to make between do you want to block kind of you know you got to wait for the LM to evaluate it and then like you're going to have for sure impact speed of the application experience and so what actually we see a lot of people doing is these evals um you don't just have to do them in production so similar to how you know in the traditional ml World you'd have a you know a performance like an offline performance as you're building the model and then you're kind of monitoring an online performance well here similarly as you're building in eval you're kind of seeing folks kind of evaluate the llm offline see how you know build some confidence essentially around um you know how how well is the application doing before they actually you go on to to pushing it out to online monitoring so there's there's a lot of these similar kind of paradigms that that still apply in in this space yeah I would imagine you try and do a little red teaming you try and make it see how much you can break it before you put it out there and and set it live if you're doing it the right way not just rushing it out the door ideally the other piece on this that I think is is important is a little bit more Upstream because you were talking about how the llm as a judge it's evaluating if the answer that the other llm gave with respect to the context that it was giving or that it was given was actually the right answer or evaluating that answer based on the context but then one step above that is actually getting the right context I would imagine and so making sure that are we getting the context that is relevant to the question that was asked and I know that's a whole another can of worms and if you've been seeing a bunch of that and maybe do you also use llm as a judge there oh yeah um okay so this is this is a great actually segue to talk about some research we've been we've been dropping lately um so yes llm as a judge can totally be used to also evaluate the performance of retrieval so for folks who are who are listening to this um what is you know what is retrievable how do you measure the performance of it well basically um you know in retrieval you're retrieving some sort of context so if someone asked a question about some kind of let's say product you're retrieving relevant information about let's just say it chat on your docs type of application it's very common um someone's asking your you know product support documentation or your customer support documentation questions and what happens is it retrieves relevant information from your you know document Corpus and then it pulls you know just relevant chunks and then uses those relevant chunks and the context window to actually answer the question the most important thing there is that it's in this type of retrieval by the way is super important because it helps llms connect to private data remember llms were not trained on every single company's individual private documents and so if you actually wanted to answer questions on your own private data then um using a retrieval is kind of one of the best ways to do that yeah um and so here what really ends up being important is that did it retrieve the right document to answer the question and that ends up being a a whole you know there's a whole set of metrics that we can use to actually evaluate that retrieval um and recently actually we've been running a ton of tests measuring different llm providers um so we evaluated gp4 we did anthropic um we checked some results on Gemini we did mistol and we actually showed um you know if you guys have been following Greg cam actually dropped the first of these like needle in a Hy stack type of test you kind like that lost in the middle yeah and you know for those of you who haven't seen it it's basically um an awesome way to think about it which is you know it essentially checks you know on one axis you have how long is the context so the context can be one k to tokens all the way to you know for some of the smaller models it's like 32k I think for some of the bigger ones um we tested uh pretty pretty significantly let me rouble check exactly what 120k I would imagine I think that it feels like anthropics goes all the way up to that or maybe it's even more these days like 240 they just said it we'll double it yeah so some of them we checked yeah like definitely close to yeah 120k and but what we did was basically so that's that's on one axis which is basically just the context length and then on the other AIS is basically where in the context you put the information so you know cuz some there there's all these theories out there like if you put it early on does it forget if you put it later down does it not use it and so kind of placement of the context within that context window to see you know can you actually find the needle in the Hast stack and the question we did for This was um so a little context was the question we asked llm was um what's the so we did kind of like a key value pair the key was the city and then the value is a number so we said something like what's Rome's special magic number and so uh and then inside the context we put something like Rome special magic number is uh like some seven digigit number uh like yeah like 1 2 3 4 5 6 s and so that was a Rome special magic number and then later we'd asked we put that somewhere so we tested kind of all of the dimensions of putting it at the very beginning of the document for a very short context window putting it at the very end for a very long context window and all the combinations in between and then we asked it what was Rome's special magic number and so it would have to go through look at the entire context window and then answer the question and sometimes it couldn't find it and it said you know unanswerable or um Etc and sometimes it you know it answered the question and what we did was we just ranked a lot of these llm providers out there on how good was it at retrieval and um gbd4 was by and large definitely the best out there um I think of the small model providers we were definitely impressed with mistol um like the 32k cont it was it was pretty impressive um but there were some were you know I think we realized some of them were very very um if you changed the prompt a little bit then the results totally varied so you got totally different varied responses based on just like adding a sentence or adding two sentences and so as a user you know as as we're coming back and we're evaluating this you know if you're using some of those llms where you have to be really careful about how you prompted um those prompt iterations can have a big difference in the actual outcome of of that retrieval so um you know I'll share the links with the metrios maybe you can link it with the the podcast interview but um course definitely I I think going back to your original question of like can you evaluate retrieval absolutely um I think it's it's uh it's really important to make sure that if you wanted to answer well and not hallucinate on your private data it's got to do well at that retrieval part yeah so it's almost like you're evaluating in these two different locations right you're evaluating the retrieval when it comes out and if that is relevant to the question and then you're evaluating the output of the llm yep once it's been given that context and if that output is relevant to the question also y exactly exactly exactly and if that output's based on the context that was retrieved so first is that yeah so EXT yeah totally so first is like is the retrievable that was retrieved even relevant to the question asked then was the output based on the retriev text and then yeah was the output itself answering um you know correctness I guess is is looking at is a correct based on the information that was retrieved so it's um there's levels to this so I'm a little bit like I just had a stroke of inspiration right now and I'm gonna let you have it because I love talking with you and I love the way that you create products but the next product that you create in this space I think I found the perfect name for it and what is it Golden Retriever you can do so many things with that and it is so perfect it's golden of course you know like it's the golden metrics it's the golden retriever and so you've got your if you're do if you create a product around that I'll just uh you know we'll talk later about the the way that we can figure out this patent and the golden retriever I love that but I mean jokes aside I I know that you have been creating products in this space I saw Phoenix and I would love to know a little bit more about Phoenix and also as I mentioned before we hit record one of my favorite things with all of the products that you've been putting out from the get-go I think when we talked in like 2020 one of the first things that I noticed with the arise product was how well put together the UI and the ux was for observing what was happening under the hood and it feels like Phoenix took that ethos and went a little bit further you're being a product person can you like break down how you think about that and how you were able to get inside of what metrics are useful and how can we present the metrics that are useful in a way that people can really grab on to it yeah well first of all thanks Nest that that's really kind um yeah I I'm super excited about Phoenix I think um got to give a big shout out to to the Phoenix team within within our eyes actually um so Phoenix is actually um for those of you who don't know it's our OSS product um it's got a ton of support for llm evaluations and llm observability so if any of you guys are looking to just try something not have to send your data outside have it be lightweight it's um you know it's open source so do do do what you want W with it um I think the intention behind Phoenix really was so there's a couple different components in Phoenix that I think folks who are trying to get observability on LMS Alik this is Skyler I lead machine learning at Health rhythms if you want to stay on top of everything happening in mlops subscribe to this podcast now now now now now now now first is if you are one of the things we just noticed very early on was that these applications many of them have not just one call they're making there's many calls under the hood like even in a simple chatbot with retrieval there's first the user's question then you have the you generate an embedding then there's the retriever then there's the actual synthesis of the context and then there's a response generation so there's five or or six different steps that have happened in something that feels like one interaction between the product and the user um and so the first thing was well if you have all these substeps some if something goes wrong something goes wrong within those you know five or six different steps that's happened then being able to pinpoint exactly what are the calls that are happening under the hood and how do I get visibility is important and so with Phoenix um one of the most popular components of it is you can see your full you know you can full see the full traces and spans of your application so kind kind of like the full stack traces how you can think about it so um you'll see the breakdown of each calls and then which calls took longer which calls used the most tokens and then you can also evaluate at each step in the calls so kind of like we were just talking about where at the end of the application at the very end um when it generated response you can have a score of of How well was the response but then if the response let's say was hallucinated or was incorrect then there's a step above you can go in and look at the individual span level evals look at well how well did it retrieve and then within the retriever you know how let's evaluate each document that it retrieved and see if it was relevant or not so there's kind of a lot of thought put into first how do I break down the entire application stack and then see you know evaluate and evaluate each step of that outcome and then the other part that's been I'd say really a lot of thought in is Phoenix does come with an evals Library um it's task evals first or you know you knowm application evals so it's it's definitely useful for folks who are actually building the application um and then we've just seen a bunch of people kind of build these evals in production so it comes with kind of a lot of these best practices baked in one of them that um actually just just went viral on Twitter last week we dropped a big research start on this which is um should you use score evals or classification EV vals um I don't know if you caught caught that one but um I saw your post blew up I definitely did but I didn't I don't know what the difference is between the two which might make me look like no no no no this this mean just space is changing so fast or early like there's I think we're all just trying to learn and soak up as much as we can um but so score evals versus um classification evals score evals is basically you can think about it as the outputs a numeric value so let's just say we were uh asking llm to evaluate how frustrated is this response and you know rank it between 1 to 10 one being someone's really not frustrated 10 being someone is super frustrated um well what would you expect you would expect that okay if it said one it's super frustrated if it's 10 it's not frustrated but then kind of somewhere if it said something like a five it's kind of like okay it's maybe it's passive aggressive it's sounds super frustrating but it's like you know like you you kind of expect it to like kind of the numbers in the middle to make sense um and what we just realized is we did This research was that the score value actually had no Clear Connection to the actual you know you know frustration that this person like if it gave a number that basically wasn't one or 10 then that score value really had no no real connection like another example which was actually the one that we posted was we we gave it a paragraph and we said um you know rank how many spell spelling how many words have a spelling error in this document between 1 to 10 if every word had a spelling error give it a 10 if no words have a spelling error give it a zero and then if it's kind of somewhere in the middle like 20% of the words have a spelling error you know give it a two if it's 80% give it an eight you know um and what we saw was that in many cases it would give a spelling you know score of like 10 but in some cases only 80% you know only 11% of the words had a spelling your like but it still said all the words were off was like this might as well be in Dutch yeah so like that value that it came back with meant nothing to it really um and so and and the reason this is important is like what we're seeing is like there's a lot of these llm eval cookbooks that are out there where people are recommending you know basically set it up as a score and what we've actually been seeing is don't do that just do it as a class just do binary stuff you know or or you can do multic class you know but like tell it explicitly like frustrated not frustrated because if you try to a score that score just doesn't it doesn't actually um have any meaning to to the corruption level so basically it's saying hey it's it has to be very clear it's frustrated not frustrated or a little frustrated and you have to make it it's not a sliding scale there's no llms do not understand spectrums it's like uh you know from all the tests we've done it is is not going to give you a meaningful value on a spectrum and and then if you're Bas yeah if you're basing stuff Downstream off of that score you're screwed exactly and so and this has been something that like has been a lot of people I I think it's kind of like as we've been putting it out people have been like I've seen that like I'm you know like I was using score then I was like like these values meant nothing to me so then I switched to classification and so um there's a whole set of research around this probably still to be deep dived into here but um yeah this is you know this is the kind of stuff that's in the Phoenix evals library is just best practices based off of what we're seeing what we're putting in production what we're helping folks actually launch and so you get kind of these best you know test in class I'd say templates that are pre-tested around things like how to test for hallucination how to test for toxicity how to test for correctness and then you can kind of go and we have people who then go off and make their own custom eals but it's a great place to kind of have a framework that runs both in a notebook and also in a pipeline uh very efficiently and is meant for for for kind of um you know you can swap in and out of offline and online very easily yeah because the other piece that I was thinking about all these these evow that you give it then it feels like you're not going to get very far if you're not doing custom evals have you seen that also totally I think there's a lot of folks who are building you know maybe they start with something but then they end up kind of building their own what makes sense for their application adding on to it so I I definitely think at the end of the day the Nuance here is it's probably different than the ml space is that that customization of that eil ends up being really important to measuring what's what's important to your app um so I I don't know that that's one of the things I predict because we're going to see a lot a lot more of this and do you feel because one thing that I've seen is like how when you put out these different evaluation test sets the next model is just trained on top of them and so then they're Obsolete and so it's it's going to be this game of cat and mouse in a way because the models are going to be able to eat up the test sets as soon as they get put out for anyone to see or is it going to be all right I've got my evaluation test set and I just keep it inhouse I'm not going to let anybody else see that so that I don't paint the model yeah I it's actually thing I wonder about a lot too um is as these new llms come out are they really blind to the test sets that they're actually then evaluating them on I think um like the Gemini paper I thought did a really good job of calling out they actually built their own data set that was blind and then tested on that data set and um they called that out explicitly which I thought was really important because you know as as people are sharing the results of like the next best Lum Etc yeah I think we're all wondering like did you just you know like you know did did have access to that training data set so I I wonder that all the time too well it's pretty clear these days that uh as I did not coin this term but I like it and I will say it a lot benchmarks are and so all these benchmarks on hugging face or on Twitter that you'll see like oh this is SOA this just came out it blew everything else out of the water by whatever 10 times or you make up a number there I don't even consider that to be valuable anymore it's it's really like what you were saying where these things I know you actually went and you did a rigorous study on it but it's so funny because we are the rest of us are just going off of Vibes and we're seeing Oh yeah this is not really working this is not doing what it I thought it was going to do and so if I use a different model does it and then you try that and you go oh yeah okay cool this is better this is easier to prompt or this prompt is much easier to control whatever it may be and so I appreciate that you did a whole rigorous study on it I also I'm conscientious of time I want to make sure that I highlight that you all are doing like paper studies right and you're meeting every once in a while I think that's awesome I know that you've got a ton of smart people around and so I I can imagine you're getting all kinds of cool ideas from Gathering and doing these paper studies I would encourage others to go and hang out and uh do those we'll put a link to the next one in the show notes so that in case anyone wants to join you and be able to Ping ideas off of you that's great I still would love to hear how did you come up with the VIS visualizations that's the coolest pieces I think you didn't get into that part and I want to get get to it before we go a I mean I just got got to say the um the the team team's amazing so the and I think the you know trying to find a way to Bubble Up you know at least in the llm space one of the cool things you know maybe you've seen some of the demos of it but like especially with the retrieval there's a lot in the edting space that's you helpful to visualize so how far away is the prompt that you have from the context that was retrieved and if you're missing any context and it's super far away or it's you know it's it's like reaching to find anything that's relevant to you know those are all really cool visualizations that you could actually surface and kind kind of help people see a little bit of okay here's my data here's what it thinks things are connected to um so yeah again you know check out Phoenix love love all the notes on on the UI yeah actually it reminds me that I think one of the first people I heard the word embeddings from was you on like that first Meetup that you came on back in like June 2020 around then because you were already thinking about them I think you were thinking about them from recommender systems and that angle and then has it how has that changed now no great great question um well I think I think edings are just so powerful and I'm I'm so glad that we're you know we're all talking about them and using them in observability because there's it's it's super powerful even in the llm space I think in the past there's you know folks used them like you mentioned recommendation systems image models but the llm space I mean the core basis of retrieval is based off of those word em you know the the embeddings um itself and doing that Vector similarity search to fetch the neous embeddings um so I think the the use case is really really strong in in rag um for llms the you know and and and so because it's such a core component it it's also something that is important to now you know and just like when we're going back if the retrieval is off then the response is just not going to be good and so you need a really good way to verify that what was retrieved was relevant and if there's any shift in again going back to all of this is now textual if the prompts are changing what your users are asking are different or the response of the LMS are different these are all things that you can you can actually measure using embeddings and embedding drift and uh things like that so um I I think there's just maybe more use cases now than ever to to dig into edings yeah it has to be treated as a first class citizen 100% these days that's a really good point and I do I saw a recent paper speaking of papers from Shrea Shankar did you see that Splat or SPL Spade I think is what it is uh talking about the prompt deltas and evaluating via the prompt Delta like you have your prompt templates but then you're evaluating The Prompt deltas and it's like wow there's so much creativity in this space and the ability to look at how can we evaluate things differently than we are right now and see if we can get a better outcome yeah I I I think I still need to dig in to to Spade specifically but yeah the um I mean I I think the the amount that the space is moving is just so fast right now it's so exciting and um it is very the the one thing maybe I'll just you know there's maybe two things I just wanted to like at least drop like my quick hot takes or or notes on oh let's do it this is great this is what we're GNA chop and put at the beginning of the episode yeah exactly right so I think there's you know I always hear this from I don't know I I just see it in the in in discussions but I see a lot of people talking about um fine tuning like really early on like their application's not even deployed and they're like oh well our use Cas is eventually we're going to go back and F tuned and and um I you know I get asked from folks like hey PARTA does that make sense as like a step in troubleshooting an llm application um and I think one of the reasons I get that question a lot is if you just think back to you know a lot of the AI teams are now you they've worked on traditional M they're shifting now to llms but that's something we're all very used to and very familiar with we're used to training model models on data and then deploying those models and fine tuning is feels very familiar right you grab data points that it doesn't work on you fine-tune it and that's how you improve the performance of your model um but in this space fine-tuning feels like you're jumping to like level 100 when sometimes a lot of this could be you know like I was telling you in the rag case change the prompt a bit and you get vly different responses and so it's like almost the thing that we're like geared towards to do which is like oh it makes sense we're going to training is now fine tuning and we're all used to that Paradigm but I I think in this space let's start with like the lowest hanging fruit and see how that improves um because I I I think you know and and drudge carpy actually Drew this like really awesome image of like you know level of effort versus the ROI you know kind of kind of of that effort and prompt engineering rack like there's so many things you could do to improve the lm's performance before you jump into fine-tuning or like training your own llm so it's just I think it's important to like start with with something that could have the highest Ry you are preaching to the choir and I laugh because I was like talking about how fine tuning to me feels like when all else fails you'll throw some fine tuning at it and it's like yeah that's what you need to you need to look at it as like the escape hatch almost not as step two it should be what you go to when you can't get anything else to work and try rigorously to get everything else to work because it is exactly like you said it is so much easier to just tweak The Prompt then fine-tune it and and I didn't connect the dot on how similar the two are and like oh if we're coming from the traditional ml space then it's easier to jump there and be like oh well that's just because we need to fine-tune it and then it'll do what we want it to do yeah totally um I I think there's just something very natural feeling about okay you know training is now fine tuning but it's you know I think it's one of those changes we all have to just just adapt with with the the space changing yeah a simulate yeah 100% excellent and then my my other hot take I guess um I it's totally a hot take but um I think sometimes I hear um a lot of this you know I yeah maybe I hear it less now than than I was in the beginning um so I I hear a lot of like well it's kind of continuation of the fine tuning well if I pick an open source model I can go in and fine-tune it more and I can and you know or I can then go and you know modify it for my use case because I I know I I have access to model weights and and do that um I think that I hear a lot of folks asking well does choosing an open source model versus a private model end up slowing down product development or like what what kind of what's kind of the pros and the cons of One Versus the other um I think I was hearing a lot more of like you know almost resistance for some of you know just the private models of the beginning and a lot more of the you know open source I am so in this horce I got to say I'm you know all for the open source Community like I think I'm I'm also all for you know whatever llm just makes your application the most successful it can be um so pick pick the one that like gets you the performance and the outcomes that you need I don't think that like um you know some people make a bet on the open source because they're like a later I can go back and find tun or it's better and Etc but it's again how many of those folks are really going to actually F tune and for what I've been seeing out out in the wild starting with the open AI or GPT 4 has just been helping most people get to kind of the outcome that they need their application to get to um and so again I think I just come back to like there's you know all for the open source Community all for our you know just getting your application to actually work as as good as it needs to to work but start with start with like what do you need for the application and last of like I think the how's this going to scale yeah like I conversation back in the day where you're like oh we're going to need to use kubernetes for this and you're like wait a minute we have no users are you sureet you're I know you're planning for the future and this is great for the tech debt but uh we might want to just get some up on streamlet before we do anything totally totally totally and I think that that's like what I keep coming back to is like the more of these you know similar in the ml space we want to get more of these deployed actually in the real world get the BR application to add value to the organization show the ROI and I think that um that's really important to to the success of these llms and companies actually and the other piece to this that I find fascinating was something that llo said probably like two years ago and llo is Infamous person in the community for those who do not know in the community slack and he was talking about how you need to get something in production as fast as possible because then you'll find where all of the bottlenecks are you'll find where everything is messing up and unless you get into production you don't necessarily know that so each day or each minute that you're not in production you're not finding all of these problems and if you can use a model to make your life easier and get you into production faster then you're going to start seeing oh maybe it's the prompts or oh maybe it's you know whatever the case may be where you're falling behind and you're making mistakes or the system isn't designed properly yeah absolutely so I think uh maybe as we wrap up the podcast that that's really is get stuff out as fast as you can you know evaluate the outcomes I think that's you know LM EV vales is something that I think is pretty pretty got a lot of momentum around it in in the in folks who are deploying and in the community so evaluations is important and then um I think knowing how to set up the right evals knowing how to you know benchmark your own evals um customize it um what types of eval score versus classification there's just so much Nuance in that whole eval space and so as we continue to drop more research or share more stuff we're learning um we we'll share with the community excellent PARTA it's been absolutely fascinating having you on as always I really appreciate it and look forward to having you back awesome thanks to me CH thanks and thanks mops Community hey everyone my name is apara founder of arise and the best way to stay up to dat with mlops is by subscribing to this podcast 