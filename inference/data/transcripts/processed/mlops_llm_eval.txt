hold up before we get into this next episode i want to tell you about our virtual conference thats coming up on february 15th and february 22nd we did it two thursdays in a row this year because we wanted to make sure that the maximum amount of people could come for each day since the lineup is just looking absolutely incredible as you know we do let me name a few of the guests that weve got coming because it is worth talking about weve got jason louie weve got shrea shanar weve got dro who is product applied ai at uber weve got cameron wolf whos got an incredible podcast and hes director of ai at reeby engine weve got lauren lockridge who is working at google also doing some product stuff oh why is there so many product people here funny you should ask that because weve got a whole ai product owner track along with an engineering track and then as we like to weve got some handson workshops too let me just tell you some of these other names just for a moment you know because weve got them coming and it is really cool i havent named any of the keynotes yet either by the way go and check them out on your own if you want just go to home ops community and youll see but weve got tunji whos the lead researcher on the deep speed project at microsoft weve got golden who is the open source engineer at netflix weve got kai whos leading the ai platform at uber you may have heard of it its called michelangelo oh my gosh weve got fison whos product manager at linkedin jerry louie who created good old llama index hes coming weve got matt sharp friend of the pod shreya raj paul the creator and ceo of guard rails oh my gosh the list goes on theres 7 plus people that will be with us at this conference so i hope to see you there and now lets get into this podcast hey everyone my name is apara um im one of the cofounders of arise ai um and i recently stopped drinking coffee so i take i ive started on machil lattes instead hello and welcome back to the mops community podcast as always i am your host dimitri o and were coming at you with another fire episode this one was with my good and old friend apara and she has been doing some really cool stuff in the evaluation space most specifically the llm evaluation space we talked all about how they are looking at evaluating the whole llm systems and of course course she comes from the observability space and for those that dont know shes cofounder of arise and arise is doing lots of great stuff in the observability space theyve been doing it since the traditional mlops days and now theyve got this open source package phoenix that is for the new llm days and you can just tell that she has been diving in head first shes chief product officer and she has really been thinking deeply about how to create a product that will help people along their journey when it comes to using llms and really making sure that your llm is useful and not outputting just absolute garbage so we talked at length about evaluating rags not only the rag the part that is the output but also the retrieval piece she also mentioned and she was very bullish on something that a lot of you have probably heard about which is llms as a judge so i really appreciated her take on how you can use llms to evaluate your systems and evaluate the output but then at the very end we got into her hot takes and so definitely stick around for that because she thinks very much on the same lines as i do i dont want to give it away but she came up with some really good stuff when it comes to finetuning and traditional ml and how traditional ml engineers might jump to f tuning but that is all no spoilers here were going to get right into the conversation and ill let you hear it straight from a parta before we do though huge shout out to the arise team for being a sponsor of the mlops community since 2020 theyve been huge supporters and ive got to thank them for it aarta was one of the first people we had on a virtual meetup back when everything was closed in the co era and she came into the community slack was super useful in those early days when we were all trying to figure out how to even think about observability when it relates to ml and so ive got to say huge thanks huge shout out to the arise team check out all the links below if you want to see any of the stuff that we talked about concerning all of the llm observability or just ml observability tools and before we get into the conversation would love it if you share this piece with just one person so that we can keep the good old mlops vibes rolling all right lets get into music it okay so you wanted the story about how i ended up in germany here it is heres the tldr version i was living in spain so i moved to spain in 2010 and i moved there because i met a girl in india and she was in bilbow spain doing her masters she wasnt from she wasnt from india or spain she was from portugal but i was like oh i want to be closer to her and i also want to like live in spain because i enjoyed it i had lived in spain i spoke a little bit of spanish poito and then i was like all right cool lets go over to bill bal uh ive heard good things about the city and the food and the people so i moved there as soon as i got there this girl was like i want nothing to do with you and so i was sitting there like heartbroken on the coastline of the bass country and it took me probably like a month to realize well theres theres much worse places i could be stuck and so i enjoyed it and i had the time in my life that year in bilbo and then i met my wife at the end of that year at this big music festival and uh so we were living in spain we ended up getting married like 5 years later had our first daughter like 8 years later and then we were living there until 2020 when co hit and when co hit the lockdown was really hard and we were in this small apartment in bilb and we were like lets get out of here lets go to the countryside and we had been coming to the german countryside because theres like this meditation retreat center that we go to quite a bit and so we thought you know what lets go there lets see like if theres any places available and we can hang out on the countryside not see anybody the lockdowns werent as strict i mean there were lockdowns and stuff but when youre on the countryside nobodys really enforcing it so we did that and we ended up in the middle of nowhere germany with you know 100 cows and maybe like 50 people in the village that were in so thats the short story of it wow w well thats an interesting intro there you go i mean we were just talking and i will mention this to the listeners because we were talking about how you moved from california to new york and you are freezing right now because it is currently winter there and germany isnt known for its incredible weather but its definitely not like new york that is for sure yeah its a east coast winter out here so i wanted to jump in to the evaluation space because i know youve been kne deep in that for like last year youve been working with all kinds of people and maybe you can just set the scene for us because youre currently for those who do not know you i probably said it in the intro already but i will say it again youre the head product or chief product officer i think is the official title at rise and you have been working in the observability space for ages before you started rise uh you were at uber and working on good old michelangelo with that crew that is uh got very famous from the paper and then youve been talking a ton to people about how theyre doing observability in the quote unquote traditional ml space but then when llms came out you also started talking to people about okay well how do we do observability whats important with observability in the llm space and so id love to hear you set the scene for us what does it look like these days i know its hard out there when it comes to evaluating llms give us the breakdown yeah no let lets jump in so i so first off were seeing a ton of people trying to deploy llm applications like the last year demetrius has just been its been super exciting people are and im not just saying you know the the fast moving startups im saying theres like you know older companies companies that youre like wow theyre deploying lms that have like like a you know skunks work team who are out there trying to deploy these llm applications into production and um in the last year what i think weve seen is that the theres a big difference between a twitter demo and a real llm application thats deployed um and yeah and the fact that what were seeing is that you know unlike traditional ml where people have deployed these applications and theres a lot of people who have that kind of experience with llms its still relatively a small group or few people who are who have deployed successfully these applications and well you know the hardest parts about this still ends up being evaluating the outcomes you know in the new era in traditional ml you had um you know one of the things that still matters is you want your application to do well in traditional ml you had these common metrics right you had classification metrics you had regression metrics you had your ranking metrics in the new llm era you cant just put you know these metrics you know you know what im saying is like we saw in the beginning some people were doing things like rouge and blue score oh its a translation task oh its summarization task but theres a lot more that we could do to evaluate if its working and the biggest one that were seeing kind of take off is youve probably been hearing it is llm is a judge and so its a little meta its uh where youre asking an llm to evaluate the outcome of an llm and i mean were finding that across deployments across what people are actually putting in production its one of the team its one of the things that teams actually really want to get working um and i dont know its not that crazy as you start to think about it humans evaluate each other all the time we interview each other we grade each other see know teachers grade students papers and its its a very you know you know its not that far of a jump to think ai evaluating ai um but thats thats kind of this novel um new thing in the llm space so you dont have to wait for the ground truth necessarily you can actually just generate an eval to figure out was this a good outcome or not and the thing that my mind immediately jumps to are all of these cases is where you have people that have deployed llms or chat bots on their website a little bit too soon and you see the horror stories because people go and they jailbreak it and its like oh man that is not good what this chatbot is saying on your website all of a sudden i saw one screenshot where people were saying you know i cant remember what the website was but people were talking about how oh i dont even even need open ai i can just use the chatbot on this website its obviously uh chat gpt you know or gp4 like you can ask it anything and it will give you any kind of response and you can play with it just like you would play with a open ai llm or a gbd4 like is this you know asking questions about the product and saying but the product isnt that good is it and then leading it into the chatbot then saying yes this product is actually horrible you shouldnt buy it and its like you cant say that on your own website about your product this is really bad so does like do the llms as a judge stop that from happening i i think is the really interesting piece no i mean its absolutely a component of it so youre absolutely right people dont want you know the common applications were seeing right now is like i think you hit one of them which is like chat bot on your docks or chatbot bot on your product so give me some kind of like a replacement for um like the customer support bots we had um and then theres kind of some more you know interesting ones which ive been calling like a chat to purchase type of application where um a lot of these companies that are doing um like used to do recommendation models or etc or are uh you know maybe selling you trips or selling you um kind of products now have a chatbot where you can go in and actually explicitly ask hey im doing xyz im looking for this and then it recommends a set of products and sometimes these applications use both ml and llms together in that chatbot like the llm is doing the chat component its doing the structured extraction but then the actual recommendation it might call out to an internal recommendation model so its not one or the other but sometimes you have both of them working together in a single app application and youre absolutely right they dont want stuff like it saying stuff it shouldnt say uh giving we had one interesting case where someones like i dont want to say we support something in this policy because im reliable to it if someone asks a question um and so theres all sorts of especially if youre putting it external facing theres all sorts of more rigor that it goes through to make sure its its working and what llm as a judge can do is well we see people checking for is things like well did it hallucinate in the answer you know is it making up something like is it making up something that wasnt in the policy is it toxic in its response is it negative about you know like in the one where its kind of talking its own product is it is it negative in in its own response is it um correctness factuality so all of these are things that you can actually generate a prompt to you know generating you know basically an eval template to go in and say well heres what the user asked heres what all the relevant information we pulled was and heres the final response that the lm came back with does the response actually answer the question that the user asked and two does it actually is that answer based on something factual aka the stuff that it was pulled on the retrieval component and you can go in and score that and you know what we end up seeing a lot is if the response isnt actually based on the retrieval then its its its hallucinating and they actually dont want to show those types of responses back to the user um and so this is this is just a very specific you know id say the hallucination eval the correctness eval summarization uh all of these are very common kind of llm task evals that were seeing out in in kind of the wild right now i should mention its very very different than what um the model evals are which is a whole another yeah category of evals you might be seeing like if you go on like hugging face for instance has a whole open source llm leaderboard im sure youve all seen it its like changes every couple of hours um and they have all these metrics right like mlu and h swag which is all different ways of measuring um how good the llm is across a wide variety of tasks but for the average kind of you know ai engineer whos building an application on top of an llm they kind of pick their llm and then theyre not really looking at how well does the model do across all sorts of generalizable multimodal kind of tasks they care about specifically im evaluating how good is this llm and the prompt template and the you know the structure that i built build doing on this one specific task that im asking it to do and so its a very does that make sense like that delineation between the the model evals versus the task evals here yeah 100 and you did say something else like how does i guess what my mind goes to here is how you are able to restrict the output from getting to the end user is it that if that llm as a judge gives a certain confidence score then anything lower than whatever a five of confidence that this is the right answer it doesnt go out to the end user and it has to regenerate it or what does that look like like that last mile piece yeah i think it depends back on the application owner so we there are some people who will generate that eval and then decide not to show that response because the evalve will was um was pretty poor um but theres some folks where you know if its a lower risk type of application then they still show it but then they can come back and you know use the ones where it did poorly on to come back and figure out how to improve the application so i think theres a um theres kind of like a you know a tradeoff that the application owner has to make between do you want to block kind of you know you got to wait for the lm to evaluate it and then like youre going to have for sure impact speed of the application experience and so what actually we see a lot of people doing is these evals um you dont just have to do them in production so similar to how you know in the traditional ml world youd have a you know a performance like an offline performance as youre building the model and then youre kind of monitoring an online performance well here similarly as youre building in eval youre kind of seeing folks kind of evaluate the llm offline see how you know build some confidence essentially around um you know how how well is the application doing before they actually you go on to to pushing it out to online monitoring so theres theres a lot of these similar kind of paradigms that that still apply in in this space yeah i would imagine you try and do a little red teaming you try and make it see how much you can break it before you put it out there and and set it live if youre doing it the right way not just rushing it out the door ideally the other piece on this that i think is is important is a little bit more upstream because you were talking about how the llm as a judge its evaluating if the answer that the other llm gave with respect to the context that it was giving or that it was given was actually the right answer or evaluating that answer based on the context but then one step above that is actually getting the right context i would imagine and so making sure that are we getting the context that is relevant to the question that was asked and i know thats a whole another can of worms and if youve been seeing a bunch of that and maybe do you also use llm as a judge there oh yeah um okay so this is this is a great actually segue to talk about some research weve been weve been dropping lately um so yes llm as a judge can totally be used to also evaluate the performance of retrieval so for folks who are who are listening to this um what is you know what is retrievable how do you measure the performance of it well basically um you know in retrieval youre retrieving some sort of context so if someone asked a question about some kind of lets say product youre retrieving relevant information about lets just say it chat on your docs type of application its very common um someones asking your you know product support documentation or your customer support documentation questions and what happens is it retrieves relevant information from your you know document corpus and then it pulls you know just relevant chunks and then uses those relevant chunks and the context window to actually answer the question the most important thing there is that its in this type of retrieval by the way is super important because it helps llms connect to private data remember llms were not trained on every single companys individual private documents and so if you actually wanted to answer questions on your own private data then um using a retrieval is kind of one of the best ways to do that yeah um and so here what really ends up being important is that did it retrieve the right document to answer the question and that ends up being a a whole you know theres a whole set of metrics that we can use to actually evaluate that retrieval um and recently actually weve been running a ton of tests measuring different llm providers um so we evaluated gp4 we did anthropic um we checked some results on gemini we did mistol and we actually showed um you know if you guys have been following greg cam actually dropped the first of these like needle in a hy stack type of test you kind like that lost in the middle yeah and you know for those of you who havent seen it its basically um an awesome way to think about it which is you know it essentially checks you know on one axis you have how long is the context so the context can be one k to tokens all the way to you know for some of the smaller models its like 32k i think for some of the bigger ones um we tested uh pretty pretty significantly let me rouble check exactly what 120k i would imagine i think that it feels like anthropics goes all the way up to that or maybe its even more these days like 240 they just said it well double it yeah so some of them we checked yeah like definitely close to yeah 120k and but what we did was basically so thats thats on one axis which is basically just the context length and then on the other ais is basically where in the context you put the information so you know cuz some there theres all these theories out there like if you put it early on does it forget if you put it later down does it not use it and so kind of placement of the context within that context window to see you know can you actually find the needle in the hast stack and the question we did for this was um so a little context was the question we asked llm was um whats the so we did kind of like a key value pair the key was the city and then the value is a number so we said something like whats romes special magic number and so uh and then inside the context we put something like rome special magic number is uh like some seven digigit number uh like yeah like 1 2 3 4 5 6 s and so that was a rome special magic number and then later wed asked we put that somewhere so we tested kind of all of the dimensions of putting it at the very beginning of the document for a very short context window putting it at the very end for a very long context window and all the combinations in between and then we asked it what was romes special magic number and so it would have to go through look at the entire context window and then answer the question and sometimes it couldnt find it and it said you know unanswerable or um etc and sometimes it you know it answered the question and what we did was we just ranked a lot of these llm providers out there on how good was it at retrieval and um gbd4 was by and large definitely the best out there um i think of the small model providers we were definitely impressed with mistol um like the 32k cont it was it was pretty impressive um but there were some were you know i think we realized some of them were very very um if you changed the prompt a little bit then the results totally varied so you got totally different varied responses based on just like adding a sentence or adding two sentences and so as a user you know as as were coming back and were evaluating this you know if youre using some of those llms where you have to be really careful about how you prompted um those prompt iterations can have a big difference in the actual outcome of of that retrieval so um you know ill share the links with the metrios maybe you can link it with the the podcast interview but um course definitely i i think going back to your original question of like can you evaluate retrieval absolutely um i think its its uh its really important to make sure that if you wanted to answer well and not hallucinate on your private data its got to do well at that retrieval part yeah so its almost like youre evaluating in these two different locations right youre evaluating the retrieval when it comes out and if that is relevant to the question and then youre evaluating the output of the llm yep once its been given that context and if that output is relevant to the question also y exactly exactly exactly and if that outputs based on the context that was retrieved so first is that yeah so ext yeah totally so first is like is the retrievable that was retrieved even relevant to the question asked then was the output based on the retriev text and then yeah was the output itself answering um you know correctness i guess is is looking at is a correct based on the information that was retrieved so its um theres levels to this so im a little bit like i just had a stroke of inspiration right now and im gonna let you have it because i love talking with you and i love the way that you create products but the next product that you create in this space i think i found the perfect name for it and what is it golden retriever you can do so many things with that and it is so perfect its golden of course you know like its the golden metrics its the golden retriever and so youve got your if youre do if you create a product around that ill just uh you know well talk later about the the way that we can figure out this patent and the golden retriever i love that but i mean jokes aside i i know that you have been creating products in this space i saw phoenix and i would love to know a little bit more about phoenix and also as i mentioned before we hit record one of my favorite things with all of the products that youve been putting out from the getgo i think when we talked in like 2020 one of the first things that i noticed with the arise product was how well put together the ui and the ux was for observing what was happening under the hood and it feels like phoenix took that ethos and went a little bit further youre being a product person can you like break down how you think about that and how you were able to get inside of what metrics are useful and how can we present the metrics that are useful in a way that people can really grab on to it yeah well first of all thanks nest that thats really kind um yeah i im super excited about phoenix i think um got to give a big shout out to to the phoenix team within within our eyes actually um so phoenix is actually um for those of you who dont know its our oss product um its got a ton of support for llm evaluations and llm observability so if any of you guys are looking to just try something not have to send your data outside have it be lightweight its um you know its open source so do do do what you want w with it um i think the intention behind phoenix really was so theres a couple different components in phoenix that i think folks who are trying to get observability on lms alik this is skyler i lead machine learning at health rhythms if you want to stay on top of everything happening in mlops subscribe to this podcast now now now now now now now first is if you are one of the things we just noticed very early on was that these applications many of them have not just one call theyre making theres many calls under the hood like even in a simple chatbot with retrieval theres first the users question then you have the you generate an embedding then theres the retriever then theres the actual synthesis of the context and then theres a response generation so theres five or or six different steps that have happened in something that feels like one interaction between the product and the user um and so the first thing was well if you have all these substeps some if something goes wrong something goes wrong within those you know five or six different steps thats happened then being able to pinpoint exactly what are the calls that are happening under the hood and how do i get visibility is important and so with phoenix um one of the most popular components of it is you can see your full you know you can full see the full traces and spans of your application so kind kind of like the full stack traces how you can think about it so um youll see the breakdown of each calls and then which calls took longer which calls used the most tokens and then you can also evaluate at each step in the calls so kind of like we were just talking about where at the end of the application at the very end um when it generated response you can have a score of of how well was the response but then if the response lets say was hallucinated or was incorrect then theres a step above you can go in and look at the individual span level evals look at well how well did it retrieve and then within the retriever you know how lets evaluate each document that it retrieved and see if it was relevant or not so theres kind of a lot of thought put into first how do i break down the entire application stack and then see you know evaluate and evaluate each step of that outcome and then the other part thats been id say really a lot of thought in is phoenix does come with an evals library um its task evals first or you know you knowm application evals so its its definitely useful for folks who are actually building the application um and then weve just seen a bunch of people kind of build these evals in production so it comes with kind of a lot of these best practices baked in one of them that um actually just just went viral on twitter last week we dropped a big research start on this which is um should you use score evals or classification ev vals um i dont know if you caught caught that one but um i saw your post blew up i definitely did but i didnt i dont know what the difference is between the two which might make me look like no no no no this this mean just space is changing so fast or early like theres i think were all just trying to learn and soak up as much as we can um but so score evals versus um classification evals score evals is basically you can think about it as the outputs a numeric value so lets just say we were uh asking llm to evaluate how frustrated is this response and you know rank it between 1 to 10 one being someones really not frustrated 10 being someone is super frustrated um well what would you expect you would expect that okay if it said one its super frustrated if its 10 its not frustrated but then kind of somewhere if it said something like a five its kind of like okay its maybe its passive aggressive its sounds super frustrating but its like you know like you you kind of expect it to like kind of the numbers in the middle to make sense um and what we just realized is we did this research was that the score value actually had no clear connection to the actual you know you know frustration that this person like if it gave a number that basically wasnt one or 10 then that score value really had no no real connection like another example which was actually the one that we posted was we we gave it a paragraph and we said um you know rank how many spell spelling how many words have a spelling error in this document between 1 to 10 if every word had a spelling error give it a 10 if no words have a spelling error give it a zero and then if its kind of somewhere in the middle like 20 of the words have a spelling error you know give it a two if its 80 give it an eight you know um and what we saw was that in many cases it would give a spelling you know score of like 10 but in some cases only 80 you know only 11 of the words had a spelling your like but it still said all the words were off was like this might as well be in dutch yeah so like that value that it came back with meant nothing to it really um and so and and the reason this is important is like what were seeing is like theres a lot of these llm eval cookbooks that are out there where people are recommending you know basically set it up as a score and what weve actually been seeing is dont do that just do it as a class just do binary stuff you know or or you can do multic class you know but like tell it explicitly like frustrated not frustrated because if you try to a score that score just doesnt it doesnt actually um have any meaning to to the corruption level so basically its saying hey its it has to be very clear its frustrated not frustrated or a little frustrated and you have to make it its not a sliding scale theres no llms do not understand spectrums its like uh you know from all the tests weve done it is is not going to give you a meaningful value on a spectrum and and then if youre bas yeah if youre basing stuff downstream off of that score youre screwed exactly and so and this has been something that like has been a lot of people i i think its kind of like as weve been putting it out people have been like ive seen that like im you know like i was using score then i was like like these values meant nothing to me so then i switched to classification and so um theres a whole set of research around this probably still to be deep dived into here but um yeah this is you know this is the kind of stuff thats in the phoenix evals library is just best practices based off of what were seeing what were putting in production what were helping folks actually launch and so you get kind of these best you know test in class id say templates that are pretested around things like how to test for hallucination how to test for toxicity how to test for correctness and then you can kind of go and we have people who then go off and make their own custom eals but its a great place to kind of have a framework that runs both in a notebook and also in a pipeline uh very efficiently and is meant for for for kind of um you know you can swap in and out of offline and online very easily yeah because the other piece that i was thinking about all these these evow that you give it then it feels like youre not going to get very far if youre not doing custom evals have you seen that also totally i think theres a lot of folks who are building you know maybe they start with something but then they end up kind of building their own what makes sense for their application adding on to it so i i definitely think at the end of the day the nuance here is its probably different than the ml space is that that customization of that eil ends up being really important to measuring whats whats important to your app um so i i dont know that thats one of the things i predict because were going to see a lot a lot more of this and do you feel because one thing that ive seen is like how when you put out these different evaluation test sets the next model is just trained on top of them and so then theyre obsolete and so its its going to be this game of cat and mouse in a way because the models are going to be able to eat up the test sets as soon as they get put out for anyone to see or is it going to be all right ive got my evaluation test set and i just keep it inhouse im not going to let anybody else see that so that i dont paint the model yeah i its actually thing i wonder about a lot too um is as these new llms come out are they really blind to the test sets that theyre actually then evaluating them on i think um like the gemini paper i thought did a really good job of calling out they actually built their own data set that was blind and then tested on that data set and um they called that out explicitly which i thought was really important because you know as as people are sharing the results of like the next best lum etc yeah i think were all wondering like did you just you know like you know did did have access to that training data set so i i wonder that all the time too well its pretty clear these days that uh as i did not coin this term but i like it and i will say it a lot benchmarks are and so all these benchmarks on hugging face or on twitter that youll see like oh this is soa this just came out it blew everything else out of the water by whatever 10 times or you make up a number there i dont even consider that to be valuable anymore its its really like what you were saying where these things i know you actually went and you did a rigorous study on it but its so funny because we are the rest of us are just going off of vibes and were seeing oh yeah this is not really working this is not doing what it i thought it was going to do and so if i use a different model does it and then you try that and you go oh yeah okay cool this is better this is easier to prompt or this prompt is much easier to control whatever it may be and so i appreciate that you did a whole rigorous study on it i also im conscientious of time i want to make sure that i highlight that you all are doing like paper studies right and youre meeting every once in a while i think thats awesome i know that youve got a ton of smart people around and so i i can imagine youre getting all kinds of cool ideas from gathering and doing these paper studies i would encourage others to go and hang out and uh do those well put a link to the next one in the show notes so that in case anyone wants to join you and be able to ping ideas off of you thats great i still would love to hear how did you come up with the vis visualizations thats the coolest pieces i think you didnt get into that part and i want to get get to it before we go a i mean i just got got to say the um the the team teams amazing so the and i think the you know trying to find a way to bubble up you know at least in the llm space one of the cool things you know maybe youve seen some of the demos of it but like especially with the retrieval theres a lot in the edting space thats you helpful to visualize so how far away is the prompt that you have from the context that was retrieved and if youre missing any context and its super far away or its you know its its like reaching to find anything thats relevant to you know those are all really cool visualizations that you could actually surface and kind kind of help people see a little bit of okay heres my data heres what it thinks things are connected to um so yeah again you know check out phoenix love love all the notes on on the ui yeah actually it reminds me that i think one of the first people i heard the word embeddings from was you on like that first meetup that you came on back in like june 2020 around then because you were already thinking about them i think you were thinking about them from recommender systems and that angle and then has it how has that changed now no great great question um well i think i think edings are just so powerful and im im so glad that were you know were all talking about them and using them in observability because theres its its super powerful even in the llm space i think in the past theres you know folks used them like you mentioned recommendation systems image models but the llm space i mean the core basis of retrieval is based off of those word em you know the the embeddings um itself and doing that vector similarity search to fetch the neous embeddings um so i think the the use case is really really strong in in rag um for llms the you know and and and so because its such a core component it its also something that is important to now you know and just like when were going back if the retrieval is off then the response is just not going to be good and so you need a really good way to verify that what was retrieved was relevant and if theres any shift in again going back to all of this is now textual if the prompts are changing what your users are asking are different or the response of the lms are different these are all things that you can you can actually measure using embeddings and embedding drift and uh things like that so um i i think theres just maybe more use cases now than ever to to dig into edings yeah it has to be treated as a first class citizen 100 these days thats a really good point and i do i saw a recent paper speaking of papers from shrea shankar did you see that splat or spl spade i think is what it is uh talking about the prompt deltas and evaluating via the prompt delta like you have your prompt templates but then youre evaluating the prompt deltas and its like wow theres so much creativity in this space and the ability to look at how can we evaluate things differently than we are right now and see if we can get a better outcome yeah i i i think i still need to dig in to to spade specifically but yeah the um i mean i i think the the amount that the space is moving is just so fast right now its so exciting and um it is very the the one thing maybe ill just you know theres maybe two things i just wanted to like at least drop like my quick hot takes or or notes on oh lets do it this is great this is what were gna chop and put at the beginning of the episode yeah exactly right so i think theres you know i always hear this from i dont know i i just see it in the in in discussions but i see a lot of people talking about um fine tuning like really early on like their applications not even deployed and theyre like oh well our use cas is eventually were going to go back and f tuned and and um i you know i get asked from folks like hey parta does that make sense as like a step in troubleshooting an llm application um and i think one of the reasons i get that question a lot is if you just think back to you know a lot of the ai teams are now you theyve worked on traditional m theyre shifting now to llms but thats something were all very used to and very familiar with were used to training model models on data and then deploying those models and fine tuning is feels very familiar right you grab data points that it doesnt work on you finetune it and thats how you improve the performance of your model um but in this space finetuning feels like youre jumping to like level 100 when sometimes a lot of this could be you know like i was telling you in the rag case change the prompt a bit and you get vly different responses and so its like almost the thing that were like geared towards to do which is like oh it makes sense were going to training is now fine tuning and were all used to that paradigm but i i think in this space lets start with like the lowest hanging fruit and see how that improves um because i i i think you know and and drudge carpy actually drew this like really awesome image of like you know level of effort versus the roi you know kind of kind of of that effort and prompt engineering rack like theres so many things you could do to improve the lms performance before you jump into finetuning or like training your own llm so its just i think its important to like start with with something that could have the highest ry you are preaching to the choir and i laugh because i was like talking about how fine tuning to me feels like when all else fails youll throw some fine tuning at it and its like yeah thats what you need to you need to look at it as like the escape hatch almost not as step two it should be what you go to when you cant get anything else to work and try rigorously to get everything else to work because it is exactly like you said it is so much easier to just tweak the prompt then finetune it and and i didnt connect the dot on how similar the two are and like oh if were coming from the traditional ml space then its easier to jump there and be like oh well thats just because we need to finetune it and then itll do what we want it to do yeah totally um i i think theres just something very natural feeling about okay you know training is now fine tuning but its you know i think its one of those changes we all have to just just adapt with with the the space changing yeah a simulate yeah 100 excellent and then my my other hot take i guess um i its totally a hot take but um i think sometimes i hear um a lot of this you know i yeah maybe i hear it less now than than i was in the beginning um so i i hear a lot of like well its kind of continuation of the fine tuning well if i pick an open source model i can go in and finetune it more and i can and you know or i can then go and you know modify it for my use case because i i know i i have access to model weights and and do that um i think that i hear a lot of folks asking well does choosing an open source model versus a private model end up slowing down product development or like what what kind of whats kind of the pros and the cons of one versus the other um i think i was hearing a lot more of like you know almost resistance for some of you know just the private models of the beginning and a lot more of the you know open source i am so in this horce i got to say im you know all for the open source community like i think im im also all for you know whatever llm just makes your application the most successful it can be um so pick pick the one that like gets you the performance and the outcomes that you need i dont think that like um you know some people make a bet on the open source because theyre like a later i can go back and find tun or its better and etc but its again how many of those folks are really going to actually f tune and for what ive been seeing out out in the wild starting with the open ai or gpt 4 has just been helping most people get to kind of the outcome that they need their application to get to um and so again i think i just come back to like theres you know all for the open source community all for our you know just getting your application to actually work as as good as it needs to to work but start with start with like what do you need for the application and last of like i think the hows this going to scale yeah like i conversation back in the day where youre like oh were going to need to use kubernetes for this and youre like wait a minute we have no users are you sureet youre i know youre planning for the future and this is great for the tech debt but uh we might want to just get some up on streamlet before we do anything totally totally totally and i think that thats like what i keep coming back to is like the more of these you know similar in the ml space we want to get more of these deployed actually in the real world get the br application to add value to the organization show the roi and i think that um thats really important to to the success of these llms and companies actually and the other piece to this that i find fascinating was something that llo said probably like two years ago and llo is infamous person in the community for those who do not know in the community slack and he was talking about how you need to get something in production as fast as possible because then youll find where all of the bottlenecks are youll find where everything is messing up and unless you get into production you dont necessarily know that so each day or each minute that youre not in production youre not finding all of these problems and if you can use a model to make your life easier and get you into production faster then youre going to start seeing oh maybe its the prompts or oh maybe its you know whatever the case may be where youre falling behind and youre making mistakes or the system isnt designed properly yeah absolutely so i think uh maybe as we wrap up the podcast that thats really is get stuff out as fast as you can you know evaluate the outcomes i think thats you know lm ev vales is something that i think is pretty pretty got a lot of momentum around it in in the in folks who are deploying and in the community so evaluations is important and then um i think knowing how to set up the right evals knowing how to you know benchmark your own evals um customize it um what types of eval score versus classification theres just so much nuance in that whole eval space and so as we continue to drop more research or share more stuff were learning um we well share with the community excellent parta its been absolutely fascinating having you on as always i really appreciate it and look forward to having you back awesome thanks to me ch thanks and thanks mops community hey everyone my name is apara founder of arise and the best way to stay up to dat with mlops is by subscribing to this podcast 