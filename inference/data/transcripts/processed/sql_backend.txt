select star from table is a universal sql syntax that will basically return all columns in the table that youre selecting its been said that select star is slow and to avoid it like dont return on column because it is slow but i want to actually dive deep into the reasons of why select star is is slow some of these reasons you might already know hey were turning 50 columns is of course lower than returning one but i think some of these reasons you might not have think about because it really involves understanding the entire stack and the backend engineering fundamentals and specifically the network aspects of things as well you know and of course programming logic memory management how the os works all of this really affects the performance of your queries and selecting all the fields while convenient because you dont have to enumerate the fields you want its really it can impact the performance in general so i wrote up an article on medium that im gonna review in this video im going to add my comments and going through the reasons of all of these things how about we jump into it alright so all right so lets get started how slow is select star lets read this little bit of a paragraph and then start discussing this though in a row store database engine rows are stored in units called pages boy dont get me started now were already in the first sentence and i already like thinking about this stuff right after i ride this and then coming back and says wow there is just so much to these things like a page is and a block is the most overloaded term in in in software engineering that is right you have no idea how the word page shows up in the entire stack you know from the database to the file system to the operating system to the uh ssd itself right or to the drive itself the page oh pages are all different in sizes and specifically here were talking about database pages right and then database pages are are really fixed size like all the databases that i know of use fixed size pages or just just think of it structure i suppose with headers and the content of the page is essentially mostly rows or if you have like a column store its going to be the columns right for that first row and then the columns for the second rows it depends on the implementations and if you have documents its going to be the document so its going to have graphs its going to be the graphs and so on right and of course this little bit changes if this is an index page versus a heap page but essentially thats the gist of it right so everything is at the page effectively right and this is an example i put where this is how the postgres page looks like theres a bunch of headers and these are called the the tuple pointers and they specifically specify where does this tuple start in byte 134 and it has a length of 20 bytes right and the second table starts at 155 and it has 10 and this is basically lives in page zero those are pages there but so technically speaking if you really think about it the row is stored in the page with all its columns so the first question is since i am when i read a page i get all the rows and in that page and i get all the cons so technically select star should be cheap right because i already have all the columns in line quote unquote online in that page right because thats how raw store stores things basically were dealing with transactional you know workload in this case so well always assume raw store but but then why is why do people tell us that select start is slow and thats basically what we need to understand here right you see every time you fetch lets say lets explain first of all how do we read something right i want to read a row right select star from table where id equal one lets say this is a student id right so assuming there is no index what the database will do is okay were going to do a full table scan so there is your table and the table is literally one file and it is organized as an array of these pages that we talked about this fixed size pages so what what the database will do is again i need to do a full table scan and what that means is i need to scan the pages one by one in the file so im gonna read page 0 from the file how do i get page zero remember when you read from disk you have very a certain operations when you want to read right you you read you specify the file descriptor where do you want to read from where the starting position is and how much bytes you want to read thats it and the how much thing is i think up to two gig thats the limiting and linux at least okay that thats all you already got so how does that convert to pages when it comes to databases well once you understand the fundamentals is this is all simple stuff right the database to read page zero page 0 starts at the zeroth position in the file so the opposite is zero and page the page size is also fixed right in postgres is 8k in annual db mysql is 16 right and so the length is 8k its all effects so you might say how do i read page seven then well seven is literally page seven is seven times the length of all the pages that went before it right so seven times eight plus one so you actually start the next page right the next buy and then you read 8k right and so on thats thats basically how you read pages once you the database reads the these raw bytes and and these in reading verses theres little more to it than then just i say read right i dont like to say these things anymore i like to understand how things work theres theres layers and layers and layers of things underneath that read operation that is the file systems involved the the bytes are converted into file system blocks and these blocks are mapped to the ssd blocks or the sectors or in the in the drive and those blocks are physically retrieved you know because the api to read offset length is not consistent across the the the the the the the storage driver right we dont read bytes we read blocks assuming this is a block storage right we we deal mostly with block storage so everything is a block so if you want to read a single byte you read whatever the number of minimum number of block the file system allows you to and thats 4k in most cases all right so im gonna im gonna talk about this maybe in another video so so just so i dont i dont go off track here but once you read that um once that page is read from the ssd transfer to the file system into many blocks and now we have the raw bytes the operating system have the raw bytes in memory hot right the database now takes those place those memory location and space them into something called the shared buffers right its the databases on cache where those pages will be lived so that hopefully someone will want to query something that is in the same page so i just pull to this shared memory effectively it is the reason is shared memory because the databases will spin up multiple processes of course most databases deal with multiprocesses and these multiprocesses all of these executing multiple transaction that you need to see a unified view of of all the pages and this is where the shared buffers are located right so thats what we do so when i do a select id right one oh thats uh page zero oh well we dont really know its page zero we just scan we read page zero now look lets read row by one is is the id1 is id1 is the id1 and then we find it if we didnt find it we read the next page page one and then it verified it is page page two and so on right until we find that id and once we find it we really have everything because the page have all the columns in line im gonna put an asterisk on that so thats thats how things work now now lets go to the reasons because now everything is there why is select star slow right lets first go to the first reason case index only index only scans goodbye first of all youre right right if youre actually reading the heap which is the table data youre right you have that select store you can have everything but heres what you miss on lets assume you have an index like a student lets go back to the example of students right if i am a student and i have a table student there is a the table uh theres an id field there is a grade there is a name theres a bunch of other stuff right so now i have the student who has an id that is a name theres a gray theres a bunch of other columns as well right so now as you might have a grade index an index on the great field so if i do a query says okay give me all the student ids im just interested in the ids that scored more than 90 right mark a mark of 90 or higher well if youre great index and assume lets assume this is my sequel right or sql server and i always i picked those two letters because they they store secondary indexes a little bit different than postgresway and were going to come to the boss guys its just its very similar there here so now because im doing a grid grid is greater than 90 definitely im going to use the index that is there right so as im scanning the index and again the scanning the index is going through the b trees and the b3s have layers and these layers store these keys and the keys are stored in pages and the pages are stored on disk very similar thing its just that data structure is different now but but the content is still pages youre reading pages now we have read this we read through this beautiful index and we found as were scanning right because the index is ordered the grades because we are having an index on the grade its order so the ones i find the 90 i find 90 and 90 and 90 and another 90 and a 91 and 92 everything is ordered and and and because the value of the secondary index in the leave page is the primary key right again this is lets see my sequel right uh or oracle of the you have an index organized uh table right then you technically have the id because the csa the id is the primary key right you have the id right there in the leave page in the index so your your work is done technically rock is done i said you found the the grade you have over there of course is right there and you have also the the primary key which is the id which is what you want so it becomes what we call a covering uh indexes scan or in in postgres speak is called index only scans right so you only really need to scan the index because you dont really need anything else you just ask for the id but the moment you do select star that optimization is just screwed the database is you really had to ask for the all the fields are you really did have to do that damn it now says okay you need the id but you also need the name and you need the date of birth and you need uh his documents and whatever right so now what the database needs to do is collect all these ids which our primary id is and now it has to do turn around and do another lookup its called index six right on the primary key hey i have i found id 7 scored more than 90 and 11 and 1007 and 1008 and and and 20 0007 they have to be random right because you dont you have no idea that the students who score more than 90 are actually in order right youll be lucky but its never the case this is called random reads so random reads are the worst right i wanted you to the database random everything is the worst when it comes to ssds reading from desk reading of databases you want to avoid those but sometimes you cant right so now you have a collection of these puppies you turn around and then do another scan or a seek on the primary index so you say okay i want to find this a student this student so youll be youll be scatter shot all over the index which will cause eventually many i o to desk why because you just chose to do a star instead of asking for what you want so really really slow stuff in postgres right in postgres by default when you just create an index on the grade right always has to do that even if you ask for the id it has to go back to the table because uh postgres secondary indexes which is everything is secondary there are there are no primary indexes in in postgres the the value is actually called the tuple id which is where we explained right here right a typical ideas is literally is literally a two pair which is page index and the index of the tuple or the row in that page right thats thats what it is so like this in this particular case page 0 comma one is the first row 02 0 comma two is is the second tuple and so on right so so that thats thats whats happening to and posca stores these tuples so it has to go back but in postgres you also can have a covering indexes where you can create an index and you include columns from the heap from the table in the index right such that that index will have the id or will have anything else you want so you can get that beautiful index only scan so if you have such index and your frontend application on this case is actually is the backend server right that actually executes the query in this case it is a frontend to the database right well will not utilize this beautiful index on this castle so thats thats the first case index only scans you basically kiss them goodbye when you do a select star right you might say okay i dont care the random reads affecting from this is really bad okay lets talk about deserialization cost correct when you select star the columns live in the page but getting them out and into the protocol that will be eventually delivered to the app will require something called deserialization because these are just a bunch of bytes at this point they are raw bytes in the page because the page is just literally a memory once it its pulled into the memory right through right you you may lock it effective effectively its just a bunch of bytes right so so now you have to actually parse right it says okay theres this row and the first this is the first column this is the second column this is the third column and once you get the content of this column you actually have to coerce you have to deserialize this from byte down to the data type of whatever this thing is if this is an integer you cast it to an integer if this is a long if this is a double it has to do a double if this is a custom data type you cast it to this custom data type theres a string request a string that also has a cost not much but it can add up if you have a lot of columns so deserialization cost you can really add up because you dont really know how many fields that exist in advance right hes doing select store you have to do all of that stuff right and and that decentralization of course again were talking about the day at the database level here converting the page raw byte to data structure that then that data structure will be serialized i suppose down to the network eventually right but but but but dealing with them into a number or a value the conversion needs work and and it needs to be stored in its own form as a variable as a data structure so youre actually allocating more memory if you think about it just to store these things right building those raw objects all right so thats uh legislation cost right not much but it can add up okay not all columns are in line first of all what do we what do you mean by inline when we say in inline when i say inline i mean when i say when i have a raw store where the rows are literally one after the other row all columns first row all columns second row all columns third row all comes just one after the other right the columns that are appearing inside the row ill call it in line in the same page right there its called inline columns okay and youre going to see this word being used all the time like inline functions right and compiler like like inlining sometimes gives poor performance things right but the problem here is remember when we said pages are fixed size i forgot to mention one thing which is rows cannot really span multiple pages if you have like what do you have like a really big role with a i dont know 20 000 columns cant happen but if you have that then the raw technically cannot fit you will have one page with a single row and that row will not fit that page because its so big right so you you will have to spin up another page to complete that role database never allowed to do that because the complexity to find the row becomes exponentially harder so no databases as well as far as im aware allow rows to be spanned between multiple pages just because of the complexity if you think about it right so rose cannot span badges because of that then hussain what do i do if i have one field of the strength that has the entire work of shakespeare in it thats what databases do like think about this or or blob or lobs right things that are actually binary that are stored in as a column jason jasons are rarely stored in line like json documents like in postgres or other databases you cant because you cant fit a page so what happens is those are assigned a pointer and when i say a pointer dont think of a c sharp position dont think of a c pointer here and its actual its an its a unique identifier that points back to that blob right so what happens is a a marker is created and that thing is placed on an external table right and and mysql has it poscus has its called toast right they have to have it theres no way out you have to have some external place to store your stuff right there are they cannot fit into the page and as a result if you think about it this actually what limits the number of columns why dont databases allow unlimited number of columns because of this reason right because at the end of the day you can have a nonvariable static type integer like the integer is not variable right its a fixed size its either four byte or eight okay 64 or 32 right if you have that then how many actually you cannot really put that outside it doesnt make sense right thats an integer right so so so what happens is you are limited by the whatever the number of columns based on certain calculation you can do the math as well so what what databases do is they take that long strength that long document that long blob that long json put it in another table put an id to it and that id is instead is stored in its very tiny like one or two bytes or even less inline so you only that column only have the pointer that identifier of the external storage its never stored in line right and again it depends on the size of the thing right sometimes if you have like 128 characters supposed to say eh okay ill put it in line right there is there is a threshold to decide that but mostly large stuff are stored outside and guess what theyre often compressed so there is a cost if you actually say give me that json give me that document give me that strength youre asking the database yeah youre saying its already in line uh no i got this but i have to now do another query to another table called the toast table and postgres the oversize something uh text or something i forgot what it stands for okay and that is an additional io because that page where that toast exists might not be in the shared buffers and as a result youre just closed another i o look at the work youre doing look at the pain youre causing youre causing pain an io call goes to the disk and the disc suffers every time you hit it with an i o right next time you do a select star think about the suffering youre causing to all this equipments okay i tried to say this with the straight fair but i couldnt okay so especially that because these are things are compressed right and you can disable compression if you want but that will just blow to your storage if you dont care about storage sure but then not only youre pulling it from an i o external now either a cpu involved now youre making the cpu work for you right youre uncompressing stop because uncompressing has to be done in memory right now im just like ah let me uncompress huffman encoding whatever right gzip all this stuff is happening now geometry stuff you deal with geometry i do with geometry all day thats my basically my my nine to five okay geometry stuff gis right so so yeah this is really expensive right so its doing select store now youre silently and and the sad part is if you ask the neighbors to do all this stuff to pull all these blobs to pull these strings and you just in the client you just say i really just needed the id well i dont really need the rest of this stuff all right its like asking someone to bring you all this stuff right and bring you a dish of food or something but you only take one thing you say oh i i dont want that anymore how rude how rude really okay not all comms are online so be careful be very careful network cost when was the last time youre back and running the same time in the same place as the database never youd ever put the database in the same place as the back end right they dont they never run in the same processes in the same host right sure you can run in the same physical halls on a separate container right but i dont know why i would do that but you might have virtualization set up but still you have to separate them there is networking involved right and and of course you need to keep them as close as possible so the latency is low right you dont want to put one in us west and the second one is the database is you and us east and the back end application in us west and you expect queries to be fast no you have to put them in the same network zone hopefully right and if if you cant then create replicas and have the replicas close by right i think i created a one video at some point i think i think it was clever keep your database closed and your no keep your servers closed and your database closer i think i think that was clever a clever way of saying it did i just call myself clever because kids kids these days will call me crunch so uh right network cost so now that we talked about okay you selected all the fields you took the hit you did you avoided the beautiful index on the scan you took the head of database uh data structure c deserialization from the page down to their finger and then you took uh uh uh what was that what else yes uh you you told me to get all these text fields and blobfish and jason fields so i had to go to a toastable an external table decompress them and and also put them in a data structure so the double double trouble and then now im ready to send it back across the network now each database has its own networking protocol right right it could be most used like binary specific proponentation i dont see i didnt see really a a unified uh and thats something i i talk about sometimes in my channel where there is no unified database protocol doesnt exist every database just does its own thing right unlike the web we have http right its a unified web protocol anything web you go http right and theres like the the use cases uh went beyond web right so everywhere else like this is the de facto standard some people brought http down to the database i dont know how how well that performs to be honest but but every database does its own thing like a sql server has its own postgres has its own redis has its own everybody database i was like ah i want to build my own and then now you you realize that data structures you have down serialized it down to bytes down to this protocol uh language that you have right so the serialization aspect to to that protocol application level protocol right your application at that the database protocol application in this case right whatever that structure is right and then that goes into the the transport protocol which we dont have anything else tcp thats all what we have right so it goes into down to ip packets and then segments sorry and then ip packets and those ip packets are shipped right so now if youre sending really really really really large columns the response from that protocol the database response the database sql result will be so large such that the network transmission you will feel it especially if you have like larger latency you will feel it because now i need to write and the the maximum i can write at a time is the mtu which is the maximum transmission unit defined by the ip layer like layer three and thats basically is defined based on on path discovery mtu right the the the the the the the protocol that discovers what is the weakest link in this network that im transferring this is okay what is the minimum i can send or maximum in this case i can set 1500 is the default on the internet you can send lower there are some routers like a support like 500 like very old routers by 1500 is the standard there are routers with jumbo frames that suppose like nine thousand and stuff like that and they are even large but thats the kind of standard so you can only send 15 bytes at a time in the ip and that even its even lower the tcp segment layer right i know im going into very details but this is the point in all my videos i always go into details right i dont make like one minutes videos and deal with it i i like to explore this thing because i i am passionate about this stuff so i apologize if you dont like this content but i really do like to go into these details it couldnt brush everything like i understand everything remove the black boxes so now if you have a lot of data even if you do compress it which is an additional cpu database overhead then you still have to transmit it and the the conjunction algorithm protocols and slow start will allow you to send multiple of these 1500 or 1460 segments without waiting for an acknowledgment right and initially youre going to send one i suppose 10 recently 10 segments at a time like google increased the initial window size for the congestion window to 10 i suppose because like there is no point the intern is really fast these days so let me send 10 at a time so 10 times 15 thats like what thats 15k this is 15k right so you can say 15k at a time if you want right at a stocked right and then but but not all network supports that right so it always starts slow and then it creates increase increases slowly right incremental increases so now as you go through this youre sending that but eventually the client have to start acknowledging this it has to process this and it only the client only acknowledged something that actually was able to receive right so if there is a data loss guess what the client didnt receive it the server timeout will hit and then well retransmit it so all of this youll feel it then more network you have latency the more you will kind of feel it thats why its like i always feel like applications if you want to really test applications like when it comes to performance like just just put put the database and the back end in a very really you know uh far apart i know its a bad idea right to do that in production but but it will it will force you to optimize because youre gonna you youre gonna send as small as possible and youre gonna receive as small as possible data youre gonna say okay ah why am i sending this much let me send less right so its like putting yourself in slow mode such that you feel the pain uh off of these kind of things right so returning all columns require decentralization right of of these large columns such as strengths blob the client will never use thats the sad part all of this youre encoding all the high cost of transmission for nothing nothing at all and then we finally talk about the client deserialization where now that i actually the client received all this raw tcp segments from the neck the neck actually transferred it to the os through dma and then that becomes just robots and the sigma and the os now just say okay whats for this listener where is this going this is ip this ip this destination all right it is this socket so it will start transmitting your data to the client app which is the the basically if you have no nodejs thats the nodejs app and specifically the library that you use for the database uh uh client right so if its a postgres client that will be that that library will be called and youre at the mercy of how this library is authored how is it parsing the protocol how is it how is it understanding this stuff thats why lazy parsing is also a thing and a client side where uh lets actually wait for the client to consume some of the stuff such that you can avoid the head of client deserialization and client building all these objects fetch has the same thing by the way right the fish api fetch api when you call it it will it will it will give you the headers only it will give you the response but itll give you the just the byte the raw body right you cannot actually do anything with the body until you call body dot json function so go ahead and actually took take the head to actually this realizer and the reason they do fish does it this way is because uh you might only lead the headers for some reason or you might only need the response code right the status code or content lag so this is the way such that the app the client will consume what it needs and when it actually is the content to the body it will do json or dot text to actually move it from binary down to the deserialization down to the data structure in this case its json because compared to json is expensive right now im assuming its the response is json in this case right and uh so so the decentralization is costly so if you can do it lazily thats even better but how about that if you can avoid sending data that you never use thats even better for the back end isnt it right so yeah guys thats thats all what i have for you today right so we talked about hawaii select star is really slow really really really slow i might have missed a reason or not let me know in the those comments down below hope you enjoyed this video im gonna see you on the next one you guys stay awesome goodbye 