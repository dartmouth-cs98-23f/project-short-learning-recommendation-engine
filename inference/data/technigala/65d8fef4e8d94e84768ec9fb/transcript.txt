what do dynamodb apache cassandra discord and  akamai cdn have in common they all use consistent   hashing now what is consistent hashing why do all  the cool kids use it in this video well learn   208696all about it lets dive right in in a large scale  distributed system data does not fit on a single   server they are distributed across many machines  this is called horizontal scaling to build such   34852a system with predictable performance it is  important to distribute the data evenly across   400672those servers a common method to distribute  data as evenly as possible among servers is   simple hashing this is how it works first for each  object we hash its key with a hashing function   like md5 or murmurhash this maps the object  key to a known range of numerical values   620496a good hashing function distributes the  hashes evenly across the entire range   second we perform the modulo operation on the  hash against the number of servers this determines   which servers the object belongs to as long  as the number of servers stays the same   an object key will always map to the same server  heres a concrete example we have four servers   871256with eight string keys with simple hashing  this is how we distribute the eight string   927268keys among the four servers now this approach  works well when the size of the cluster is fixed   and the data distribution is even but what happens  when new servers get added to meet new demand   or when existing servers get removed  back to our example if server 1 goes down   the size of the cluster is now three even though  the hashes for the object keys stay the same   we are now applying the modulo operation to a  different set of n in this case it is now three   the impact is pretty drastic most of the keys get  redistributed this affects almost all objects its   not just the objects originally stored in the  server that is now offline this triggers a storm   1399260of misses and lots of objects to be moved for  situations where servers constantly come and go   this design is untenable consistent hashing is  an effective technique to mitigate this issue   the goal of consistent hashing is this we want  almost all objects to stay assigned to the same   158868server even as the number of servers changes  here is the core insight of consistent hashing   in addition to hashing the object keys like before  we also hash the server names the objects and   1727268servers are hashed with the same hashing function  to the same range of values in our example we have   a range of x0 to xn this range is called a hash  space next we connect both ends of the hash space   to form a ring this is a hash ring using a hashing  function we hash each server by its name or ip   1964896address and place the server onto the ring here we  place our four servers onto the ring next we hash   each object by its key with the same hashing  function unlike simple hashing where we perform   a modulo operation on the hash here we use the  hash directly to map the object key onto the ring   2188568here is what it would look like for our four  objects to locate the server for a particular   object we go clockwise from the location of the  object key on the ring until a server is found   2316784continue with our example key 0 is on server 0  and key 1 is on server 1 now lets take a look at   what happens when we add a server here we insert  a new server s4 to the left of s0 on the ring   note that only k0 needs to be moved from s0 to  s4 this is because s4 is the first server k0   encounters by going clockwise from k0s position  on the ring keys k1 k2 and k3 are not affected   with simple hashing when a new server is added  almost all the keys need to be remapped with   consistent hashing adding a new server only  requires redistribution of a fraction of the keys   lets walk through a quick example of removing  a server when s1 is removed only k1 needs to be   remapped to s2 the rest of the keys are unaffected  lets recap what have we learned so far one   2941660we map both servers and objects onto the hash  ring using a uniformly distributed hash function   two to locate a server for an object we go  clockwise on the ring from the objects position   until a server is found now lets consider  a potential issue with this design   the distribution of the objects in the  servers on the ring is likely to be uneven   conceptually we pick n random points on the ring  we are very unlikely to get a perfect partition of   3259268the ring into equally sized segments for example  if servers are mapped on the ring like this   3333684most of the objects are stored in s2 with s1  and s3 storing no data this problem gets worse   if servers come and go frequently in our example  even if the servers were originally evenly spaced   3488648if s1 is removed the segment for s2 is now  twice as large as the ones for s0 and s3   356864virtual nodes are used to fix this problem the  idea is to have each server appear at multiple   3632744locations on the ring each location is a virtual  node representing a server in this hash ring we   have two servers with each having three virtual  nodes instead of having s0 and s1 we now have   3791296s00 s01 and s02 to represent server 0 and s10  s11 and s12 to represent server 1 on the ring   with virtual nodes each server handles multiple  segments on the ring in our example the segments   3968624labeled s0 are managed by server 0 and  those labeled s1 are handled by server 1   4040448in real world systems the number of  virtual nodes is much larger than   three as the number of virtual nodes increases  the distribution of objects becomes more balanced   4156856having more virtual nodes means taking more space  to store the metadata about the virtual nodes   this is a tradeoff and we can tune the number  of virtual nodes to fit our system requirements  4276576lets see how consistent hashing is used in  the real world some popular nosql databases   4333668like amazon dynamodb and apache cassandra use  consistent hashing where it is used for data   partitioning it helps these databases minimize  data movement during rebalancing content delivery   networks like akamai use consistent hashing to  help distribute web contents evenly among the   edge servers load balancers like google load  balancer use consistent hashing to distribute   4608568persistent connections evenly across backend  servers this limits the number of connections   4664864that need to be reestablished when a backend  server goes down thats it for consistent hashing   if you would like to learn more about system  design check out our books and weekly newsletters   4780536please subscribe if you learned something new  thank you so much and well see you next time 