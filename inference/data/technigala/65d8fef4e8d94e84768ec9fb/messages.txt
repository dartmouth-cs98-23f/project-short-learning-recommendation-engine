[{"role": "user", "content": "Transcript:\nhello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of innovations in the code especially when they use the x forers library with the block attention so i want to guide you into understanding the code because it can be really hard for beginners to understand and find thems around there are some topics that are related to mistal but will not be covered in this current video because i already covered them in my previous video about llama and in particular i will not be talking about the rms normalization the rotary positional encodin  and we will see later uh how it works but we just remember that we have eight experts and for each token we use two experts but later i will clarify how it works lets proceed further so lets talk about the sliding window attention but before we talk about the sliding window attention i need to review a little bit of the self attention mechanism so what is self attention self attention is a mechanism that allows the model to relate tokens to each other so tokens that are in the same sentence are related with each other through the self attention mechanism this is why its called self attention because each token is watching other tokens of the of the same sentence and when when and this is means basically that the query key and values are the same metrix um so imagine we have the following  only depend on itself the second token will only depend on the first two tokens the third output token will only depend on the first three tokens the fourth will depend on the token number two because the first token is not used the token number two the token number three and the token number four etc etc until the last here the last token will depend only on the last three tokens because the first three tokens are masked out and this is the importance of the mask that we apply in the self attention mechanism this concept that i show you now is very important to understand the rest of the video so please if you didnt understand it you can take a little pause you can try to do it by your by yourself because its really important that you understand how the self attention mechanism works with his will become the new input which will produce love that can then we take the last token of the output and we append it to the input for the time step four so it will become the new output will become love that can quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the last token output by the model because we already have the previous one but of course we need to feed all the previous tokens to uh to to the model which is belonging to the prompt because the model ne aling with a cy cach we need to build up this kv cache so we need to add the tokens of our prompt to the kv cache that so that we can then exploit this kv cache to build new tokens future tokens now the prompt is known in advance right because because its the input of our user its what you ask to ch gpd for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so its known in advance so we dont we dont need to generate it okay so what we can do is we can prefill the kv cache using the tokens of the prompt but there are many ways to do it like we were doing before when i was teaching you about the k cach we work with one token at a time so one way to um to add the tokens to the k cach is to add one token at at a time but this can be very time consuming be m of the wids to be only one otherwise the output range may change from model to model and usually its not a good idea to have the range of the output to change from one model to the next so to keep the range of the output stable they apply the soft marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so lets talk about it when we have a model that is too big to fit in a single gpu we can divide the model into groups of layers and place each group of layers in a single gpu for example in the case of mistal we have 32 layers of encoders you can see here one after another i didnt do  and then we buil this sequence which is a concatenation of all the prompts that we receive we take this mega sequence we run it through our llm ser llm model so it could be mistal or it could be llama this as i told you before uh and input sequence in a transformer will result in n output tokens in the output so we have uh here we have three  4 so 7 7  5 12 tokens as input it will produce 12 tokens as output to understand what is the next token for each prompt we need to check the the we need to check the embedding corresponding to the token number three for the first prompt to the token number seven for the second prompt and the last token for the third prompt so we take all these embeddings we run them through the linear layer then we apply the soft marks and then we understand what is  because my computer is not very powerful so i never run the actual model on my computer what i did to study the model was to run some random tensors through a model and i created basically a model with randomly initialized uh weights but with less number of layers so it could fit in my gpu and then i just run some random tensors to study all the shapes of the tensor and all the information passing so i dont know if the code works but i hope it will works i mean i didnt touch the logic i just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model i actually really enjoyed studying it i really enjoyed studying the code and i learned a lot of stuff you know um i think its very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you dont learn much by the end of the day anyway guys thanks you for watching my video i hope you also enjoyed this journey with me even if it was very complicated i hope that you likeed this video and you will subscribe to my channel if you didnt please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on linkedin on twitter etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you dont understand i am always available to help and connect with me on linkedin byebye \nInstruction:\nGeneral Topics:\n[\n  \"Algorithms and Data Structures\",\n  \"Artificial Intelligence (AI) and Machine Learning\",\n  \"Computer Architecture\",\n  \"Data Science and Analytics\",\n  \"Database Systems and Management\",\n  \"Human-Computer Interaction (HCI)\",\n  \"Programming Languages and Software Development\",\n  \"Software Engineering and System Design\",\n  \"Web Development and Internet Technologies\",\n  \"Computer Graphics and Visualization\",\n  \"Theoretical Computer Science\",\n  \"Quantum Computing\"\n]\n\nComplexity scale:\n[\n  0.00-0.09: Elementary,\n  0.10-0.19: Middle School,\n  0.20-0.29: High School,\n  0.30-0.39: Advanced High School,\n  0.40-0.49: Introductory College,\n  0.50-0.59: Average College Lecture,\n  0.60-0.69: Advanced College,\n  0.70-0.79: Graduate Level,\n  0.80-0.89: Advanced Graduate Level,\n  0.90-1.00: PhD Level Lecture\n]\n\nGenerate a JSON summary the computer science or software engineering video. The summary should include:\n\n1. A high-level synopsis labeled as \"introduction\".\n2. A breakdown into 5 sections, each with:\n   - A title starting with \"Section x\" where x is the section number from 1 to 5.\n   - A \"content\" field containing 4 bullet points summarizing the section.\n   - A \"topics\" field listing the 3 most important topics covered in the section.\n3. A global \"topics\" array listing 5 key descriptors (1-2 words each) of topics covered in the video.\n4. A global \"general topics\" array selecting 3 topics from the predefined list, each associated with an exact \"complexity\" score based on the provided scale. The complexity score should reflect the detailed complexity level with full two-decimal-place precision.\n\nThe JSON structure should strictly follow this template:\n{\n  \"introduction\": \"High level synopsis here\",\n  \"sections\": [\n    {\n      \"title\": \"Section 1\",\n      \"content\": [\n        \"Bullet point 1\",\n        \"Bullet point 2\",\n        \"Bullet point 3\",\n        \"Bullet point 4\"\n      ],\n      \"topics\": [\"Topic1\", \"Topic2\", \"Topic3\"]\n    },\n    ... (total 5 sections)\n  ],\n  \"topics\": [\"Descriptor1\", \"Descriptor2\", \"Descriptor3\", \"Descriptor4\", \"Descriptor5\"],\n  \"general topics\": [\n    {\n      \"name\": \"Selected general topic 1\",\n      \"complexity\": \"Exact complexity float with two decimals\"\n    },\n    ... (total 3 general topics)\n  ]\n}\n\nPlease adhere strictly to the JSON format. Use domain-specific terminology in the \"title\" and \"content\" for each section. The \"topics\" and \"general topics\" should accurately reflect the video's content based on the predefined lists and complexity scale. The output should conclude with a '}' character to end the json above.\n"}, {"role": "assistant", "content": "{\n  \"introduction\": \"This video presents an in-depth analysis and tutorial on the Mistral language model, developed by Mistral AI. It covers architectural differences from traditional models, key features like sliding window attention, kv cache, and innovations such as sparse mixture of experts, model sharding, and the use of the xformers library for efficient processing. The presenter also delves into coding aspects, highlighting how these advanced concepts are implemented.\",\n  \"sections\": [\n    {\n      \"title\": \"Section 1: Introduction to Mistral\",\n      \"content\": [\n        \"Overview of Mistral AI and its emergence as a leading startup in language models.\",\n        \"Distinction between Mistral's 7 billion and 8x7 billion models.\",\n        \"Comparison of Mistral's architecture to vanilla transformers.\",\n        \"Introduction to sliding window attention and its benefits.\"\n      ],\n      \"topics\": [\"Mistral AI\", \"Architecture Comparison\", \"Sliding Window Attention\"]\n    },\n    {\n      \"title\": \"Section 2: Advanced Attention Mechanisms\",\n      \"content\": [\n        \"Explanation of sliding window attention and its implementation.\",\n        \"The concept of receptive fields in relation to convolutional neural networks.\",\n        \"Introduction to the kv cache and its significance.\",\n        \"The evolution of kv cache to rolling buffer cache for efficiency.\"\n      ],\n      \"topics\": [\"Sliding Window Attention\", \"Receptive Fields\", \"KV Cache\"]\n    },\n    {\n      \"title\": \"Section 3: Sparse Mixture of Experts\",\n      \"content\": [\n        \"Overview of the mixture of experts model and its application in Mistral.\",\n        \"Differentiation between sparse and dense mixture of experts models.\",\n        \"Implementation of gating mechanisms to select experts.\",\n        \"Benefits of using a sparse mixture for performance and model capacity.\"\n      ],\n      \"topics\": [\"Mixture of Experts\", \"Gating Mechanisms\", \"Sparse Mixture Benefits\"]\n    },\n    {\n      \"title\": \"Section 4: Model Sharding and Pipeline Parallelism\",\n      \"content\": [\n        \"Introduction to model sharding and its necessity for large models.\",\n        \"Comparison of traditional model training to pipeline parallelism.\",\n        \"How pipeline parallelism optimizes GPU utilization.\",\n        \"Potential challenges and solutions in implementing pipeline parallelism.\"\n      ],\n      \"topics\": [\"Model Sharding\", \"Pipeline Parallelism\", \"GPU Utilization\"]\n    },\n    {\n      \"title\": \"Section 5: Code Innovations and xformers Library\",\n      \"content\": [\n        \"The complexity of Mistral's code and the challenge for beginners.\",\n        \"Use of the xformers library for memory-efficient attention calculations.\",\n        \"Creation of complex attention masks for variable-length inputs.\",\n        \"Importance of block attention and its implications for model performance.\"\n      ],\n      \"topics\": [\"Code Complexity\", \"xformers Library\", \"Block Attention\"]\n    }\n  ],\n  \"topics\": [\"Language Models\", \"Attention Mechanisms\", \"Model Optimization\", \"GPU Efficiency\", \"Code Implementation\"],\n  \"general topics\": [\n    {\n      \"name\": \"Artificial Intelligence (AI) and Machine Learning\",\n      \"complexity\": \"0.87\"\n    },\n    {\n      \"name\": \"Software Engineering and System Design\",\n      \"complexity\": \"0.68\"\n    },\n    {\n      \"name\": \"Programming Languages and Software Development\",\n      \"complexity\": \"0.65\"\n    }\n  ]\n}"}, {"role": "user", "content": "Transcript:\nwhats up guys back today with another longer video which is to build a job board app that could actually be a real startup that people use theres a lot of different places to find entry level or junior developer jobs on the internet for example github jobs stack overflow jobs you have angel list you have indeed but filtering through those especially because you kind of have to spam applications for your first job can take a long time so the good thing about all these job boards is they have public apis for the most part you always want to start with an mvp that is the minimum viable product today that means were going to start with a single data feed that is a single job board api feeding into our app and then finally were gonna have a super minimal simple front end were gonna do this in react and material ui on the front end were gonna use an express server redis for our data storage because were not writing or persisting any data that is not part of the job feed and then finally a node cron job to fetch these jobs and refresh them at a certain interval which are going to be filtered through an algorithm so lets draw this all out to get a better idea of our plan heres our front end what we basically want here is a list of jobs this is going to be pulling data from our api and that will be express and then this will pull from our store which is actually going to be redis and redis will be populated by our cron called a worker which is pulling from music one or more apis its going to be eventually multiple this is going to be job uh board apis so like in lets say indeed we h ew terminal tab going out to our main directory and within our main directory im just going to create a package json which is going to be accessible from all our different folders so you can see that was created here and then we just want to install cron as a dependency okay now inside a worker i just this is the way i pretty much start with every uh with every app you just copy kind of the basic usage and you build your way out from there so all this does is its running a node process where were importing the library and then this is called a um cron shorthand cron syntax something like that uh cronguru you can see it here okay so this site is really useful no not this site crown guru crontab guru okay so these asterisks are called a cron schedule expression and its basically symbols for  ur redis store and then you can serve it up in our case jumping back over to these um these job uh json that we got from github you can see every job has a uuid which is a guaranteed unique id because it has so many characters and its random characters so itll be guaranteed unique every time every job has one of these ids so thats a natural candidate to use as a redis key im thinking within redis our key is going to be this id and our value is just going to be the whole job object so it would be a string version of this that we can then pull out we know its already json so we can just stringify it and then when we pull it out well just parse it into a javascript object now the slightly tricky part of this is okay how do we access redis through our node um worker right because we have all t  length matches our our length that we put in lets parse it and then lets do dot lake okay so its kind of a lot that we did without testing it lets restart our server do the same curl oh man okay 245 i dont remember if thats how many we put in were definitely getting a list of 245 which is a good sign now lets just try sending this back and see if we get that full uh stringified object thats exactly what we want and its that easy so thats already done so i dont know how long its been guys but with this simple of an api we can literally weve literally been able to create all this like in such a short amount of time we have a data feed and you can replicate like before before i go further you can replicate this in any this is like 90 applications if you have a data feed you have storage you   function and um all right finally im going to just put this whole thing below our uh jobs map and um lets see if by pasting it directly in we we need to do anything else just going to make sure thats formatted correctly saved it and um lets just take this out classes direction rtl okay lets see how theme is being used because this is kind of a um darn all right i dont know why that would change from left to right but um it might be a mobile implementation specific thing so lets just take this and hard code it as right and well hard code this one as left and see if we run in any problems okay save that and cant find module oh i forgot about this material ui icons is a separate library so we have to also install that in our client yarn add material ui icons now while thats installing lets t configurations and ill put all these links in the description so like i mentioned first uh we need a domain i already bought one on google domains but you can buy a domain a ton of different places i just like google because the interface and its because where my uh other domains are so ive opened my domain i already bought it a normalcom domain is going to be about 14 a year and the first thing we need to do is use the digital ocean name servers so that domain can be directed over to our server so youll see that i have clicked on my domain ive opened the dns tab and then theres a name servers tab and ive clicked use custom name servers now im going to put that on pause and jump over to the digital ocean dashboard you can make a digitalocean account for free and itll give you this dashboar  one critical change that will allow us to access our api on our local server and that is to basically change this to just a relative path so im changing this to just slash jobs and actually i want to change it to be a little more clear to be slash api jobs now that means we also have to change our api route which shouldnt be a problem slash api slash jobs and we have to make one change in our package json and thats adding a proxy config which allows this to still work on local development this doesnt affect anything on production so um proxying all api routes to a target of localhost sorry need the double quotes there http localhost uh 3001 was our api server i believe and ill save that and again that just helps us you know still develop in the same way but we need to make one config chan ocal if i click into these jobs lets see uh it works theres our description logo and everything and then if i click apply itll take us to that position so thats pretty cool guys we just deployed our whole app and it didnt even take that long i feel like that was around 30 minutes or so all right guys thats it we basically built a whole production application in less than 100 minutes and we got it deployed and now its running on the public internet at entrylevelsoftwarejobscom so go check it out but um im thinking next theres a few critical things we have to do before were ready for like a full production launch so to speak this was just our base framework that were building more features off of but at minimum i think we need a few more data feeds and we need to make our algorithm a lot more robust prevent it from breaking on errors as well as filtering down those jobs a lot more because if we get in five feeds were just gonna have a huge amount of jobs and we really wanna make sure those are entry level jobs so im thinking that we parse through the description and look for like three to five years of experience and take those out i think anything less than three years would qualify as an entrylevel job so again were gonna have to keep working on the algorithm and then otherwise were gonna want to add a little bit of monitoring logging different things like that but that can all come later our mvp is done and our base of our application is done so thats pretty cool so anyway hope you learned something um let me know if you have any questions and i will talk to you guys soon \nInstruction:\nGeneral Topics:\n[\n  \"Algorithms and Data Structures\",\n  \"Artificial Intelligence (AI) and Machine Learning\",\n  \"Computer Architecture\",\n  \"Data Science and Analytics\",\n  \"Database Systems and Management\",\n  \"Human-Computer Interaction (HCI)\",\n  \"Programming Languages and Software Development\",\n  \"Software Engineering and System Design\",\n  \"Web Development and Internet Technologies\",\n  \"Computer Graphics and Visualization\",\n  \"Theoretical Computer Science\",\n  \"Quantum Computing\"\n]\n\nComplexity scale:\n[\n  0.00-0.09: Elementary,\n  0.10-0.19: Middle School,\n  0.20-0.29: High School,\n  0.30-0.39: Advanced High School,\n  0.40-0.49: Introductory College,\n  0.50-0.59: Average College Lecture,\n  0.60-0.69: Advanced College,\n  0.70-0.79: Graduate Level,\n  0.80-0.89: Advanced Graduate Level,\n  0.90-1.00: PhD Level Lecture\n]\n\nGenerate a JSON summary the computer science or software engineering video. The summary should include:\n\n1. A high-level synopsis labeled as \"introduction\".\n2. A breakdown into 5 sections, each with:\n   - A title starting with \"Section x\" where x is the section number from 1 to 5.\n   - A \"content\" field containing 4 bullet points summarizing the section.\n   - A \"topics\" field listing the 3 most important topics covered in the section.\n3. A global \"topics\" array listing 5 key descriptors (1-2 words each) of topics covered in the video.\n4. A global \"general topics\" array selecting 3 topics from the predefined list, each associated with an exact \"complexity\" score based on the provided scale. The complexity score should reflect the detailed complexity level with full two-decimal-place precision.\n\nThe JSON structure should strictly follow this template:\n{\n  \"introduction\": \"High level synopsis here\",\n  \"sections\": [\n    {\n      \"title\": \"Section 1\",\n      \"content\": [\n        \"Bullet point 1\",\n        \"Bullet point 2\",\n        \"Bullet point 3\",\n        \"Bullet point 4\"\n      ],\n      \"topics\": [\"Topic1\", \"Topic2\", \"Topic3\"]\n    },\n    ... (total 5 sections)\n  ],\n  \"topics\": [\"Descriptor1\", \"Descriptor2\", \"Descriptor3\", \"Descriptor4\", \"Descriptor5\"],\n  \"general topics\": [\n    {\n      \"name\": \"Selected general topic 1\",\n      \"complexity\": \"Exact complexity float with two decimals\"\n    },\n    ... (total 3 general topics)\n  ]\n}\n\nPlease adhere strictly to the JSON format. Use domain-specific terminology in the \"title\" and \"content\" for each section. The \"topics\" and \"general topics\" should accurately reflect the video's content based on the predefined lists and complexity scale. The output should conclude with a '}' character to end the json above.\n"}, {"role": "assistant", "content": "{\n  \"introduction\": \"This video guides viewers through building a job board application that aggregates entry-level developer job postings from various APIs. It covers the entire development process, including setting up the front end with React and Material UI, backend server with Express, data storage using Redis, and regular job fetching using a cron job. The project aims at simplifying the job search for entry-level positions by filtering out senior roles, focusing on a practical approach to developing a real-world application.\",\n  \"sections\": [\n    {\n      \"title\": \"Section 1: Project Setup and Initial Planning\",\n      \"content\": [\n        \"Introduction to the job board app concept, focusing on entry-level developer positions.\",\n        \"Decision to aggregate job postings from various APIs to simplify the job search process.\",\n        \"Outline of the tech stack: React and Material UI for the front end, Express for the backend, Redis for data storage, and node-cron for job fetching.\",\n        \"Initial setup of the project directory and installation of necessary libraries and frameworks.\"\n      ],\n      \"topics\": [\"Project Planning\", \"Tech Stack Overview\", \"Initial Setup\"]\n    },\n    {\n      \"title\": \"Section 2: Frontend Development with React and Material UI\",\n      \"content\": [\n        \"Creation of the React app using create-react-app and setup of Material UI.\",\n        \"Building the front-end structure, including components for job listings and individual job details.\",\n        \"Introduction to React hooks for state management and API calls.\",\n        \"Styling of the front end with Material UI components for a user-friendly interface.\"\n      ],\n      \"topics\": [\"React Setup\", \"Component Design\", \"State Management with Hooks\"]\n    },\n    {\n      \"title\": \"Section 3: Backend Server and Data Handling\",\n      \"content\": [\n        \"Setup of the Express server to handle API requests and serve the front end.\",\n        \"Introduction to Redis as an in-memory data store for caching job postings.\",\n        \"Implementation of a node-cron job to fetch new job postings at regular intervals.\",\n        \"Design of data structure and filtering algorithm to exclude senior-level jobs from the results.\"\n      ],\n      \"topics\": [\"Express Server Setup\", \"Redis Data Storage\", \"Cron Job Implementation\"]\n    },\n    {\n      \"title\": \"Section 4: Integrating Job Board APIs\",\n      \"content\": [\n        \"Exploration of various job board APIs such as GitHub Jobs and Stack Overflow Jobs.\",\n        \"Implementation of API calls within the node-cron job to fetch job postings.\",\n        \"Discussion on handling API pagination and data aggregation.\",\n        \"Filtering and processing job data to match the application's criteria for entry-level positions.\"\n      ],\n      \"topics\": [\"API Integration\", \"Data Fetching\", \"Pagination Handling\"]\n    },\n    {\n      \"title\": \"Section 5: Deployment and Final Touches\",\n      \"content\": [\n        \"Review of the completed application and its functionalities.\",\n        \"Discussion on potential improvements and additional features for future development.\",\n        \"Steps for deploying the application to a server using nginx and PM2 for process management.\",\n        \"Final testing and demonstration of the application in action, showcasing its ability to aggregate and display job postings.\"\n      ],\n      \"topics\": [\"Application Review\", \"Future Improvements\", \"Deployment Process\"]\n    }\n  ],\n  \"topics\": [\"React\", \"Express\", \"Redis\", \"API Integration\", \"Deployment\"],\n  \"general topics\": [\n    {\n      \"name\": \"Web Development and Internet Technologies\",\n      \"complexity\": 0.59\n    },\n    {\n      \"name\": \"Software Engineering and System Design\",\n      \"complexity\": 0.61\n    },\n    {\n      \"name\": \"Database Systems and Management\",\n      \"complexity\": 0.53\n    }\n  ]\n}\n"}, {"role": "user", "content": "Transcript:\nwhat do dynamodb apache cassandra discord and\u00a0 akamai cdn have in common they all use consistent\u00a0\u00a0 hashing now what is consistent hashing why do all\u00a0 the cool kids use it in this video well learn\u00a0\u00a0 208696all about it lets dive right in in a large scale\u00a0 distributed system data does not fit on a single\u00a0\u00a0 server they are distributed across many machines\u00a0 this is called horizontal scaling to build such\u00a0\u00a0 34852a system with predictable performance it is\u00a0 important to distribute the data evenly across\u00a0\u00a0 400672those servers a common method to distribute\u00a0 data as evenly as possible among servers is\u00a0\u00a0 simple hashing this is how it works first for each\u00a0 object we hash its key with a hashing function\u00a0\u00a0 like md5 or murmurhash this maps the object\u00a0 key to a known range of numerical values\u00a0\u00a0 620496a good hashing function distributes the\u00a0 hashes evenly across the entire range\u00a0\u00a0 second we perform the modulo operation on the\u00a0 hash against the number of servers this determines\u00a0\u00a0 which servers the object belongs to as long\u00a0 as the number of servers stays the same\u00a0\u00a0 an object key will always map to the same server\u00a0 heres a concrete example we have four servers\u00a0\u00a0 871256with eight string keys with simple hashing\u00a0 this is how we distribute the eight string\u00a0\u00a0 927268keys among the four servers now this approach\u00a0 works well when the size of the cluster is fixed\u00a0\u00a0 and the data distribution is even but what happens\u00a0 when new servers get added to meet new demand\u00a0\u00a0 or when existing servers get removed\u00a0 back to our example if server 1 goes down\u00a0\u00a0 the size of the cluster is now three even though\u00a0 the hashes for the object keys stay the same\u00a0\u00a0 we are now applying the modulo operation to a\u00a0 different set of n in this case it is now three\u00a0\u00a0 the impact is pretty drastic most of the keys get\u00a0 redistributed this affects almost all objects its\u00a0\u00a0 not just the objects originally stored in the\u00a0 server that is now offline this triggers a storm\u00a0\u00a0 1399260of misses and lots of objects to be moved for\u00a0 situations where servers constantly come and go\u00a0\u00a0 this design is untenable consistent hashing is\u00a0 an effective technique to mitigate this issue\u00a0\u00a0 the goal of consistent hashing is this we want\u00a0 almost all objects to stay assigned to the same\u00a0\u00a0 158868server even as the number of servers changes\u00a0 here is the core insight of consistent hashing\u00a0\u00a0 in addition to hashing the object keys like before\u00a0 we also hash the server names the objects and\u00a0\u00a0 1727268servers are hashed with the same hashing function\u00a0 to the same range of values in our example we have\u00a0\u00a0 a range of x0 to xn this range is called a hash\u00a0 space next we connect both ends of the hash space\u00a0\u00a0 to form a ring this is a hash ring using a hashing\u00a0 function we hash each server by its name or ip\u00a0\u00a0 1964896address and place the server onto the ring here we\u00a0 place our four servers onto the ring next we hash\u00a0\u00a0 each object by its key with the same hashing\u00a0 function unlike simple hashing where we perform\u00a0\u00a0 a modulo operation on the hash here we use the\u00a0 hash directly to map the object key onto the ring\u00a0\u00a0 2188568here is what it would look like for our four\u00a0 objects to locate the server for a particular\u00a0\u00a0 object we go clockwise from the location of the\u00a0 object key on the ring until a server is found\u00a0\u00a0 2316784continue with our example key 0 is on server 0\u00a0 and key 1 is on server 1 now lets take a look at\u00a0\u00a0 what happens when we add a server here we insert\u00a0 a new server s4 to the left of s0 on the ring\u00a0\u00a0 note that only k0 needs to be moved from s0 to\u00a0 s4 this is because s4 is the first server k0\u00a0\u00a0 encounters by going clockwise from k0s position\u00a0 on the ring keys k1 k2 and k3 are not affected\u00a0\u00a0 with simple hashing when a new server is added\u00a0 almost all the keys need to be remapped with\u00a0\u00a0 consistent hashing adding a new server only\u00a0 requires redistribution of a fraction of the keys\u00a0\u00a0 lets walk through a quick example of removing\u00a0 a server when s1 is removed only k1 needs to be\u00a0\u00a0 remapped to s2 the rest of the keys are unaffected\u00a0 lets recap what have we learned so far one\u00a0\u00a0 2941660we map both servers and objects onto the hash\u00a0 ring using a uniformly distributed hash function\u00a0\u00a0 two to locate a server for an object we go\u00a0 clockwise on the ring from the objects position\u00a0\u00a0 until a server is found now lets consider\u00a0 a potential issue with this design\u00a0\u00a0 the distribution of the objects in the\u00a0 servers on the ring is likely to be uneven\u00a0\u00a0 conceptually we pick n random points on the ring\u00a0 we are very unlikely to get a perfect partition of\u00a0\u00a0 3259268the ring into equally sized segments for example\u00a0 if servers are mapped on the ring like this\u00a0\u00a0 3333684most of the objects are stored in s2 with s1\u00a0 and s3 storing no data this problem gets worse\u00a0\u00a0 if servers come and go frequently in our example\u00a0 even if the servers were originally evenly spaced\u00a0\u00a0 3488648if s1 is removed the segment for s2 is now\u00a0 twice as large as the ones for s0 and s3\u00a0\u00a0 356864virtual nodes are used to fix this problem the\u00a0 idea is to have each server appear at multiple\u00a0\u00a0 3632744locations on the ring each location is a virtual\u00a0 node representing a server in this hash ring we\u00a0\u00a0 have two servers with each having three virtual\u00a0 nodes instead of having s0 and s1 we now have\u00a0\u00a0 3791296s00 s01 and s02 to represent server 0 and s10\u00a0 s11 and s12 to represent server 1 on the ring\u00a0\u00a0 with virtual nodes each server handles multiple\u00a0 segments on the ring in our example the segments\u00a0\u00a0 3968624labeled s0 are managed by server 0 and\u00a0 those labeled s1 are handled by server 1\u00a0\u00a0 4040448in real world systems the number of\u00a0 virtual nodes is much larger than\u00a0\u00a0 three as the number of virtual nodes increases\u00a0 the distribution of objects becomes more balanced\u00a0\u00a0 4156856having more virtual nodes means taking more space\u00a0 to store the metadata about the virtual nodes\u00a0\u00a0 this is a tradeoff and we can tune the number\u00a0 of virtual nodes to fit our system requirements\u00a0 4276576lets see how consistent hashing is used in\u00a0 the real world some popular nosql databases\u00a0\u00a0 4333668like amazon dynamodb and apache cassandra use\u00a0 consistent hashing where it is used for data\u00a0\u00a0 partitioning it helps these databases minimize\u00a0 data movement during rebalancing content delivery\u00a0\u00a0 networks like akamai use consistent hashing to\u00a0 help distribute web contents evenly among the\u00a0\u00a0 edge servers load balancers like google load\u00a0 balancer use consistent hashing to distribute\u00a0\u00a0 4608568persistent connections evenly across backend\u00a0 servers this limits the number of connections\u00a0\u00a0 4664864that need to be reestablished when a backend\u00a0 server goes down thats it for consistent hashing\u00a0\u00a0 if you would like to learn more about system\u00a0 design check out our books and weekly newsletters\u00a0\u00a0 4780536please subscribe if you learned something new\u00a0 thank you so much and well see you next time \nInstruction:\nGeneral Topics:\n[\n  \"Algorithms and Data Structures\",\n  \"Artificial Intelligence (AI) and Machine Learning\",\n  \"Computer Architecture\",\n  \"Data Science and Analytics\",\n  \"Database Systems and Management\",\n  \"Human-Computer Interaction (HCI)\",\n  \"Programming Languages and Software Development\",\n  \"Software Engineering and System Design\",\n  \"Web Development and Internet Technologies\",\n  \"Computer Graphics and Visualization\",\n  \"Theoretical Computer Science\",\n  \"Quantum Computing\"\n]\n\nComplexity scale:\n[\n  0.00-0.09: Elementary,\n  0.10-0.19: Middle School,\n  0.20-0.29: High School,\n  0.30-0.39: Advanced High School,\n  0.40-0.49: Introductory College,\n  0.50-0.59: Average College Lecture,\n  0.60-0.69: Advanced College,\n  0.70-0.79: Graduate Level,\n  0.80-0.89: Advanced Graduate Level,\n  0.90-1.00: PhD Level Lecture\n]\n\nGenerate a JSON summary the computer science or software engineering video. The summary should include:\n\n1. A high-level synopsis labeled as \"introduction\".\n2. A breakdown into 5 sections, each with:\n   - A title starting with \"Section x\" where x is the section number from 1 to 5.\n   - A \"content\" field containing 4 bullet points summarizing the section.\n   - A \"topics\" field listing the 3 most important topics covered in the section.\n3. A global \"topics\" array listing 5 key descriptors (1-2 words each) of topics covered in the video.\n4. A global \"general topics\" array selecting 3 topics from the predefined list, each associated with an exact \"complexity\" score based on the provided scale. The complexity score should reflect the detailed complexity level with full two-decimal-place precision.\n\nThe JSON structure should strictly follow this template:\n{\n  \"introduction\": \"High level synopsis here\",\n  \"sections\": [\n    {\n      \"title\": \"Section 1\",\n      \"content\": [\n        \"Bullet point 1\",\n        \"Bullet point 2\",\n        \"Bullet point 3\",\n        \"Bullet point 4\"\n      ],\n      \"topics\": [\"Topic1\", \"Topic2\", \"Topic3\"]\n    },\n    ... (total 5 sections)\n  ],\n  \"topics\": [\"Descriptor1\", \"Descriptor2\", \"Descriptor3\", \"Descriptor4\", \"Descriptor5\"],\n  \"general topics\": [\n    {\n      \"name\": \"Selected general topic 1\",\n      \"complexity\": \"Exact complexity float with two decimals\"\n    },\n    ... (total 3 general topics)\n  ]\n}\n\nPlease adhere strictly to the JSON format. Use domain-specific terminology in the \"title\" and \"content\" for each section. The \"topics\" and \"general topics\" should accurately reflect the video's content based on the predefined lists and complexity scale. The output should conclude with a '}' character to end the json above.\n"}]