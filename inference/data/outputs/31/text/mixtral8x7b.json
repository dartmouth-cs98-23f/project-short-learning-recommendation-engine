Hello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will be exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistral later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce you to the concept of rolling buffer cache and also the concept of prefilling and chunking uh uh um