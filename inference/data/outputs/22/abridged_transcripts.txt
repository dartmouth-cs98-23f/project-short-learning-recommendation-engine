hello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of innovations in the code especially when they use the x forers library with the block attention so i want to guide you into understanding the code because it can be really hard for beginners to understand and find thems around there are some topics that are related to mistal but will not be covered in this current video because i already covered them in my previous video about llama and in particular i will not be talking about the rms normalization the rotary positional encodin  and we will see later uh how it works but we just remember that we have eight experts and for each token we use two experts but later i will clarify how it works lets proceed further so lets talk about the sliding window attention but before we talk about the sliding window attention i need to review a little bit of the self attention mechanism so what is self attention self attention is a mechanism that allows the model to relate tokens to each other so tokens that are in the same sentence are related with each other through the self attention mechanism this is why its called self attention because each token is watching other tokens of the of the same sentence and when when and this is means basically that the query key and values are the same metrix um so imagine we have the following  only depend on itself the second token will only depend on the first two tokens the third output token will only depend on the first three tokens the fourth will depend on the token number two because the first token is not used the token number two the token number three and the token number four etc etc until the last here the last token will depend only on the last three tokens because the first three tokens are masked out and this is the importance of the mask that we apply in the self attention mechanism this concept that i show you now is very important to understand the rest of the video so please if you didnt understand it you can take a little pause you can try to do it by your by yourself because its really important that you understand how the self attention mechanism works with his will become the new input which will produce love that can then we take the last token of the output and we append it to the input for the time step four so it will become the new output will become love that can quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the last token output by the model because we already have the previous one but of course we need to feed all the previous tokens to uh to to the model which is belonging to the prompt because the model ne aling with a cy cach we need to build up this kv cache so we need to add the tokens of our prompt to the kv cache that so that we can then exploit this kv cache to build new tokens future tokens now the prompt is known in advance right because because its the input of our user its what you ask to ch gpd for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so its known in advance so we dont we dont need to generate it okay so what we can do is we can prefill the kv cache using the tokens of the prompt but there are many ways to do it like we were doing before when i was teaching you about the k cach we work with one token at a time so one way to um to add the tokens to the k cach is to add one token at at a time but this can be very time consuming be m of the wids to be only one otherwise the output range may change from model to model and usually its not a good idea to have the range of the output to change from one model to the next so to keep the range of the output stable they apply the soft marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so lets talk about it when we have a model that is too big to fit in a single gpu we can divide the model into groups of layers and place each group of layers in a single gpu for example in the case of mistal we have 32 layers of encoders you can see here one after another i didnt do  and then we buil this sequence which is a concatenation of all the prompts that we receive we take this mega sequence we run it through our llm ser llm model so it could be mistal or it could be llama this as i told you before uh and input sequence in a transformer will result in n output tokens in the output so we have uh here we have three  4 so 7 7  5 12 tokens as input it will produce 12 tokens as output to understand what is the next token for each prompt we need to check the the we need to check the embedding corresponding to the token number three for the first prompt to the token number seven for the second prompt and the last token for the third prompt so we take all these embeddings we run them through the linear layer then we apply the soft marks and then we understand what is  because my computer is not very powerful so i never run the actual model on my computer what i did to study the model was to run some random tensors through a model and i created basically a model with randomly initialized uh weights but with less number of layers so it could fit in my gpu and then i just run some random tensors to study all the shapes of the tensor and all the information passing so i dont know if the code works but i hope it will works i mean i didnt touch the logic i just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model i actually really enjoyed studying it i really enjoyed studying the code and i learned a lot of stuff you know um i think its very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you dont learn much by the end of the day anyway guys thanks you for watching my video i hope you also enjoyed this journey with me even if it was very complicated i hope that you likeed this video and you will subscribe to my channel if you didnt please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on linkedin on twitter etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you dont understand i am always available to help and connect with me on linkedin byebye 

whats up guys back today with another longer video which is to build a job board app that could actually be a real startup that people use theres a lot of different places to find entry level or junior developer jobs on the internet for example github jobs stack overflow jobs you have angel list you have indeed but filtering through those especially because you kind of have to spam applications for your first job can take a long time so the good thing about all these job boards is they have public apis for the most part you always want to start with an mvp that is the minimum viable product today that means were going to start with a single data feed that is a single job board api feeding into our app and then finally were gonna have a super minimal simple front end were gonna do this in react and material ui on the front end were gonna use an express server redis for our data storage because were not writing or persisting any data that is not part of the job feed and then finally a node cron job to fetch these jobs and refresh them at a certain interval which are going to be filtered through an algorithm so lets draw this all out to get a better idea of our plan heres our front end what we basically want here is a list of jobs this is going to be pulling data from our api and that will be express and then this will pull from our store which is actually going to be redis and redis will be populated by our cron called a worker which is pulling from music one or more apis its going to be eventually multiple this is going to be job uh board apis so like in lets say indeed we h ew terminal tab going out to our main directory and within our main directory im just going to create a package json which is going to be accessible from all our different folders so you can see that was created here and then we just want to install cron as a dependency okay now inside a worker i just this is the way i pretty much start with every uh with every app you just copy kind of the basic usage and you build your way out from there so all this does is its running a node process where were importing the library and then this is called a um cron shorthand cron syntax something like that uh cronguru you can see it here okay so this site is really useful no not this site crown guru crontab guru okay so these asterisks are called a cron schedule expression and its basically symbols for  ur redis store and then you can serve it up in our case jumping back over to these um these job uh json that we got from github you can see every job has a uuid which is a guaranteed unique id because it has so many characters and its random characters so itll be guaranteed unique every time every job has one of these ids so thats a natural candidate to use as a redis key im thinking within redis our key is going to be this id and our value is just going to be the whole job object so it would be a string version of this that we can then pull out we know its already json so we can just stringify it and then when we pull it out well just parse it into a javascript object now the slightly tricky part of this is okay how do we access redis through our node um worker right because we have all t  length matches our our length that we put in lets parse it and then lets do dot lake okay so its kind of a lot that we did without testing it lets restart our server do the same curl oh man okay 245 i dont remember if thats how many we put in were definitely getting a list of 245 which is a good sign now lets just try sending this back and see if we get that full uh stringified object thats exactly what we want and its that easy so thats already done so i dont know how long its been guys but with this simple of an api we can literally weve literally been able to create all this like in such a short amount of time we have a data feed and you can replicate like before before i go further you can replicate this in any this is like 90 applications if you have a data feed you have storage you   function and um all right finally im going to just put this whole thing below our uh jobs map and um lets see if by pasting it directly in we we need to do anything else just going to make sure thats formatted correctly saved it and um lets just take this out classes direction rtl okay lets see how theme is being used because this is kind of a um darn all right i dont know why that would change from left to right but um it might be a mobile implementation specific thing so lets just take this and hard code it as right and well hard code this one as left and see if we run in any problems okay save that and cant find module oh i forgot about this material ui icons is a separate library so we have to also install that in our client yarn add material ui icons now while thats installing lets t configurations and ill put all these links in the description so like i mentioned first uh we need a domain i already bought one on google domains but you can buy a domain a ton of different places i just like google because the interface and its because where my uh other domains are so ive opened my domain i already bought it a normalcom domain is going to be about 14 a year and the first thing we need to do is use the digital ocean name servers so that domain can be directed over to our server so youll see that i have clicked on my domain ive opened the dns tab and then theres a name servers tab and ive clicked use custom name servers now im going to put that on pause and jump over to the digital ocean dashboard you can make a digitalocean account for free and itll give you this dashboar  one critical change that will allow us to access our api on our local server and that is to basically change this to just a relative path so im changing this to just slash jobs and actually i want to change it to be a little more clear to be slash api jobs now that means we also have to change our api route which shouldnt be a problem slash api slash jobs and we have to make one change in our package json and thats adding a proxy config which allows this to still work on local development this doesnt affect anything on production so um proxying all api routes to a target of localhost sorry need the double quotes there http localhost uh 3001 was our api server i believe and ill save that and again that just helps us you know still develop in the same way but we need to make one config chan ocal if i click into these jobs lets see uh it works theres our description logo and everything and then if i click apply itll take us to that position so thats pretty cool guys we just deployed our whole app and it didnt even take that long i feel like that was around 30 minutes or so all right guys thats it we basically built a whole production application in less than 100 minutes and we got it deployed and now its running on the public internet at entrylevelsoftwarejobscom so go check it out but um im thinking next theres a few critical things we have to do before were ready for like a full production launch so to speak this was just our base framework that were building more features off of but at minimum i think we need a few more data feeds and we need to make our algorithm a lot more robust prevent it from breaking on errors as well as filtering down those jobs a lot more because if we get in five feeds were just gonna have a huge amount of jobs and we really wanna make sure those are entry level jobs so im thinking that we parse through the description and look for like three to five years of experience and take those out i think anything less than three years would qualify as an entrylevel job so again were gonna have to keep working on the algorithm and then otherwise were gonna want to add a little bit of monitoring logging different things like that but that can all come later our mvp is done and our base of our application is done so thats pretty cool so anyway hope you learned something um let me know if you have any questions and i will talk to you guys soon 

this video is about a new approach to training language models specifically it allows you to move the probabilities of a language model away from bad answers and towards good answers direct preference optimization or dpo for short is a type of reinforcement learning its a more efficient type than has long been used by companies like open ai and meta in developing lama 2 and with this dpo technique it makes it much easier to find you models towards a chat format that is aligned in a similar way to those commercial models available for agenda were going to take a look at how normal training works standard finetuning or supervised finetuning as ive done in previous videos then ill contrast how direct preference optimization works and why its a bit different in terms of a technique and its useful in a different and complimentary way to standard finetuning then ill show you two dpo data sets the data sets need to be in a specific format if youre going to use the hugging face trainer that we will use today and then ill walk step by step through the training notebook ive developed for doing direct preference optimization by the end of this tutorial you should be able to take a model youve developed a language model ideally you do some supervised finetuning to get it into a chat format and this will allow you to do the final stage of aligning the model um another way to think of that is moving the model probabilities away from bad answers and towards good answers that will make sense as we get a bit deeper into this video lets start off with a description very high level of how standard training works standard language model training involves penalizing the model based on predicted what it predicts for the next token versus the actual next token for example if you feed in a phrase or part of a phrase that says the capital of ireland is and you ask the model to predict the next token if the model predicts dublin and then you wouldnt free penalize it because that answer is the next token here in this sentence but if it says something like cork then you would penalize the model and back back propagate through so fundamentally language models are statistical models that look at the frequency of information that is in the training data set if i train a language model with the capital of ireland is dublin the capital of ireland is dublin the capital of ireland is cork well if will see dublin more frequently and so that is more likely to be returned when the model predicts the next token if i want to bias the model towards saying dublin with a very high probability i can just keep feeding in the capital of ireland is dublin the captain ireland is dublin and eventually the relative frequency it is seen for dublin will be much greater than cork and so it is going to respond with the word dublin as the answer and that if you generalize it is how language models work vel what were doing here is imagine a training data set so youve come along with the model youve done some supervised fine tuning its been trained on a trillion tokens there are lots and lots of data points here within um the data set and in that data set of course because weve trained it on the internet there are some bad answers some answers that you know people have written things on the internet that arent correct and then there are some answers we consider good answers uh where maybe theres more of these but theres not enough more to statistically get the right answer all of the time so we have this data set body with bad answers and great answers and how direct preference optimization works is by using pairs of answers by feeding it some pairs we bias the model away from bad answers and we bias it towards great ideally great but certainly better answers so again to contrast this with standard training if we want to bias the model towards better answers with standard training the only way we can do that is by feeding more great answers so with standard training you have to keep feeding in more of these great answers the bad answers will still be there in the data set but over time these great answers will start to dominate as we train on more and more sets of the capital of ireland is dublin whereas by contrast in dpo were penalizing the model for a bad answer and were incentivizing it for a good answer now im giving a ve what we want to incentivize the model towards is i dont know thats an interesting question so its kind of harmless certainly not helpful but its kind of harmless and the idea is to pull the model away from giving out dangerous answers and then the rejected answer might be something like go and blow xyz um and this clearly is some kind of a dangerous answer and this is what you would be trying to bias the model away from this is the data set that were going to use today for demonstrating dpo im going to demonstrate it on tiny lama which is a 1 billion model its a fairly small model which allows me to do the dpo training process fairly quickly i think in about 45 minutes i was able to train on 16the overall data set and so the time for doing training on dpo its roughly about twice the time that you need for doing supervised finetuning because you are running inference forward through two models the reference model which is duplicate of the original model and the model that youre training so you have twice the amount of forward passes and although youve got the same amount of backward passes probably is a rule of th your training in dpo is going to take twice the length as doing finetuning another caveat before i move to show you the notebook is that when you train a model to be harmless like this youre training the model when it gets a question to say something thats kind of benign and useless and while that can be helpful becau aid i would be training on this is a model that i have trained with supervised finetuning using the open assist data set its available here um for you to download if you like and as i said its a chat finetuned model tiny lama is only trained on a 2k context length i did the finetuning using 4k context it helps a little bit with the 2k performance but its not good enough to get the model to be a 4k data set so well be considering it a 2k data set even though that i call it 4k because i used 4k long or up to 4k long uh data set which is open assist and with that i think were ready to move on and do some finetuning in the notebook you can get access to the notebook by purchasing it through the link below or you can purchase access to the full advanced finetuning repository which now has not just dpo but scripts for embeddings function calling training long context uh quantization supervised finetuning and unsupervised fine tuning quite a lot of scripts in that advanced finetuning repo and quite a lot of members of it right now all right lets get started and here we are with the direct preference optimiz ation notebook im going to take you through it step by step so right up the top we are going to connect with hugging face this will allow us to push models and also if youre accessing a or finetuning a gated model its necessary to do the login lately ive always been also logging in with weights and biases it allows us to view the  questions that are relevant to the data set that you choose ideally they should provide you some metric of how the model is performing were trying to fine tune here to have a model that is more harmless so were going to have to ask some ugly questions and then we want to see that it gives what wed consider an an unacceptable answer before the fine tuning and gives hopefully a more acceptable answer after the dpo so ive set up uh some questions here um yeah unfortunately a lot of this kind of training involves ugly questions um which you will ask the model and when you run that base evaluation here its bluntly answering the question which you may not want um if you want to have a model that is going to be better behaved or aligned but this provides us with a baseline then we can then that we can then compare after the training and hopefully this data set will have the effect that we intend okay so were loading the data set that i just took you through its the dpo um helpful and harmless reinforcement learning data set from anthropic so thats loaded and here you can see there is a prompt talking about dinosaurs so this is a more harmless question um but it ends with the user accusing the machine of not being able to read and the harmless answer is you can read question mark and rejected answer is a more aggressive retort which is theres a lot of stuff humans dont know okay so well move on and ive just got a test here where i tok e model is not giving um a more restrained response to the more dangerous questions so we havent achieved the goal here with dpo meaning that we would need to train perhaps for more uh more of an epoch ive only trained for 01 epochs its possible that simply the kind of nature of this question im asking here hasnt being covered in the training data set so its not being affected statistically by any of the samples that weve run through in dpo so running for a full epoch would be one way to start with this and maybe playing as well with having a little bit higher of learning rate so im going to show you the example i have with a little higher learning rate where i have 1 e minus 5 ill show you how it diverges but also it actually gives you a sense for the effect it has in the model because it does start to change the answer so right here with the 1 e minus 5 everything is very same the only difference is that when i ran the training i had it at a learning rate of 1 e minus 5 which is higher than 1 eus 6 by factor of 10 and indeed when i run that training and then i run the first sample here um you can see the answer is quite different now so when its asked about killing it starts to give an answer that um is a bit more restrained its um now asking that peoples um human rights and emotions be considered its giving a much more verbose answer trying to indicate more nuance around the topic i wouldnt say that its got the answer to as  of epoch before i go im just going to show you exactly how i set things up on runp pod um what ill do here is use an a6000 so if i go to secure cloud i should see a list of all of the servers that are available like this and this i think is probably one of the best values per unit of vram its got 48 vram for 79 cents per hour um ill before i click deploy using a pytorch instance im just going to make sure i have plenty of memory here um 50 gb is way more than enough because the model is going to be about 2 gab and ill click continue and then ill deploy so this is now going to deploy a p p torch instance and you can see i have an exited instance of an a100 that i was working on earlier and once this is booted up ill be able to connect using a jupiter notebook once the notebook uh is ready once the pod is ready ill click connect and ill click connect to jupiter ive just uploaded a dp iynb notebook and ill open it here in the screen and im going to go through the steps that i discussed earlier starting off by and logging in with my hugging face id so here we have the hugging face id then doing the installation of weights and biases and using my login for that so that we have logs so heres my weights and biases id and next up we will do the installation so install all of packages necessary move on to loading the model here as i said not loading at quantize because its a small model so there shouldnt be any issue with doing that a e and we have a learning rate schedular type of a constant im actually going to move back to the original um recommended learning rate well you know i might leave 1 e minus 6 but instead of using constant i think i will use linear here so the learning rate will decrease as we move through all of the epoch and you can see ive set up the results folder here for one epoch and ill go ahead and get started now to train the model it should set up awaits and biases run you can see here its got 10steps to go through instead of 1its going to be quite a bit longer as a training and im going to install my plot lib so i can plot the results at the end ill do a quick evaluation this here i can remove can remove all of these and i will push the model to hugging face so that i have a copy of it so ill just go ahead and push that adapter and additionally i dont need to upload any other trainable parameters because theyre all included in the laura adapter so i should be free to just merge and unload the model and then i should be free to push the tokenizer to hugging face hub and push the model to hugging face hub um what i might do is push tokenizer do model which is a file in the original repo that file is needed to do gptq and also ggf quantizations so it can be handy to just push that as well so ill scroll up here now and take a look at the training which should be underway and here the trainer is up and running and you can see were now 17 steps into 10about um its about an 8 hour run so fairly long even though this uh is quite a small model just a 1 billion parameter model so just gives you a feel for the amount of time if youre doing a full fine tune u rather if youre doing a lower fine tune on the model and here we can just click on the weights and biases and we should be able to pull up a copy of this run and you can see here training loss is progressing and im going to just go to overview and just put in here what the run is about one e minus 6 and well say one e but run tiny like this and thats it ill post up later how the run goes that folks is an overview of direct preference optimization it is a lot easier than doing reinforcement learning where you need to train a separate helper model still doing dpo direct preference op optimization its not an easy feat and you need to pay a lot of attention to having a data set thats comprehensive choosing some questions for evaluation that allow you to tell whether your training is progressing or not you also need to be comfortable with chat find tuning a model which is a form of supervised finetuning if you want to get started with something a little easier i recommend going back over the videos for embeddings unsupervised fine tuning and supervised fine tuning with respect to this video ill put all of the links on resources below and you can find out more about getting access to the scripts and data sets cheers 

#####################
Shot: mixtral8x7b
#####################
#####################
Shot: full-stack
#####################
#####################
Target: dpo
#####################
hello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of innovations in the code especially when they use the x forers library with the block attention so i want to guide you into understanding the code because it can be really hard for beginners to understand and find thems around there are some topics that are related to mistal but will not be covered in this current video because i already covered them in my previous video about llama and in particular i will not be talking about the rms normalization the rotary positional encodin  and we will see later uh how it works but we just remember that we have eight experts and for each token we use two experts but later i will clarify how it works lets proceed further so lets talk about the sliding window attention but before we talk about the sliding window attention i need to review a little bit of the self attention mechanism so what is self attention self attention is a mechanism that allows the model to relate tokens to each other so tokens that are in the same sentence are related with each other through the self attention mechanism this is why its called self attention because each token is watching other tokens of the of the same sentence and when when and this is means basically that the query key and values are the same metrix um so imagine we have the following  only depend on itself the second token will only depend on the first two tokens the third output token will only depend on the first three tokens the fourth will depend on the token number two because the first token is not used the token number two the token number three and the token number four etc etc until the last here the last token will depend only on the last three tokens because the first three tokens are masked out and this is the importance of the mask that we apply in the self attention mechanism this concept that i show you now is very important to understand the rest of the video so please if you didnt understand it you can take a little pause you can try to do it by your by yourself because its really important that you understand how the self attention mechanism works with his will become the new input which will produce love that can then we take the last token of the output and we append it to the input for the time step four so it will become the new output will become love that can quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the last token output by the model because we already have the previous one but of course we need to feed all the previous tokens to uh to to the model which is belonging to the prompt because the model ne aling with a cy cach we need to build up this kv cache so we need to add the tokens of our prompt to the kv cache that so that we can then exploit this kv cache to build new tokens future tokens now the prompt is known in advance right because because its the input of our user its what you ask to ch gpd for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so its known in advance so we dont we dont need to generate it okay so what we can do is we can prefill the kv cache using the tokens of the prompt but there are many ways to do it like we were doing before when i was teaching you about the k cach we work with one token at a time so one way to um to add the tokens to the k cach is to add one token at at a time but this can be very time consuming be m of the wids to be only one otherwise the output range may change from model to model and usually its not a good idea to have the range of the output to change from one model to the next so to keep the range of the output stable they apply the soft marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so lets talk about it when we have a model that is too big to fit in a single gpu we can divide the model into groups of layers and place each group of layers in a single gpu for example in the case of mistal we have 32 layers of encoders you can see here one after another i didnt do  and then we buil this sequence which is a concatenation of all the prompts that we receive we take this mega sequence we run it through our llm ser llm model so it could be mistal or it could be llama this as i told you before uh and input sequence in a transformer will result in n output tokens in the output so we have uh here we have three  4 so 7 7  5 12 tokens as input it will produce 12 tokens as output to understand what is the next token for each prompt we need to check the the we need to check the embedding corresponding to the token number three for the first prompt to the token number seven for the second prompt and the last token for the third prompt so we take all these embeddings we run them through the linear layer then we apply the soft marks and then we understand what is  because my computer is not very powerful so i never run the actual model on my computer what i did to study the model was to run some random tensors through a model and i created basically a model with randomly initialized uh weights but with less number of layers so it could fit in my gpu and then i just run some random tensors to study all the shapes of the tensor and all the information passing so i dont know if the code works but i hope it will works i mean i didnt touch the logic i just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model i actually really enjoyed studying it i really enjoyed studying the code and i learned a lot of stuff you know um i think its very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you dont learn much by the end of the day anyway guys thanks you for watching my video i hope you also enjoyed this journey with me even if it was very complicated i hope that you likeed this video and you will subscribe to my channel if you didnt please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on linkedin on twitter etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you dont understand i am always available to help and connect with me on linkedin byebye 

whats up guys back today with another longer video which is to build a job board app that could actually be a real startup that people use theres a lot of different places to find entry level or junior developer jobs on the internet for example github jobs stack overflow jobs you have angel list you have indeed but filtering through those especially because you kind of have to spam applications for your first job can take a long time so the good thing about all these job boards is they have public apis for the most part you always want to start with an mvp that is the minimum viable product today that means were going to start with a single data feed that is a single job board api feeding into our app and then finally were gonna have a super minimal simple front end were gonna do this in react and material ui on the front end were gonna use an express server redis for our data storage because were not writing or persisting any data that is not part of the job feed and then finally a node cron job to fetch these jobs and refresh them at a certain interval which are going to be filtered through an algorithm so lets draw this all out to get a better idea of our plan heres our front end what we basically want here is a list of jobs this is going to be pulling data from our api and that will be express and then this will pull from our store which is actually going to be redis and redis will be populated by our cron called a worker which is pulling from music one or more apis its going to be eventually multiple this is going to be job uh board apis so like in lets say indeed we h ew terminal tab going out to our main directory and within our main directory im just going to create a package json which is going to be accessible from all our different folders so you can see that was created here and then we just want to install cron as a dependency okay now inside a worker i just this is the way i pretty much start with every uh with every app you just copy kind of the basic usage and you build your way out from there so all this does is its running a node process where were importing the library and then this is called a um cron shorthand cron syntax something like that uh cronguru you can see it here okay so this site is really useful no not this site crown guru crontab guru okay so these asterisks are called a cron schedule expression and its basically symbols for  ur redis store and then you can serve it up in our case jumping back over to these um these job uh json that we got from github you can see every job has a uuid which is a guaranteed unique id because it has so many characters and its random characters so itll be guaranteed unique every time every job has one of these ids so thats a natural candidate to use as a redis key im thinking within redis our key is going to be this id and our value is just going to be the whole job object so it would be a string version of this that we can then pull out we know its already json so we can just stringify it and then when we pull it out well just parse it into a javascript object now the slightly tricky part of this is okay how do we access redis through our node um worker right because we have all t  length matches our our length that we put in lets parse it and then lets do dot lake okay so its kind of a lot that we did without testing it lets restart our server do the same curl oh man okay 245 i dont remember if thats how many we put in were definitely getting a list of 245 which is a good sign now lets just try sending this back and see if we get that full uh stringified object thats exactly what we want and its that easy so thats already done so i dont know how long its been guys but with this simple of an api we can literally weve literally been able to create all this like in such a short amount of time we have a data feed and you can replicate like before before i go further you can replicate this in any this is like 90 applications if you have a data feed you have storage you   function and um all right finally im going to just put this whole thing below our uh jobs map and um lets see if by pasting it directly in we we need to do anything else just going to make sure thats formatted correctly saved it and um lets just take this out classes direction rtl okay lets see how theme is being used because this is kind of a um darn all right i dont know why that would change from left to right but um it might be a mobile implementation specific thing so lets just take this and hard code it as right and well hard code this one as left and see if we run in any problems okay save that and cant find module oh i forgot about this material ui icons is a separate library so we have to also install that in our client yarn add material ui icons now while thats installing lets t configurations and ill put all these links in the description so like i mentioned first uh we need a domain i already bought one on google domains but you can buy a domain a ton of different places i just like google because the interface and its because where my uh other domains are so ive opened my domain i already bought it a normalcom domain is going to be about 14 a year and the first thing we need to do is use the digital ocean name servers so that domain can be directed over to our server so youll see that i have clicked on my domain ive opened the dns tab and then theres a name servers tab and ive clicked use custom name servers now im going to put that on pause and jump over to the digital ocean dashboard you can make a digitalocean account for free and itll give you this dashboar  one critical change that will allow us to access our api on our local server and that is to basically change this to just a relative path so im changing this to just slash jobs and actually i want to change it to be a little more clear to be slash api jobs now that means we also have to change our api route which shouldnt be a problem slash api slash jobs and we have to make one change in our package json and thats adding a proxy config which allows this to still work on local development this doesnt affect anything on production so um proxying all api routes to a target of localhost sorry need the double quotes there http localhost uh 3001 was our api server i believe and ill save that and again that just helps us you know still develop in the same way but we need to make one config chan ocal if i click into these jobs lets see uh it works theres our description logo and everything and then if i click apply itll take us to that position so thats pretty cool guys we just deployed our whole app and it didnt even take that long i feel like that was around 30 minutes or so all right guys thats it we basically built a whole production application in less than 100 minutes and we got it deployed and now its running on the public internet at entrylevelsoftwarejobscom so go check it out but um im thinking next theres a few critical things we have to do before were ready for like a full production launch so to speak this was just our base framework that were building more features off of but at minimum i think we need a few more data feeds and we need to make our algorithm a lot more robust prevent it from breaking on errors as well as filtering down those jobs a lot more because if we get in five feeds were just gonna have a huge amount of jobs and we really wanna make sure those are entry level jobs so im thinking that we parse through the description and look for like three to five years of experience and take those out i think anything less than three years would qualify as an entrylevel job so again were gonna have to keep working on the algorithm and then otherwise were gonna want to add a little bit of monitoring logging different things like that but that can all come later our mvp is done and our base of our application is done so thats pretty cool so anyway hope you learned something um let me know if you have any questions and i will talk to you guys soon 

whats up guys back today with another longer video which is to build a job board app that could actually be a real startup that people use theres a lot of different places to find entry level or junior developer jobs on the internet for example github jobs stack overflow jobs you have angel list you have indeed but filtering through those especially because you kind of have to spam applications for your first job can take a long time so the good thing about all these job boards is they have public apis for the most part you always want to start with an mvp that is the minimum viable product today that means were going to start with a single data feed that is a single job board api feeding into our app and then finally were gonna have a super minimal simple front end were gonna do this in react and material ui on the front end were gonna use an express server redis for our data storage because were not writing or persisting any data that is not part of the job feed and then finally a node cron job to fetch these jobs and refresh them at a certain interval which are going to be filtered through an algorithm so lets draw this all out to get a better idea of our plan heres our front end what we basically want here is a list of jobs this is going to be pulling data from our api and that will be express and then this will pull from our store which is actually going to be redis and redis will be populated by our cron called a worker which is pulling from music one or more apis its going to be eventually multiple this is going to be job uh board apis so like in lets say indeed we have github we have stack jobs and this is going to be filtered through our algorithm were filtering down the jobs getting rid of the senior jobs getting rid of the manager jobs and eventually when we make it more complicated we could even look through the description for years of experience and create uh search indexes and different things like that but again the minimum is just going to be pulling the jobs in and filtering them down okay so this is our plan lets start with the front end i know a lot of you guys like react and its gonna actually be super simple lets just do a create react app to get started uh i already have a folder our directory here called junior dev so lets uh do a create react app and then call it client you guys know that i love material ui so lets install that as well once we get into our client and lets do yarn add okay thats done and we will start our react at okay lets go through the stuff we can actually delete no tests you always want to write tests in real life but not today logo and also clear this out here now lets open the material ui docs lets get our component api and lets get a title for our page typography h1 lets just copy this and instead of putting it directly in app lets make a new component for jobs and then lets make a new component for  nd of whatever interval you want so you could do like once uh a day i think one yeah so you got it you just got to mess with it to do what you want and then you could just google exactly what you want to do and you get pulled the chronic especially from that but this is kind of a playground to get the chronic expression you you definitely want to do all right so lets test this um this basic implementation of cron to just see if it works and in our real production app this is actually going to run every second not every second every um lets say hour to fetch our jobs so this will fire off some function which is now being passed in as this a anonymous function that is just console logging but what were eventually going to put in there is our script to fetch the jobs and then filter them down so in this worker folder im going to make a github um or rather lets do it this way im gonna make a tasks folder and then well do one called fetchgithubjs and we just want to play around with that you know github api and get it to do what we want so the good part about the github api the the good thing about the github api and part of the reason why i picked it is because it is pretty much all developer jobs so we dont have to filter out for developer jobs we could just assume theyre all tech jobs and then we will take the ones that are just junior software engineer or more specifically not senior or not manager so we know our url is roughly  it into a javascript object now the slightly tricky part of this is okay how do we access redis through our node um worker right because we have all the jobs here right now were just logging them and throwing them away but we want to put them in redis and the answer to that is theres a node client library to access and connect to and uh do all the redis actions so its an interface that node uses called the client thats a library that will do everything you need to do in redis and you can do it through node with this library so im just going to search for node redis and then we will install it just like the same way as before and im gonna do exactly like i said before what the getting started docs say i know this is going to be better if we promisify this library because you can see right now that we have all the redis commands here like set is setting a key and then theres one called get which will retrieve that key just think of it like an object so if you put in key a and then the val is one then you do clientget youll retrieve that one from the a key set and get functions are async so its not going to be in line with your javascript code and thats just the name of the game because redis is not in javascript so theres no way to know when its response comes back programmatically in line its its going to wait redis could be over network so its going to take an indeterminate amount of time to serve the request and then thats g  if thats how many we put in were definitely getting a list of 245 which is a good sign now lets just try sending this back and see if we get that full uh stringified object thats exactly what we want and its that easy so thats already done so i dont know how long its been guys but with this simple of an api we can literally weve literally been able to create all this like in such a short amount of time we have a data feed and you can replicate like before before i go further you can replicate this in any this is like 90 applications if you have a data feed you have storage you have a worker thats doing some async task on an interval uh its called batch processing you have an api thats called by front end this is like 90 of crud applications lets create read upload delete in this case were only really reading from the front end but the backend is doing some more complex stuff including an algorithm the pieces that were using are used in some way in almost every large scale application so doing this as an mvp is like a really good exercise all right lets push on i might be getting a little bit tired but lets keep it going okay so we have our api uh running lets keep this running all right lets just restart this so we have the uh log that we know were listening and um im gonna go back into our client folder okay so were back in our app close that were back in our app function which we we mentioned were going to put the state in  be in jobs and not in the app file okay so since these each have their own function handle next set uh oh sorry handle next handle back im just going to copy these directly over to paste those right in function inside a function and um all right finally im going to just put this whole thing below our uh jobs map and um lets see if by pasting it directly in we we need to do anything else just going to make sure thats formatted correctly saved it and um lets just take this out classes direction rtl okay lets see how theme is being used because this is kind of a um darn all right i dont know why that would change from left to right but um it might be a mobile implementation specific thing so lets just take this and hard code it as right and well hard code this one as left and see if we run in any problems okay save that and cant find module oh i forgot about this material ui icons is a separate library so we have to also install that in our client yarn add material ui icons now while thats installing lets think about how else we want to use our pages um if we want to artificially construct these pages from a larger list then we basically just want to break things into a slice depending on the number of results we want to show at a given time so lets think of it as a moving window of 50 so if step equals 1 or lets say step equals 0 then we want to show 0 through 49 step equals 1 show 50 through 100 and so on or maybe 5099 okay so  ual private server which is hosting nginx now nginx is called a reverse proxy it connects your server to the public internet and provides like protection between those requests the other thing were going to need is a process manager on our server after we install our dependencies that we have in our local environment so were going to need the same things redis were going to need nodejs and then were just going to need to send over all our files so they can actually run on our server in the same way theyre running locally so lets do it ive pulled up a bunch of docs here were going to need its just going to be a lot of copy and pasting for server configurations and ill put all these links in the description so like i mentioned first uh we need a domain i already bought one on google domains but you can buy a domain a ton of different places i just like google because the interface and its because where my uh other domains are so ive opened my domain i already bought it a normalcom domain is going to be about 14 a year and the first thing we need to do is use the digital ocean name servers so that domain can be directed over to our server so youll see that i have clicked on my domain ive opened the dns tab and then theres a name servers tab and ive clicked use custom name servers now im going to put that on pause and jump over to the digital ocean dashboard you can make a digitalocean account for free and itll give you this dashb his so hopefully this just um doesnt give us any problems later and uh lets see if that worked okay sweet so lets do the and the same command as before npm install gpm2 oops okay and then to run our react app we all we need to npm install g another dependency called serve okay so we got those two done and lets just test pm2 with our hello js file so lets do pm2 start lljs and well see our pm2 daemon is spawned so now we can see our pm2 processes with pm2 ps and theres a status cpu and memory which is pretty cool that we get those stats in a dashboard and then if i refresh my page the app is still running in the background so thats great so if you remember the steps we outlined at the beginning the next thing we need to do is actually get our files onto this server and once we do that we should be pretty much done so im going to open up a second command window here and this is youll see in the command prompt my local machine so what we want to do is send through the files that we made in local development before we do that im going to open up this uh set of files and make one critical change that will allow us to access our api on our local server and that is to basically change this to just a relative path so im changing this to just slash jobs and actually i want to change it to be a little more clear to be slash api jobs now that means we also have to change our api route which shouldnt be a problem slash api slash jobs and  ng redis cli as we did on local and it seems to work so exit and now that we have all our files here lets just check that we do lets do npm install to get those dependencies and now lets start our worker or rather lets start our api bm2 uh start api indexjs ill do name api and then well also start our worker pm2 start indexjs name worker okay everything seems to be running but how are we sure that our work is working well lets do pm2 logs worker just to see if those logs that we set with the console logs that actually give us the indication workers working are actually going to run so youll remember that we did a cron for every minute so lets see if those logs are actually going to run after a minute so we just got to wait and see if that happens and then once that happens our redis will be loaded so in theory we should be able to refresh our page and then see all the jobs there but right now we just gotta wait the suspense is is kind of killing me here uh i feel like its been longer than a minute i dont know if its gonna work or not but i actually have faith okay there we go come on worker you can do it okay it says success okay and uh so now our reddish should be full if i refresh theres all our jobs amazing so we have a deployed application everything looks just as it did on local if i click into these jobs lets see uh it works theres our description logo and everything and then if i click apply itll take us to that position so thats pretty cool guys we just deployed our whole app and it didnt even take that long i feel like that was around 30 minutes or so all right guys thats it we basically built a whole production application in less than 100 minutes and we got it deployed and now its running on the public internet at entrylevelsoftwarejobscom so go check it out but um im thinking next theres a few critical things we have to do before were ready for like a full production launch so to speak this was just our base framework that were building more features off of but at minimum i think we need a few more data feeds and we need to make our algorithm a lot more robust prevent it from breaking on errors as well as filtering down those jobs a lot more because if we get in five feeds were just gonna have a huge amount of jobs and we really wanna make sure those are entry level jobs so im thinking that we parse through the description and look for like three to five years of experience and take those out i think anything less than three years would qualify as an entrylevel job so again were gonna have to keep working on the algorithm and then otherwise were gonna want to add a little bit of monitoring logging different things like that but that can all come later our mvp is done and our base of our application is done so thats pretty cool so anyway hope you learned something um let me know if you have any questions and i will talk to you guys soon 

#####################
Shot: mixtral8x7b
#####################
#####################
Shot: full-stack
#####################
#####################
Target: full-stack
#####################
hello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of innovations in the code especially when they use the x forers library with the block attention so i want to guide you into understanding the code because it can be really hard for beginners to understand and find thems around there are some topics that are related to mistal but will not be covered in this current video because i already covered them in my previous video about llama and in particular i will not be talking about the rms normalization the rotary positional encodin  and we will see later uh how it works but we just remember that we have eight experts and for each token we use two experts but later i will clarify how it works lets proceed further so lets talk about the sliding window attention but before we talk about the sliding window attention i need to review a little bit of the self attention mechanism so what is self attention self attention is a mechanism that allows the model to relate tokens to each other so tokens that are in the same sentence are related with each other through the self attention mechanism this is why its called self attention because each token is watching other tokens of the of the same sentence and when when and this is means basically that the query key and values are the same metrix um so imagine we have the following  only depend on itself the second token will only depend on the first two tokens the third output token will only depend on the first three tokens the fourth will depend on the token number two because the first token is not used the token number two the token number three and the token number four etc etc until the last here the last token will depend only on the last three tokens because the first three tokens are masked out and this is the importance of the mask that we apply in the self attention mechanism this concept that i show you now is very important to understand the rest of the video so please if you didnt understand it you can take a little pause you can try to do it by your by yourself because its really important that you understand how the self attention mechanism works with his will become the new input which will produce love that can then we take the last token of the output and we append it to the input for the time step four so it will become the new output will become love that can quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the last token output by the model because we already have the previous one but of course we need to feed all the previous tokens to uh to to the model which is belonging to the prompt because the model ne aling with a cy cach we need to build up this kv cache so we need to add the tokens of our prompt to the kv cache that so that we can then exploit this kv cache to build new tokens future tokens now the prompt is known in advance right because because its the input of our user its what you ask to ch gpd for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so its known in advance so we dont we dont need to generate it okay so what we can do is we can prefill the kv cache using the tokens of the prompt but there are many ways to do it like we were doing before when i was teaching you about the k cach we work with one token at a time so one way to um to add the tokens to the k cach is to add one token at at a time but this can be very time consuming be m of the wids to be only one otherwise the output range may change from model to model and usually its not a good idea to have the range of the output to change from one model to the next so to keep the range of the output stable they apply the soft marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so lets talk about it when we have a model that is too big to fit in a single gpu we can divide the model into groups of layers and place each group of layers in a single gpu for example in the case of mistal we have 32 layers of encoders you can see here one after another i didnt do  and then we buil this sequence which is a concatenation of all the prompts that we receive we take this mega sequence we run it through our llm ser llm model so it could be mistal or it could be llama this as i told you before uh and input sequence in a transformer will result in n output tokens in the output so we have uh here we have three  4 so 7 7  5 12 tokens as input it will produce 12 tokens as output to understand what is the next token for each prompt we need to check the the we need to check the embedding corresponding to the token number three for the first prompt to the token number seven for the second prompt and the last token for the third prompt so we take all these embeddings we run them through the linear layer then we apply the soft marks and then we understand what is  because my computer is not very powerful so i never run the actual model on my computer what i did to study the model was to run some random tensors through a model and i created basically a model with randomly initialized uh weights but with less number of layers so it could fit in my gpu and then i just run some random tensors to study all the shapes of the tensor and all the information passing so i dont know if the code works but i hope it will works i mean i didnt touch the logic i just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model i actually really enjoyed studying it i really enjoyed studying the code and i learned a lot of stuff you know um i think its very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you dont learn much by the end of the day anyway guys thanks you for watching my video i hope you also enjoyed this journey with me even if it was very complicated i hope that you likeed this video and you will subscribe to my channel if you didnt please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on linkedin on twitter etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you dont understand i am always available to help and connect with me on linkedin byebye 

whats up guys back today with another longer video which is to build a job board app that could actually be a real startup that people use theres a lot of different places to find entry level or junior developer jobs on the internet for example github jobs stack overflow jobs you have angel list you have indeed but filtering through those especially because you kind of have to spam applications for your first job can take a long time so the good thing about all these job boards is they have public apis for the most part you always want to start with an mvp that is the minimum viable product today that means were going to start with a single data feed that is a single job board api feeding into our app and then finally were gonna have a super minimal simple front end were gonna do this in react and material ui on the front end were gonna use an express server redis for our data storage because were not writing or persisting any data that is not part of the job feed and then finally a node cron job to fetch these jobs and refresh them at a certain interval which are going to be filtered through an algorithm so lets draw this all out to get a better idea of our plan heres our front end what we basically want here is a list of jobs this is going to be pulling data from our api and that will be express and then this will pull from our store which is actually going to be redis and redis will be populated by our cron called a worker which is pulling from music one or more apis its going to be eventually multiple this is going to be job uh board apis so like in lets say indeed we h ew terminal tab going out to our main directory and within our main directory im just going to create a package json which is going to be accessible from all our different folders so you can see that was created here and then we just want to install cron as a dependency okay now inside a worker i just this is the way i pretty much start with every uh with every app you just copy kind of the basic usage and you build your way out from there so all this does is its running a node process where were importing the library and then this is called a um cron shorthand cron syntax something like that uh cronguru you can see it here okay so this site is really useful no not this site crown guru crontab guru okay so these asterisks are called a cron schedule expression and its basically symbols for  ur redis store and then you can serve it up in our case jumping back over to these um these job uh json that we got from github you can see every job has a uuid which is a guaranteed unique id because it has so many characters and its random characters so itll be guaranteed unique every time every job has one of these ids so thats a natural candidate to use as a redis key im thinking within redis our key is going to be this id and our value is just going to be the whole job object so it would be a string version of this that we can then pull out we know its already json so we can just stringify it and then when we pull it out well just parse it into a javascript object now the slightly tricky part of this is okay how do we access redis through our node um worker right because we have all t  length matches our our length that we put in lets parse it and then lets do dot lake okay so its kind of a lot that we did without testing it lets restart our server do the same curl oh man okay 245 i dont remember if thats how many we put in were definitely getting a list of 245 which is a good sign now lets just try sending this back and see if we get that full uh stringified object thats exactly what we want and its that easy so thats already done so i dont know how long its been guys but with this simple of an api we can literally weve literally been able to create all this like in such a short amount of time we have a data feed and you can replicate like before before i go further you can replicate this in any this is like 90 applications if you have a data feed you have storage you   function and um all right finally im going to just put this whole thing below our uh jobs map and um lets see if by pasting it directly in we we need to do anything else just going to make sure thats formatted correctly saved it and um lets just take this out classes direction rtl okay lets see how theme is being used because this is kind of a um darn all right i dont know why that would change from left to right but um it might be a mobile implementation specific thing so lets just take this and hard code it as right and well hard code this one as left and see if we run in any problems okay save that and cant find module oh i forgot about this material ui icons is a separate library so we have to also install that in our client yarn add material ui icons now while thats installing lets t configurations and ill put all these links in the description so like i mentioned first uh we need a domain i already bought one on google domains but you can buy a domain a ton of different places i just like google because the interface and its because where my uh other domains are so ive opened my domain i already bought it a normalcom domain is going to be about 14 a year and the first thing we need to do is use the digital ocean name servers so that domain can be directed over to our server so youll see that i have clicked on my domain ive opened the dns tab and then theres a name servers tab and ive clicked use custom name servers now im going to put that on pause and jump over to the digital ocean dashboard you can make a digitalocean account for free and itll give you this dashboar  one critical change that will allow us to access our api on our local server and that is to basically change this to just a relative path so im changing this to just slash jobs and actually i want to change it to be a little more clear to be slash api jobs now that means we also have to change our api route which shouldnt be a problem slash api slash jobs and we have to make one change in our package json and thats adding a proxy config which allows this to still work on local development this doesnt affect anything on production so um proxying all api routes to a target of localhost sorry need the double quotes there http localhost uh 3001 was our api server i believe and ill save that and again that just helps us you know still develop in the same way but we need to make one config chan ocal if i click into these jobs lets see uh it works theres our description logo and everything and then if i click apply itll take us to that position so thats pretty cool guys we just deployed our whole app and it didnt even take that long i feel like that was around 30 minutes or so all right guys thats it we basically built a whole production application in less than 100 minutes and we got it deployed and now its running on the public internet at entrylevelsoftwarejobscom so go check it out but um im thinking next theres a few critical things we have to do before were ready for like a full production launch so to speak this was just our base framework that were building more features off of but at minimum i think we need a few more data feeds and we need to make our algorithm a lot more robust prevent it from breaking on errors as well as filtering down those jobs a lot more because if we get in five feeds were just gonna have a huge amount of jobs and we really wanna make sure those are entry level jobs so im thinking that we parse through the description and look for like three to five years of experience and take those out i think anything less than three years would qualify as an entrylevel job so again were gonna have to keep working on the algorithm and then otherwise were gonna want to add a little bit of monitoring logging different things like that but that can all come later our mvp is done and our base of our application is done so thats pretty cool so anyway hope you learned something um let me know if you have any questions and i will talk to you guys soon 

in a previous episode of the vacuum engineering show i talked about hash tables and how uh powerful and very commonly used they are given a key we can find its corresponding value in no time in zero seek time were not searching were not scanning were not doing anything its a single axis in memory retrieving that value the power behind hash tables is the use of the arrays which is a very common data structure obviously very very common data structure right knowing the array position the the index you can get the value of the array immediately because you know how how does memory work if you know the address of the ram where your cell exists where your values exist you can get the value immediately right the cpu can fetch the value immediately for you right so once you find the index you find the address you can value so what the hash table guys did it says okay were gonna take your name your string your color your car your va anything that has been this key were going to hash it using a oneway function and then convert that into an index using a modulo function based on the array size so we eventually from the name we convert and continue an index and that will give us the address and the memory and we can get the value so thats the trick the hash table guys use you know so the only cost you add is like what the hashing function which is not really that bad but thats the power of the hash table but the problem is the moment the size of the array changes the hash table size changes this all forms apart because what what the if blue the key blue used to fit in index number 11 if you increased or decreased the array size and i you know where im going with this then the blue will fit into index 99 now and as a result right you either wont find the value or youre gonna start writing the values duplicate and everything will basically be bad so you now have to really move things around your resize and its a really big problem so why im mentioning that the problem that we are seeing today with distributive system is as follows if my database are increasing in size and it no longer can fit in a single instance i i bumped up the vertical scaling to its maximum its now 48 core cpu three gigahertz whatever you know its 512 gigabyte or even one terabyte ram but still i have billions of rows i cannot i added all the indexes i try to partition it horizontally in the same instance by range so but still the even the partitions are so large so i that machine cannot handle the queries um throwing at my beautiful database so what do we do we distribute it we shard the database thats i really dont like to go there unless i kind of exhaust all my options and believe me people dont even look at the options anymore people are very quick to follow modern things you know without actually going to the basics and trying to tune your database and getting a better performance bu ept of hashing hashing tables appear here so in this episode of the vacancy show id like to talk about that a little bit distributed hashing and then what problems did we have and how consistent hashing solves this problem and obviously nothing is perfect in this world so id also like like to talk about the problems consisting hashing actually have today welcome to the backend engineering show with your host hussein nelson and the concept of distributed system is a must when you get to a certain scale yeah i always try as much as possible to you know push people against being distributed if they can do things to have their single instance you know be more performant when it comes to query because you see a lot of a lot of people our engineers are hurry up to scale right and spend more money to start to work with the distributor without actually while their query is actually using 500 of their cpu and then doing like a million logical reads you know where they can tweak it a little bit and tweak it and tune the database a little bit understand their queries so they can have or even you know lower that cost you know as a result but we dont think this way anymore we always take the shortcut unfortunately but regardless so usually advanced and adept dpas try to optimize a single coin as a result if you if you can get a query to consume less cpu even if its a small query scaling that query right will eventually give you better scal roduced a friction that didnt exist before first previously so i just one server we know the server right but now we have to figure out from the key we have to figure out the iprs of the server to connect to in order where our key actually exists that answer can be answered using a very simple hashing function so were going to hash the value lets say im have value number four uh gonna hash it get some value and then that value were gonna do modulo number four and that gives you server zero so store server zero right here right in this case right so four modulo four is zero and then okay how about key number five right or even whatever that key used to be maybe lets say its a its a blue or red right red youre gonna hash it get that value number five which is a number and then five modulo four will give you s1 and s1 will go right here right so five right lives in s1 and same thing 6 will live in s2 and 7 will live in s3 youre just gonna do module four because four is your server pool size right and then lets say going back lets say eight then eight module four oh so back to zero and then use get the point which is which works perfectly from the key i was able to figure out the server so cost is zero is nothing heres we solve distributed system right there heres one problem though as long as you have four servers your love is beautiful you dont have to worry about anything but now even those four servers reach their limit again  actually gives you value four so its now as an s4 so now what you need to do is like move the key with a value four from the server zero to server four and now you have to move everything else and this is this is basically the same thing five modulo five right the server five the the key four will move to server four the key eight will move to server three the key 5 will move to server 0 the key 6 will move to server 1 and the key 7 will move to the key 7 will move to s1 right so everything will be shuffled and the operation of adding a new server will cost us a huge amount of effort to kind of shuffle things around so look think about networking think about database usage just to add another server to not only bothering one server youre bothering the entire cluster with your operation because you have to shuffle things around right because your key no longer maps so what what people think what the smart people say okay lets invent something that might solve this problem which is called consistent hashing so how does it really work so they go they went back to the idea of having so they went back to idea the core idea we use the index as the server names here right lets flip this lets change this a little bit lets not be very discreet like oh server 0 1 2 3 4 lets build a ring lets actually build a ring of these servers all right so instead they think of this idea as an actual rank as an actual circle so values go back right a m picking im picking values that are so rounded up this is just for example youre never going to get youre hardly going to get s 90 right exactly right youre going to probably get x is 27 right but yeah x90 same thing server 3 or server 4 youre going to get s 180 if after you do that and then server 3 gonna get s2270 we have s0 s90 s180 and s270 you know thats that completes an actual range and for people listening in the podcast think of this as an actual ring with the values for servers and each corner effectively so why are we doing this the beauty here is we have values and this is the key here lets have a key value of a thousand same thing youre gonna do right the thousand the key value thousand right however this key exists maybe it was blue and then you did a hash and then got you got the number thousand you take that thousand right and you do a modular 360 will you get an actual server immediately the server index no youre not going to get that so now lets lets actually put it in an actual example right here right so now im about to insert the value im going to insert a new key value 1 500 thats thats my key right there so what youre going to do is youre going to do modulo 360 which is the circle again dont dont be very uh specific on the the value 360 you can double this and double this and double this to get more values but im again this is just an example so 360 here so if i take 1500 right and modulo 360 im going t  60 now okay oh 60 is like whats right after 60 is 90 now okay so the value of 1500 fits on server s90 thats how we solve it so by doing this scan what is the next one we really removed that discrete value lookup you know we instead changed it with a scan and the beauty of this scan is going to save us a lot of things in the future lets continue these examples all right 1500 lets take another example as a 2000 2000 modular 360 is 200 200 fits between server s180 and server s270 again 180 and 270 think of the as degrees right so right there in this case okay whats exactly right after 200 its 270 so 2000 less than 270 okay so think of it like youre looking at clockwise so whatever is the next value youre going to get that and then you put another value say 3000 modular 360 thats 120 120 if its between server 90 and server 180 select moves directly to the 180 and then you store the value 3000 there how about a thousand thousand modular 360 is actually gives you 280 wait a minute 280 is the there are no server after the last server the largest value is actually s270 there is no larger after that right there is no larger than two server 217 this is values 280 so what do you do if theres no larger values you go back to the circle right and this is very nicely when you actually draw the circle you see that s 280 is between s270 and s0 so you immediately go and put the value 1000 in server 0 and thats how you build the ring lets take  eeds to be fit in server 50 right because thats the algorithm thats the range here that were looking at right here and so if you the the algorithms look like this like okay so what is the server right after me well the server right after me is right here is 90 right so lets query that server find out all the hash values and out all the hash values where the actual hash value is less than 40 right because s90 would have stored everything between 0 and 90 right so find out everything that is less than 50 because i i should store these now right so we found one entry which is 40 oh 40 doesnt belong 90 anymore so go ahead and talk to server 90 establish a communication could be tcp could be anything and then start moving data around right hey its 90 and do you have anything with a hash value that is less than 40 is this less than 50 which is me oh yeah i have this value move it to me and as a result we just established the data here so now think of it as as i add another server the only change is my neighbor really im only gonna bother my neighbor and its only the neighbor right after me right thats thats what im  gonna do in this case right and so instead of actually bothering all the servers in my cluster im only bothering one server so thats much much much better than actual hashing but still there is a cost to it right there is complexity you need to build all that out what if the operation failed how do you roll back what if do you add the server anyway what if one of the keys didnt transmit so were adding so much complexity so distributed systems are not easy you guys right even if you think about it lets take another example where were removing a server if we go back to our original case where we have s0 s90 s180 and s27 and lets say im going to remove s90 altogether so now what happens here if you remove s90 then anything that s90 had must go to s180 if you think about it right because any value so now what what happens is the operation should go okay remove server s90 that operation that we need to do right im gonna remove that physically remove it not talking about a crash thats a completely different story right crash you dont even have time to move anything right so if you thats why you have to have redundancy thats another complication right there but if you have a physical operation that say okay i want to remove this for maintenance remove s90 well its 90 holes and all values between 0 and 90 so all of those should go directly to the server right after me clockwise which is 180 so remove all those puppies and stick them to where to server 180 thats a very expensive operation as well so if you think about it consistent hashing are powerful is powerful the algorithm of consistency are very powerful but the limitation here becomes you still need to move data around and the more data you have you know in these instances then the transmit of these will take more time so adding or removing server is actually not a trivial operation you know and then you really think need to think about another thing that is called replication you need to duplicate this data as much as possible because a server might crash so you need to have a backup so just using this algorithm blindly is not enough you have to cater for crashes you know and as a result you have to have a backup oh if this server is not available lets lets have a mapping server that actually directly copies that data immediately right so thats what you do you have to do that well what if two servers map to the same hash you cant have it if you have a lot of more than 360 servers youre gonna youre bound to have the value that fits in the same server and im not sure what you can do here you can what you can do is like i suppose in this case you can treat it as a as a replication so you have like a a backup scenario where we have this server you can either put it in both right that was an episode of the consistent hashing hope you enjoyed this video im gonna see on the next one guys thousand goodbye 

#####################
Shot: mixtral8x7b
#####################
#####################
Shot: full-stack
#####################
#####################
Target: hashing
#####################
hello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of innovations in the code especially when they use the x forers library with the block attention so i want to guide you into understanding the code because it can be really hard for beginners to understand and find thems around there are some topics that are related to mistal but will not be covered in this current video because i already covered them in my previous video about llama and in particular i will not be talking about the rms normalization the rotary positional encodin  and we will see later uh how it works but we just remember that we have eight experts and for each token we use two experts but later i will clarify how it works lets proceed further so lets talk about the sliding window attention but before we talk about the sliding window attention i need to review a little bit of the self attention mechanism so what is self attention self attention is a mechanism that allows the model to relate tokens to each other so tokens that are in the same sentence are related with each other through the self attention mechanism this is why its called self attention because each token is watching other tokens of the of the same sentence and when when and this is means basically that the query key and values are the same metrix um so imagine we have the following  only depend on itself the second token will only depend on the first two tokens the third output token will only depend on the first three tokens the fourth will depend on the token number two because the first token is not used the token number two the token number three and the token number four etc etc until the last here the last token will depend only on the last three tokens because the first three tokens are masked out and this is the importance of the mask that we apply in the self attention mechanism this concept that i show you now is very important to understand the rest of the video so please if you didnt understand it you can take a little pause you can try to do it by your by yourself because its really important that you understand how the self attention mechanism works with his will become the new input which will produce love that can then we take the last token of the output and we append it to the input for the time step four so it will become the new output will become love that can quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the last token output by the model because we already have the previous one but of course we need to feed all the previous tokens to uh to to the model which is belonging to the prompt because the model ne aling with a cy cach we need to build up this kv cache so we need to add the tokens of our prompt to the kv cache that so that we can then exploit this kv cache to build new tokens future tokens now the prompt is known in advance right because because its the input of our user its what you ask to ch gpd for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so its known in advance so we dont we dont need to generate it okay so what we can do is we can prefill the kv cache using the tokens of the prompt but there are many ways to do it like we were doing before when i was teaching you about the k cach we work with one token at a time so one way to um to add the tokens to the k cach is to add one token at at a time but this can be very time consuming be m of the wids to be only one otherwise the output range may change from model to model and usually its not a good idea to have the range of the output to change from one model to the next so to keep the range of the output stable they apply the soft marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so lets talk about it when we have a model that is too big to fit in a single gpu we can divide the model into groups of layers and place each group of layers in a single gpu for example in the case of mistal we have 32 layers of encoders you can see here one after another i didnt do  and then we buil this sequence which is a concatenation of all the prompts that we receive we take this mega sequence we run it through our llm ser llm model so it could be mistal or it could be llama this as i told you before uh and input sequence in a transformer will result in n output tokens in the output so we have uh here we have three  4 so 7 7  5 12 tokens as input it will produce 12 tokens as output to understand what is the next token for each prompt we need to check the the we need to check the embedding corresponding to the token number three for the first prompt to the token number seven for the second prompt and the last token for the third prompt so we take all these embeddings we run them through the linear layer then we apply the soft marks and then we understand what is  because my computer is not very powerful so i never run the actual model on my computer what i did to study the model was to run some random tensors through a model and i created basically a model with randomly initialized uh weights but with less number of layers so it could fit in my gpu and then i just run some random tensors to study all the shapes of the tensor and all the information passing so i dont know if the code works but i hope it will works i mean i didnt touch the logic i just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model i actually really enjoyed studying it i really enjoyed studying the code and i learned a lot of stuff you know um i think its very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you dont learn much by the end of the day anyway guys thanks you for watching my video i hope you also enjoyed this journey with me even if it was very complicated i hope that you likeed this video and you will subscribe to my channel if you didnt please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on linkedin on twitter etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you dont understand i am always available to help and connect with me on linkedin byebye 

whats up guys back today with another longer video which is to build a job board app that could actually be a real startup that people use theres a lot of different places to find entry level or junior developer jobs on the internet for example github jobs stack overflow jobs you have angel list you have indeed but filtering through those especially because you kind of have to spam applications for your first job can take a long time so the good thing about all these job boards is they have public apis for the most part you always want to start with an mvp that is the minimum viable product today that means were going to start with a single data feed that is a single job board api feeding into our app and then finally were gonna have a super minimal simple front end were gonna do this in react and material ui on the front end were gonna use an express server redis for our data storage because were not writing or persisting any data that is not part of the job feed and then finally a node cron job to fetch these jobs and refresh them at a certain interval which are going to be filtered through an algorithm so lets draw this all out to get a better idea of our plan heres our front end what we basically want here is a list of jobs this is going to be pulling data from our api and that will be express and then this will pull from our store which is actually going to be redis and redis will be populated by our cron called a worker which is pulling from music one or more apis its going to be eventually multiple this is going to be job uh board apis so like in lets say indeed we h ew terminal tab going out to our main directory and within our main directory im just going to create a package json which is going to be accessible from all our different folders so you can see that was created here and then we just want to install cron as a dependency okay now inside a worker i just this is the way i pretty much start with every uh with every app you just copy kind of the basic usage and you build your way out from there so all this does is its running a node process where were importing the library and then this is called a um cron shorthand cron syntax something like that uh cronguru you can see it here okay so this site is really useful no not this site crown guru crontab guru okay so these asterisks are called a cron schedule expression and its basically symbols for  ur redis store and then you can serve it up in our case jumping back over to these um these job uh json that we got from github you can see every job has a uuid which is a guaranteed unique id because it has so many characters and its random characters so itll be guaranteed unique every time every job has one of these ids so thats a natural candidate to use as a redis key im thinking within redis our key is going to be this id and our value is just going to be the whole job object so it would be a string version of this that we can then pull out we know its already json so we can just stringify it and then when we pull it out well just parse it into a javascript object now the slightly tricky part of this is okay how do we access redis through our node um worker right because we have all t  length matches our our length that we put in lets parse it and then lets do dot lake okay so its kind of a lot that we did without testing it lets restart our server do the same curl oh man okay 245 i dont remember if thats how many we put in were definitely getting a list of 245 which is a good sign now lets just try sending this back and see if we get that full uh stringified object thats exactly what we want and its that easy so thats already done so i dont know how long its been guys but with this simple of an api we can literally weve literally been able to create all this like in such a short amount of time we have a data feed and you can replicate like before before i go further you can replicate this in any this is like 90 applications if you have a data feed you have storage you   function and um all right finally im going to just put this whole thing below our uh jobs map and um lets see if by pasting it directly in we we need to do anything else just going to make sure thats formatted correctly saved it and um lets just take this out classes direction rtl okay lets see how theme is being used because this is kind of a um darn all right i dont know why that would change from left to right but um it might be a mobile implementation specific thing so lets just take this and hard code it as right and well hard code this one as left and see if we run in any problems okay save that and cant find module oh i forgot about this material ui icons is a separate library so we have to also install that in our client yarn add material ui icons now while thats installing lets t configurations and ill put all these links in the description so like i mentioned first uh we need a domain i already bought one on google domains but you can buy a domain a ton of different places i just like google because the interface and its because where my uh other domains are so ive opened my domain i already bought it a normalcom domain is going to be about 14 a year and the first thing we need to do is use the digital ocean name servers so that domain can be directed over to our server so youll see that i have clicked on my domain ive opened the dns tab and then theres a name servers tab and ive clicked use custom name servers now im going to put that on pause and jump over to the digital ocean dashboard you can make a digitalocean account for free and itll give you this dashboar  one critical change that will allow us to access our api on our local server and that is to basically change this to just a relative path so im changing this to just slash jobs and actually i want to change it to be a little more clear to be slash api jobs now that means we also have to change our api route which shouldnt be a problem slash api slash jobs and we have to make one change in our package json and thats adding a proxy config which allows this to still work on local development this doesnt affect anything on production so um proxying all api routes to a target of localhost sorry need the double quotes there http localhost uh 3001 was our api server i believe and ill save that and again that just helps us you know still develop in the same way but we need to make one config chan ocal if i click into these jobs lets see uh it works theres our description logo and everything and then if i click apply itll take us to that position so thats pretty cool guys we just deployed our whole app and it didnt even take that long i feel like that was around 30 minutes or so all right guys thats it we basically built a whole production application in less than 100 minutes and we got it deployed and now its running on the public internet at entrylevelsoftwarejobscom so go check it out but um im thinking next theres a few critical things we have to do before were ready for like a full production launch so to speak this was just our base framework that were building more features off of but at minimum i think we need a few more data feeds and we need to make our algorithm a lot more robust prevent it from breaking on errors as well as filtering down those jobs a lot more because if we get in five feeds were just gonna have a huge amount of jobs and we really wanna make sure those are entry level jobs so im thinking that we parse through the description and look for like three to five years of experience and take those out i think anything less than three years would qualify as an entrylevel job so again were gonna have to keep working on the algorithm and then otherwise were gonna want to add a little bit of monitoring logging different things like that but that can all come later our mvp is done and our base of our application is done so thats pretty cool so anyway hope you learned something um let me know if you have any questions and i will talk to you guys soon 

#####################
Shot: mixtral8x7b
#####################
#####################
Shot: full-stack
#####################
#####################
Target: localized_deployment
#####################
hello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of innovations in the code especially when they use the x forers library with the block attention so i want to guide you into understanding the code because it can be really hard for beginners to understand and find thems around there are some topics that are related to mistal but will not be covered in this current video because i already covered them in my previous video about llama and in particular i will not be talking about the rms normalization the rotary positional encodin  and we will see later uh how it works but we just remember that we have eight experts and for each token we use two experts but later i will clarify how it works lets proceed further so lets talk about the sliding window attention but before we talk about the sliding window attention i need to review a little bit of the self attention mechanism so what is self attention self attention is a mechanism that allows the model to relate tokens to each other so tokens that are in the same sentence are related with each other through the self attention mechanism this is why its called self attention because each token is watching other tokens of the of the same sentence and when when and this is means basically that the query key and values are the same metrix um so imagine we have the following  only depend on itself the second token will only depend on the first two tokens the third output token will only depend on the first three tokens the fourth will depend on the token number two because the first token is not used the token number two the token number three and the token number four etc etc until the last here the last token will depend only on the last three tokens because the first three tokens are masked out and this is the importance of the mask that we apply in the self attention mechanism this concept that i show you now is very important to understand the rest of the video so please if you didnt understand it you can take a little pause you can try to do it by your by yourself because its really important that you understand how the self attention mechanism works with his will become the new input which will produce love that can then we take the last token of the output and we append it to the input for the time step four so it will become the new output will become love that can quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the last token output by the model because we already have the previous one but of course we need to feed all the previous tokens to uh to to the model which is belonging to the prompt because the model ne aling with a cy cach we need to build up this kv cache so we need to add the tokens of our prompt to the kv cache that so that we can then exploit this kv cache to build new tokens future tokens now the prompt is known in advance right because because its the input of our user its what you ask to ch gpd for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so its known in advance so we dont we dont need to generate it okay so what we can do is we can prefill the kv cache using the tokens of the prompt but there are many ways to do it like we were doing before when i was teaching you about the k cach we work with one token at a time so one way to um to add the tokens to the k cach is to add one token at at a time but this can be very time consuming be m of the wids to be only one otherwise the output range may change from model to model and usually its not a good idea to have the range of the output to change from one model to the next so to keep the range of the output stable they apply the soft marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so lets talk about it when we have a model that is too big to fit in a single gpu we can divide the model into groups of layers and place each group of layers in a single gpu for example in the case of mistal we have 32 layers of encoders you can see here one after another i didnt do  and then we buil this sequence which is a concatenation of all the prompts that we receive we take this mega sequence we run it through our llm ser llm model so it could be mistal or it could be llama this as i told you before uh and input sequence in a transformer will result in n output tokens in the output so we have uh here we have three  4 so 7 7  5 12 tokens as input it will produce 12 tokens as output to understand what is the next token for each prompt we need to check the the we need to check the embedding corresponding to the token number three for the first prompt to the token number seven for the second prompt and the last token for the third prompt so we take all these embeddings we run them through the linear layer then we apply the soft marks and then we understand what is  because my computer is not very powerful so i never run the actual model on my computer what i did to study the model was to run some random tensors through a model and i created basically a model with randomly initialized uh weights but with less number of layers so it could fit in my gpu and then i just run some random tensors to study all the shapes of the tensor and all the information passing so i dont know if the code works but i hope it will works i mean i didnt touch the logic i just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model i actually really enjoyed studying it i really enjoyed studying the code and i learned a lot of stuff you know um i think its very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you dont learn much by the end of the day anyway guys thanks you for watching my video i hope you also enjoyed this journey with me even if it was very complicated i hope that you likeed this video and you will subscribe to my channel if you didnt please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on linkedin on twitter etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you dont understand i am always available to help and connect with me on linkedin byebye 

whats up guys back today with another longer video which is to build a job board app that could actually be a real startup that people use theres a lot of different places to find entry level or junior developer jobs on the internet for example github jobs stack overflow jobs you have angel list you have indeed but filtering through those especially because you kind of have to spam applications for your first job can take a long time so the good thing about all these job boards is they have public apis for the most part you always want to start with an mvp that is the minimum viable product today that means were going to start with a single data feed that is a single job board api feeding into our app and then finally were gonna have a super minimal simple front end were gonna do this in react and material ui on the front end were gonna use an express server redis for our data storage because were not writing or persisting any data that is not part of the job feed and then finally a node cron job to fetch these jobs and refresh them at a certain interval which are going to be filtered through an algorithm so lets draw this all out to get a better idea of our plan heres our front end what we basically want here is a list of jobs this is going to be pulling data from our api and that will be express and then this will pull from our store which is actually going to be redis and redis will be populated by our cron called a worker which is pulling from music one or more apis its going to be eventually multiple this is going to be job uh board apis so like in lets say indeed we h ew terminal tab going out to our main directory and within our main directory im just going to create a package json which is going to be accessible from all our different folders so you can see that was created here and then we just want to install cron as a dependency okay now inside a worker i just this is the way i pretty much start with every uh with every app you just copy kind of the basic usage and you build your way out from there so all this does is its running a node process where were importing the library and then this is called a um cron shorthand cron syntax something like that uh cronguru you can see it here okay so this site is really useful no not this site crown guru crontab guru okay so these asterisks are called a cron schedule expression and its basically symbols for  ur redis store and then you can serve it up in our case jumping back over to these um these job uh json that we got from github you can see every job has a uuid which is a guaranteed unique id because it has so many characters and its random characters so itll be guaranteed unique every time every job has one of these ids so thats a natural candidate to use as a redis key im thinking within redis our key is going to be this id and our value is just going to be the whole job object so it would be a string version of this that we can then pull out we know its already json so we can just stringify it and then when we pull it out well just parse it into a javascript object now the slightly tricky part of this is okay how do we access redis through our node um worker right because we have all t  length matches our our length that we put in lets parse it and then lets do dot lake okay so its kind of a lot that we did without testing it lets restart our server do the same curl oh man okay 245 i dont remember if thats how many we put in were definitely getting a list of 245 which is a good sign now lets just try sending this back and see if we get that full uh stringified object thats exactly what we want and its that easy so thats already done so i dont know how long its been guys but with this simple of an api we can literally weve literally been able to create all this like in such a short amount of time we have a data feed and you can replicate like before before i go further you can replicate this in any this is like 90 applications if you have a data feed you have storage you   function and um all right finally im going to just put this whole thing below our uh jobs map and um lets see if by pasting it directly in we we need to do anything else just going to make sure thats formatted correctly saved it and um lets just take this out classes direction rtl okay lets see how theme is being used because this is kind of a um darn all right i dont know why that would change from left to right but um it might be a mobile implementation specific thing so lets just take this and hard code it as right and well hard code this one as left and see if we run in any problems okay save that and cant find module oh i forgot about this material ui icons is a separate library so we have to also install that in our client yarn add material ui icons now while thats installing lets t configurations and ill put all these links in the description so like i mentioned first uh we need a domain i already bought one on google domains but you can buy a domain a ton of different places i just like google because the interface and its because where my uh other domains are so ive opened my domain i already bought it a normalcom domain is going to be about 14 a year and the first thing we need to do is use the digital ocean name servers so that domain can be directed over to our server so youll see that i have clicked on my domain ive opened the dns tab and then theres a name servers tab and ive clicked use custom name servers now im going to put that on pause and jump over to the digital ocean dashboard you can make a digitalocean account for free and itll give you this dashboar  one critical change that will allow us to access our api on our local server and that is to basically change this to just a relative path so im changing this to just slash jobs and actually i want to change it to be a little more clear to be slash api jobs now that means we also have to change our api route which shouldnt be a problem slash api slash jobs and we have to make one change in our package json and thats adding a proxy config which allows this to still work on local development this doesnt affect anything on production so um proxying all api routes to a target of localhost sorry need the double quotes there http localhost uh 3001 was our api server i believe and ill save that and again that just helps us you know still develop in the same way but we need to make one config chan ocal if i click into these jobs lets see uh it works theres our description logo and everything and then if i click apply itll take us to that position so thats pretty cool guys we just deployed our whole app and it didnt even take that long i feel like that was around 30 minutes or so all right guys thats it we basically built a whole production application in less than 100 minutes and we got it deployed and now its running on the public internet at entrylevelsoftwarejobscom so go check it out but um im thinking next theres a few critical things we have to do before were ready for like a full production launch so to speak this was just our base framework that were building more features off of but at minimum i think we need a few more data feeds and we need to make our algorithm a lot more robust prevent it from breaking on errors as well as filtering down those jobs a lot more because if we get in five feeds were just gonna have a huge amount of jobs and we really wanna make sure those are entry level jobs so im thinking that we parse through the description and look for like three to five years of experience and take those out i think anything less than three years would qualify as an entrylevel job so again were gonna have to keep working on the algorithm and then otherwise were gonna want to add a little bit of monitoring logging different things like that but that can all come later our mvp is done and our base of our application is done so thats pretty cool so anyway hope you learned something um let me know if you have any questions and i will talk to you guys soon 

hello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of innovations in the code especially when they use the x forers library with the block attention so i want to guide you into understanding the code because it can be really hard for beginners to understand and find thems around there are some topics that are related to mistal but will not be covered in this current video because i already covered them in my previous video about llama and in particular i will not be talking about the rms normalization the rotary positional encoding and the group query attention because i already um teach them in depth in my previous video on llama so if you want to know about them please watch my previous video on llama another the only prerequisite that i hope you have before watching this video because the topics we are going to touch are quite advanced is that you are familiar with the transformer model so if youre not familiar with the transformer model and the attention mechanism in particular and in particular the self attention mechanism please go watch my video on the transformer in which i teach all this concept very thoroughly very in detail these are really a prerequisite for watching this video because the top the topics here are quite advanced okay lets proceed further so lets watch the differences between the vanilla transformer and mistal at the architecture level as you can see from the um image here which i built by myself using the code because they didnt release any architecture picture in the paper u the architecture of mistal first of all lets talk about some terminology when you have a model like this made up of many encoder layers plus linear and the soft max we are talking about a decoder only model because this part this model here looks like the decoder of the vanilla transformer you can see here ry key and values are the same matrix in the case of self attention this will produce a matrix that is 6x 6 because the inner two dimensions kind of cancel out and the outer dimensions indicate the dimension of the output matrix here now what is the values what are the values in this matrix representing the first value here indicates the dot product of the first token with the first uh the first row of the query with the first column of the keys so basically the dot product of the embedding of the first token with itself the second value here indicates the dot product of the first row of the query matrix with the second column of the key matrix here the transpose of the keys matrix here which basically means that its the dot product of the embedding of the first token so the with the embedding of the second token which is cat and etc etc for all the other values dont concentrate too much on the values because all the values i put here are random and also the fact that these numbers are less than one its not necessary because the dot product can be bigger than one its not a uh condition of the dot product usually in the formula we also normalize here we divide by the dimension of the dc dk basically is the um the size the part of the embedding to which this particular attention head will attend to but lets pretend that we only have one one ahead so dk is equal to d model so basically this head will watch the full embedding of e ke a little pause you can try to do it by your by yourself because its really important that you understand how the self attention mechanism works with the mask okay now that we have seen this concept i want to introduce you to the next one so as we saw before the output of the self attention mechanism is another matrix with the same shape as the query matrix in which each token is represented by an embedding of size 496 but each embedding now captures information also about other tokens according to the mask and if we check the um this mask here so the output here we can safely say that the input of our slide uh sliding window attention was the initial uh sequence d cat is on on a chair but after applying the self attention the first token is now related to itself the second token is related to itself and the token before it the third is related to the token before it and the one also before it the last one only depends on the previous two tokens etc according to the mask right now what happens if we feed this one because as you know in the transformer word and also in mistal and also in lama we have many layers of encoders one after another which are also called transformer block in the code and the output of each layer is fed to the next one so this is the first layer of the uh transformer so we take the input sequence and we feed it to the first layer which will produce a list of tokens where each token now captures inform n quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the last token output by the model because we already have the previous one but of course we need to feed all the previous tokens to uh to to the model which is belonging to the prompt because the model needs to access the prompt to understand which token to produce next so for example we cannot produce the word gen only by giving the word the we need to give all this sentence to produce this output gentle here but at the same time we are only interested in the last word gentle and this is the reason we introduce the k cach because the k cach allow us to reduce the computations that we are doing by only producing one output at a time the one that we need but um without doing all the intermediate computations for all the other tokens that we never use so basically basically when we want the word heart we dont want to produce the output for the word love that can quickly seize the gentle because we already have them in the prompt we dont need to produce all these tokens we just want to produce the output for the token heart so we want to reduce the computa ets talk about another concept that is very important which is chunking and prefeeding basically when we generate text using a language model we use a prompt and then we use this prompt to generate future tokens when dealing with a cy cach we need to build up this kv cache so we need to add the tokens of our prompt to the kv cache that so that we can then exploit this kv cache to build new tokens future tokens now the prompt is known in advance right because because its the input of our user its what you ask to ch gpd for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so its known in advance so we dont we dont need to generate it okay so what we can do is we can prefill the kv cache using the tokens of the prompt but there are many ways to do it like we were doing before when i was teaching you about the k cach we work with one token at a time so one way to um to add the tokens to the k cach is to add one token at at a time but this can be very time consuming because imagine you have a very large prompt which happens with retrieval augmented generation which we have very big prompts like 5even bigger so this if we add one token at a time it will mean that we have to take 5network which is can be very time consuming and also doesnt exploit our gpu very much the other way is to take all these tokens and feed them all at once to the model but that may be limited by the size of our gpu be rx of the two best performing instead of taking the soft marx of everyone well the first problem is that if we take the soft max of all of the logits then the two best performing may not sum up to one which is um which is a condition that we need in case we want to train multiple models and compare them because im pretty sure that the guys at mistal did not only train one model maybe they trained multiple models with multiple hyper parameter maybe they tried with four mixture of four experts but also with three experts or two experts then they choose the best one so if you want to compare models you want the weighted sum to always perform the sum of the wids to be only one otherwise the output range may change from model to model and usually its not a good idea to have the range of the output to change from one model to the next so to keep the range of the output stable they apply the soft marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so lets talk about it when we have a model that is too big to fit in a single gpu we can divide the model into groups of layers and place each group of layers in a single gpu for example in the case of mistal we have 32 layers of encoders you can see here one after another i didnt so we will be using the output at the third token at the for the second prompt the output at the fourth token and only in the last token we will be checking the last output of the output of the self attention but for the first two prompts we will not be even checking the last token output from the self attention because they correspond to the padding token so is there avoid a way to avoid these padding tokens uh being introduced in our calculation and calculating all these dot products which will result in output tokens that we will not even use well there is a better solution and the solution is this the solution is to combine all the tokens of of all the prompts into one big sequence consecutively and we also keep track of what is the actual size of each prompt so we know that the prompt are coming from our api because we are running an ai company and we have this api so we know that the first customer has a token size prompt of size three tokens the second one has four tokens and the third one has five tokens so we can keep track of these sizes in an array for example and then we buil this sequence which is a concatenation of all the prompts that we receive we take this mega sequence we run it through our llm ser llm model so it could be mistal or it could be llama this as i told you before uh and input sequence in a transformer will result in n output tokens in the output so we have uh here we have three  4 so 7 7  5 12 to rst prompt is very short only three tokens are actually part in are in the k cach um but when we pass the k cach to the calculation of the attention we pass all the tensor which is all the 10 items so we need a way to tell to the mask that he should only use the first three items from the kv cach and not all the kv cach not all the tensor and this is done with block diagonal with offset pading mas so this method here its very long name very complicated but this is why they use it and it will produce a mask like this so it takes into consideration the actual size of the kv c even if the k all the kv cach have the same size because its a fixed size tensor but it tells you how many items there actually it should use from each cache okay guys it has been a very demanding video i have to say uh i had to record it more than once i actually had to cut some parts because i even i got confused sometimes uh its very complicated topics its a lot of things that you have to grasp but i hope that it will make your life easier when you want to understand the mistal code i actually am also putting online my notes the one that you have seen so the two notebooks that i have shown you plus also the code annotated by me on the mistal source code now the mistal source code i actually never run it so because my computer is not very powerful so i never run the actual model on my computer what i did to study the model was to run some random tensors through a model and i created basically a model with randomly initialized uh weights but with less number of layers so it could fit in my gpu and then i just run some random tensors to study all the shapes of the tensor and all the information passing so i dont know if the code works but i hope it will works i mean i didnt touch the logic i just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model i actually really enjoyed studying it i really enjoyed studying the code and i learned a lot of stuff you know um i think its very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you dont learn much by the end of the day anyway guys thanks you for watching my video i hope you also enjoyed this journey with me even if it was very complicated i hope that you likeed this video and you will subscribe to my channel if you didnt please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on linkedin on twitter etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you dont understand i am always available to help and connect with me on linkedin byebye 

#####################
Shot: mixtral8x7b
#####################
#####################
Shot: full-stack
#####################
#####################
Target: mixtral8x7b
#####################
hello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of innovations in the code especially when they use the x forers library with the block attention so i want to guide you into understanding the code because it can be really hard for beginners to understand and find thems around there are some topics that are related to mistal but will not be covered in this current video because i already covered them in my previous video about llama and in particular i will not be talking about the rms normalization the rotary positional encodin  and we will see later uh how it works but we just remember that we have eight experts and for each token we use two experts but later i will clarify how it works lets proceed further so lets talk about the sliding window attention but before we talk about the sliding window attention i need to review a little bit of the self attention mechanism so what is self attention self attention is a mechanism that allows the model to relate tokens to each other so tokens that are in the same sentence are related with each other through the self attention mechanism this is why its called self attention because each token is watching other tokens of the of the same sentence and when when and this is means basically that the query key and values are the same metrix um so imagine we have the following  only depend on itself the second token will only depend on the first two tokens the third output token will only depend on the first three tokens the fourth will depend on the token number two because the first token is not used the token number two the token number three and the token number four etc etc until the last here the last token will depend only on the last three tokens because the first three tokens are masked out and this is the importance of the mask that we apply in the self attention mechanism this concept that i show you now is very important to understand the rest of the video so please if you didnt understand it you can take a little pause you can try to do it by your by yourself because its really important that you understand how the self attention mechanism works with his will become the new input which will produce love that can then we take the last token of the output and we append it to the input for the time step four so it will become the new output will become love that can quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the last token output by the model because we already have the previous one but of course we need to feed all the previous tokens to uh to to the model which is belonging to the prompt because the model ne aling with a cy cach we need to build up this kv cache so we need to add the tokens of our prompt to the kv cache that so that we can then exploit this kv cache to build new tokens future tokens now the prompt is known in advance right because because its the input of our user its what you ask to ch gpd for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so its known in advance so we dont we dont need to generate it okay so what we can do is we can prefill the kv cache using the tokens of the prompt but there are many ways to do it like we were doing before when i was teaching you about the k cach we work with one token at a time so one way to um to add the tokens to the k cach is to add one token at at a time but this can be very time consuming be m of the wids to be only one otherwise the output range may change from model to model and usually its not a good idea to have the range of the output to change from one model to the next so to keep the range of the output stable they apply the soft marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so lets talk about it when we have a model that is too big to fit in a single gpu we can divide the model into groups of layers and place each group of layers in a single gpu for example in the case of mistal we have 32 layers of encoders you can see here one after another i didnt do  and then we buil this sequence which is a concatenation of all the prompts that we receive we take this mega sequence we run it through our llm ser llm model so it could be mistal or it could be llama this as i told you before uh and input sequence in a transformer will result in n output tokens in the output so we have uh here we have three  4 so 7 7  5 12 tokens as input it will produce 12 tokens as output to understand what is the next token for each prompt we need to check the the we need to check the embedding corresponding to the token number three for the first prompt to the token number seven for the second prompt and the last token for the third prompt so we take all these embeddings we run them through the linear layer then we apply the soft marks and then we understand what is  because my computer is not very powerful so i never run the actual model on my computer what i did to study the model was to run some random tensors through a model and i created basically a model with randomly initialized uh weights but with less number of layers so it could fit in my gpu and then i just run some random tensors to study all the shapes of the tensor and all the information passing so i dont know if the code works but i hope it will works i mean i didnt touch the logic i just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model i actually really enjoyed studying it i really enjoyed studying the code and i learned a lot of stuff you know um i think its very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you dont learn much by the end of the day anyway guys thanks you for watching my video i hope you also enjoyed this journey with me even if it was very complicated i hope that you likeed this video and you will subscribe to my channel if you didnt please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on linkedin on twitter etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you dont understand i am always available to help and connect with me on linkedin byebye 

whats up guys back today with another longer video which is to build a job board app that could actually be a real startup that people use theres a lot of different places to find entry level or junior developer jobs on the internet for example github jobs stack overflow jobs you have angel list you have indeed but filtering through those especially because you kind of have to spam applications for your first job can take a long time so the good thing about all these job boards is they have public apis for the most part you always want to start with an mvp that is the minimum viable product today that means were going to start with a single data feed that is a single job board api feeding into our app and then finally were gonna have a super minimal simple front end were gonna do this in react and material ui on the front end were gonna use an express server redis for our data storage because were not writing or persisting any data that is not part of the job feed and then finally a node cron job to fetch these jobs and refresh them at a certain interval which are going to be filtered through an algorithm so lets draw this all out to get a better idea of our plan heres our front end what we basically want here is a list of jobs this is going to be pulling data from our api and that will be express and then this will pull from our store which is actually going to be redis and redis will be populated by our cron called a worker which is pulling from music one or more apis its going to be eventually multiple this is going to be job uh board apis so like in lets say indeed we h ew terminal tab going out to our main directory and within our main directory im just going to create a package json which is going to be accessible from all our different folders so you can see that was created here and then we just want to install cron as a dependency okay now inside a worker i just this is the way i pretty much start with every uh with every app you just copy kind of the basic usage and you build your way out from there so all this does is its running a node process where were importing the library and then this is called a um cron shorthand cron syntax something like that uh cronguru you can see it here okay so this site is really useful no not this site crown guru crontab guru okay so these asterisks are called a cron schedule expression and its basically symbols for  ur redis store and then you can serve it up in our case jumping back over to these um these job uh json that we got from github you can see every job has a uuid which is a guaranteed unique id because it has so many characters and its random characters so itll be guaranteed unique every time every job has one of these ids so thats a natural candidate to use as a redis key im thinking within redis our key is going to be this id and our value is just going to be the whole job object so it would be a string version of this that we can then pull out we know its already json so we can just stringify it and then when we pull it out well just parse it into a javascript object now the slightly tricky part of this is okay how do we access redis through our node um worker right because we have all t  length matches our our length that we put in lets parse it and then lets do dot lake okay so its kind of a lot that we did without testing it lets restart our server do the same curl oh man okay 245 i dont remember if thats how many we put in were definitely getting a list of 245 which is a good sign now lets just try sending this back and see if we get that full uh stringified object thats exactly what we want and its that easy so thats already done so i dont know how long its been guys but with this simple of an api we can literally weve literally been able to create all this like in such a short amount of time we have a data feed and you can replicate like before before i go further you can replicate this in any this is like 90 applications if you have a data feed you have storage you   function and um all right finally im going to just put this whole thing below our uh jobs map and um lets see if by pasting it directly in we we need to do anything else just going to make sure thats formatted correctly saved it and um lets just take this out classes direction rtl okay lets see how theme is being used because this is kind of a um darn all right i dont know why that would change from left to right but um it might be a mobile implementation specific thing so lets just take this and hard code it as right and well hard code this one as left and see if we run in any problems okay save that and cant find module oh i forgot about this material ui icons is a separate library so we have to also install that in our client yarn add material ui icons now while thats installing lets t configurations and ill put all these links in the description so like i mentioned first uh we need a domain i already bought one on google domains but you can buy a domain a ton of different places i just like google because the interface and its because where my uh other domains are so ive opened my domain i already bought it a normalcom domain is going to be about 14 a year and the first thing we need to do is use the digital ocean name servers so that domain can be directed over to our server so youll see that i have clicked on my domain ive opened the dns tab and then theres a name servers tab and ive clicked use custom name servers now im going to put that on pause and jump over to the digital ocean dashboard you can make a digitalocean account for free and itll give you this dashboar  one critical change that will allow us to access our api on our local server and that is to basically change this to just a relative path so im changing this to just slash jobs and actually i want to change it to be a little more clear to be slash api jobs now that means we also have to change our api route which shouldnt be a problem slash api slash jobs and we have to make one change in our package json and thats adding a proxy config which allows this to still work on local development this doesnt affect anything on production so um proxying all api routes to a target of localhost sorry need the double quotes there http localhost uh 3001 was our api server i believe and ill save that and again that just helps us you know still develop in the same way but we need to make one config chan ocal if i click into these jobs lets see uh it works theres our description logo and everything and then if i click apply itll take us to that position so thats pretty cool guys we just deployed our whole app and it didnt even take that long i feel like that was around 30 minutes or so all right guys thats it we basically built a whole production application in less than 100 minutes and we got it deployed and now its running on the public internet at entrylevelsoftwarejobscom so go check it out but um im thinking next theres a few critical things we have to do before were ready for like a full production launch so to speak this was just our base framework that were building more features off of but at minimum i think we need a few more data feeds and we need to make our algorithm a lot more robust prevent it from breaking on errors as well as filtering down those jobs a lot more because if we get in five feeds were just gonna have a huge amount of jobs and we really wanna make sure those are entry level jobs so im thinking that we parse through the description and look for like three to five years of experience and take those out i think anything less than three years would qualify as an entrylevel job so again were gonna have to keep working on the algorithm and then otherwise were gonna want to add a little bit of monitoring logging different things like that but that can all come later our mvp is done and our base of our application is done so thats pretty cool so anyway hope you learned something um let me know if you have any questions and i will talk to you guys soon 

hold up before we get into this next episode i want to tell you about our virtual conference thats coming up on february 15th and february 22nd we did it two thursdays in a row this year because we wanted to make sure that the maximum amount of people could come for each day since the lineup is just looking absolutely incredible as you know we do let me name a few of the guests that weve got coming because it is worth talking about weve got jason louie weve got shrea shanar weve got dro who is product applied ai at uber weve got cameron wolf whos got an incredible podcast and hes director of ai at reeby engine weve got lauren lockridge who is working at google also doing some product stuff oh why is there so many product people here funny you should ask that because weve got a whole ai product owner track along with an engineering track and then as we like to weve got some handson workshops too let me just tell you some of these other names just for a moment you know because weve got them coming and it is really cool i havent named any of the keynotes yet either by the way go and check them out on your own if you want just go to home ops community and youll see but weve got tunji whos the lead researcher on the deep speed project at microsoft weve got golden who is the open source engineer at netflix weve got kai whos leading the ai platform at uber you may have heard of it its called michelangelo oh my gosh weve got fison whos product manager at linkedin jerry louie who created good old llama index hes coming weve got matt sharp friend of the pod shreya raj paul the creator and ceo of guard rails oh my gosh the list goes on theres 7 plus people that will be with us at this conference so i hope to see you there and now lets get into this podcast hey everyone my name is apara um im one of the cofounders of arise ai um and i recently stopped drinking coffee so i take i ive started on machil lattes instead hello and welcome back to the mops community podcast as always i am your host dimitri o and were coming at you with another fire episode this one was with my good and old friend apara and she has been doing some really cool stuff in the evaluation space most specifically the llm evaluation space we talked all about how they are looking at evaluating the whole llm systems and of course course she comes from the observability space and for those that dont know shes cofounder of arise and arise is doing lots of great stuff in the observability space theyve been doing it since the traditional mlops days and now theyve got this open source package phoenix that is for the new llm days and you can just tell that she has been diving in head first shes chief product officer and she has really been thinking deeply about how to create a product that will help people along their journey when it comes to using llms and really making sure that your llm is useful  ddle of nowhere germany with you know 100 cows and maybe like 50 people in the village that were in so thats the short story of it wow w well thats an interesting intro there you go i mean we were just talking and i will mention this to the listeners because we were talking about how you moved from california to new york and you are freezing right now because it is currently winter there and germany isnt known for its incredible weather but its definitely not like new york that is for sure yeah its a east coast winter out here so i wanted to jump in to the evaluation space because i know youve been kne deep in that for like last year youve been working with all kinds of people and maybe you can just set the scene for us because youre currently for those who do not know you i probably said it in the intro already but i will say it again youre the head product or chief product officer i think is the official title at rise and you have been working in the observability space for ages before you started rise uh you were at uber and working on good old michelangelo with that crew that is uh got very famous from the paper and then youve been talking a ton to people about how theyre doing observability in the quote unquote traditional ml space but then when llms came out you also started talking to people about okay well how do we do observability whats important with observability in the llm space and so id love to hear you set the   response is it negative about you know like in the one where its kind of talking its own product is it is it negative in in its own response is it um correctness factuality so all of these are things that you can actually generate a prompt to you know generating you know basically an eval template to go in and say well heres what the user asked heres what all the relevant information we pulled was and heres the final response that the lm came back with does the response actually answer the question that the user asked and two does it actually is that answer based on something factual aka the stuff that it was pulled on the retrieval component and you can go in and score that and you know what we end up seeing a lot is if the response isnt actually based on the retrieval then its its its hallucinating and they actually dont want to show those types of responses back to the user um and so this is this is just a very specific you know id say the hallucination eval the correctness eval summarization uh all of these are very common kind of llm task evals that were seeing out in in kind of the wild right now i should mention its very very different than what um the model evals are which is a whole another yeah category of evals you might be seeing like if you go on like hugging face for instance has a whole open source llm leaderboard im sure youve all seen it its like changes every couple of hours um and they have all these metric long is the context so the context can be one k to tokens all the way to you know for some of the smaller models its like 32k i think for some of the bigger ones um we tested uh pretty pretty significantly let me rouble check exactly what 120k i would imagine i think that it feels like anthropics goes all the way up to that or maybe its even more these days like 240 they just said it well double it yeah so some of them we checked yeah like definitely close to yeah 120k and but what we did was basically so thats thats on one axis which is basically just the context length and then on the other ais is basically where in the context you put the information so you know cuz some there theres all these theories out there like if you put it early on does it forget if you put it later down does it not use it and so kind of placement of the context within that context window to see you know can you actually find the needle in the hast stack and the question we did for this was um so a little context was the question we asked llm was um whats the so we did kind of like a key value pair the key was the city and then the value is a number so we said something like whats romes special magic number and so uh and then inside the context we put something like rome special magic number is uh like some seven digigit number uh like yeah like 1 2 3 4 5 6 s and so that was a rome special magic number and then later wed asked we put that somewhere  ppened then being able to pinpoint exactly what are the calls that are happening under the hood and how do i get visibility is important and so with phoenix um one of the most popular components of it is you can see your full you know you can full see the full traces and spans of your application so kind kind of like the full stack traces how you can think about it so um youll see the breakdown of each calls and then which calls took longer which calls used the most tokens and then you can also evaluate at each step in the calls so kind of like we were just talking about where at the end of the application at the very end um when it generated response you can have a score of of how well was the response but then if the response lets say was hallucinated or was incorrect then theres a step above you can go in and look at the individual span level evals look at well how well did it retrieve and then within the retriever you know how lets evaluate each document that it retrieved and see if it was relevant or not so theres kind of a lot of thought put into first how do i break down the entire application stack and then see you know evaluate and evaluate each step of that outcome and then the other part thats been id say really a lot of thought in is phoenix does come with an evals library um its task evals first or you know you knowm application evals so its its definitely useful for folks who are actually building the application ng to be able to eat up the test sets as soon as they get put out for anyone to see or is it going to be all right ive got my evaluation test set and i just keep it inhouse im not going to let anybody else see that so that i dont paint the model yeah i its actually thing i wonder about a lot too um is as these new llms come out are they really blind to the test sets that theyre actually then evaluating them on i think um like the gemini paper i thought did a really good job of calling out they actually built their own data set that was blind and then tested on that data set and um they called that out explicitly which i thought was really important because you know as as people are sharing the results of like the next best lum etc yeah i think were all wondering like did you just you know like you know did did have access to that training data set so i i wonder that all the time too well its pretty clear these days that uh as i did not coin this term but i like it and i will say it a lot benchmarks are and so all these benchmarks on hugging face or on twitter that youll see like oh this is soa this just came out it blew everything else out of the water by whatever 10 times or you make up a number there i dont even consider that to be valuable anymore its its really like what you were saying where these things i know you actually went and you did a rigorous study on it but its so funny because we are the rest of us are just goi in this space finetuning feels like youre jumping to like level 100 when sometimes a lot of this could be you know like i was telling you in the rag case change the prompt a bit and you get vly different responses and so its like almost the thing that were like geared towards to do which is like oh it makes sense were going to training is now fine tuning and were all used to that paradigm but i i think in this space lets start with like the lowest hanging fruit and see how that improves um because i i i think you know and and drudge carpy actually drew this like really awesome image of like you know level of effort versus the roi you know kind of kind of of that effort and prompt engineering rack like theres so many things you could do to improve the lms performance before you jump into finetuning or like training your own llm so its just i think its important to like start with with something that could have the highest ry you are preaching to the choir and i laugh because i was like talking about how fine tuning to me feels like when all else fails youll throw some fine tuning at it and its like yeah thats what you need to you need to look at it as like the escape hatch almost not as step two it should be what you go to when you cant get anything else to work and try rigorously to get everything else to work because it is exactly like you said it is so much easier to just tweak the prompt then finetune it and and i didnt con outcome that they need their application to get to um and so again i think i just come back to like theres you know all for the open source community all for our you know just getting your application to actually work as as good as it needs to to work but start with start with like what do you need for the application and last of like i think the hows this going to scale yeah like i conversation back in the day where youre like oh were going to need to use kubernetes for this and youre like wait a minute we have no users are you sureet youre i know youre planning for the future and this is great for the tech debt but uh we might want to just get some up on streamlet before we do anything totally totally totally and i think that thats like what i keep coming back to is like the more of these you know similar in the ml space we want to get more of these deployed actually in the real world get the br application to add value to the organization show the roi and i think that um thats really important to to the success of these llms and companies actually and the other piece to this that i find fascinating was something that llo said probably like two years ago and llo is infamous person in the community for those who do not know in the community slack and he was talking about how you need to get something in production as fast as possible because then youll find where all of the bottlenecks are youll find where everything is messing up and unless you get into production you dont necessarily know that so each day or each minute that youre not in production youre not finding all of these problems and if you can use a model to make your life easier and get you into production faster then youre going to start seeing oh maybe its the prompts or oh maybe its you know whatever the case may be where youre falling behind and youre making mistakes or the system isnt designed properly yeah absolutely so i think uh maybe as we wrap up the podcast that thats really is get stuff out as fast as you can you know evaluate the outcomes i think thats you know lm ev vales is something that i think is pretty pretty got a lot of momentum around it in in the in folks who are deploying and in the community so evaluations is important and then um i think knowing how to set up the right evals knowing how to you know benchmark your own evals um customize it um what types of eval score versus classification theres just so much nuance in that whole eval space and so as we continue to drop more research or share more stuff were learning um we well share with the community excellent parta its been absolutely fascinating having you on as always i really appreciate it and look forward to having you back awesome thanks to me ch thanks and thanks mops community hey everyone my name is apara founder of arise and the best way to stay up to dat with mlops is by subscribing to this podcast 

#####################
Shot: mixtral8x7b
#####################
#####################
Shot: full-stack
#####################
#####################
Target: mlops_llm_eval
#####################
hello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of innovations in the code especially when they use the x forers library with the block attention so i want to guide you into understanding the code because it can be really hard for beginners to understand and find thems around there are some topics that are related to mistal but will not be covered in this current video because i already covered them in my previous video about llama and in particular i will not be talking about the rms normalization the rotary positional encodin  and we will see later uh how it works but we just remember that we have eight experts and for each token we use two experts but later i will clarify how it works lets proceed further so lets talk about the sliding window attention but before we talk about the sliding window attention i need to review a little bit of the self attention mechanism so what is self attention self attention is a mechanism that allows the model to relate tokens to each other so tokens that are in the same sentence are related with each other through the self attention mechanism this is why its called self attention because each token is watching other tokens of the of the same sentence and when when and this is means basically that the query key and values are the same metrix um so imagine we have the following  only depend on itself the second token will only depend on the first two tokens the third output token will only depend on the first three tokens the fourth will depend on the token number two because the first token is not used the token number two the token number three and the token number four etc etc until the last here the last token will depend only on the last three tokens because the first three tokens are masked out and this is the importance of the mask that we apply in the self attention mechanism this concept that i show you now is very important to understand the rest of the video so please if you didnt understand it you can take a little pause you can try to do it by your by yourself because its really important that you understand how the self attention mechanism works with his will become the new input which will produce love that can then we take the last token of the output and we append it to the input for the time step four so it will become the new output will become love that can quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the last token output by the model because we already have the previous one but of course we need to feed all the previous tokens to uh to to the model which is belonging to the prompt because the model ne aling with a cy cach we need to build up this kv cache so we need to add the tokens of our prompt to the kv cache that so that we can then exploit this kv cache to build new tokens future tokens now the prompt is known in advance right because because its the input of our user its what you ask to ch gpd for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so its known in advance so we dont we dont need to generate it okay so what we can do is we can prefill the kv cache using the tokens of the prompt but there are many ways to do it like we were doing before when i was teaching you about the k cach we work with one token at a time so one way to um to add the tokens to the k cach is to add one token at at a time but this can be very time consuming be m of the wids to be only one otherwise the output range may change from model to model and usually its not a good idea to have the range of the output to change from one model to the next so to keep the range of the output stable they apply the soft marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so lets talk about it when we have a model that is too big to fit in a single gpu we can divide the model into groups of layers and place each group of layers in a single gpu for example in the case of mistal we have 32 layers of encoders you can see here one after another i didnt do  and then we buil this sequence which is a concatenation of all the prompts that we receive we take this mega sequence we run it through our llm ser llm model so it could be mistal or it could be llama this as i told you before uh and input sequence in a transformer will result in n output tokens in the output so we have uh here we have three  4 so 7 7  5 12 tokens as input it will produce 12 tokens as output to understand what is the next token for each prompt we need to check the the we need to check the embedding corresponding to the token number three for the first prompt to the token number seven for the second prompt and the last token for the third prompt so we take all these embeddings we run them through the linear layer then we apply the soft marks and then we understand what is  because my computer is not very powerful so i never run the actual model on my computer what i did to study the model was to run some random tensors through a model and i created basically a model with randomly initialized uh weights but with less number of layers so it could fit in my gpu and then i just run some random tensors to study all the shapes of the tensor and all the information passing so i dont know if the code works but i hope it will works i mean i didnt touch the logic i just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model i actually really enjoyed studying it i really enjoyed studying the code and i learned a lot of stuff you know um i think its very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you dont learn much by the end of the day anyway guys thanks you for watching my video i hope you also enjoyed this journey with me even if it was very complicated i hope that you likeed this video and you will subscribe to my channel if you didnt please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on linkedin on twitter etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you dont understand i am always available to help and connect with me on linkedin byebye 

whats up guys back today with another longer video which is to build a job board app that could actually be a real startup that people use theres a lot of different places to find entry level or junior developer jobs on the internet for example github jobs stack overflow jobs you have angel list you have indeed but filtering through those especially because you kind of have to spam applications for your first job can take a long time so the good thing about all these job boards is they have public apis for the most part you always want to start with an mvp that is the minimum viable product today that means were going to start with a single data feed that is a single job board api feeding into our app and then finally were gonna have a super minimal simple front end were gonna do this in react and material ui on the front end were gonna use an express server redis for our data storage because were not writing or persisting any data that is not part of the job feed and then finally a node cron job to fetch these jobs and refresh them at a certain interval which are going to be filtered through an algorithm so lets draw this all out to get a better idea of our plan heres our front end what we basically want here is a list of jobs this is going to be pulling data from our api and that will be express and then this will pull from our store which is actually going to be redis and redis will be populated by our cron called a worker which is pulling from music one or more apis its going to be eventually multiple this is going to be job uh board apis so like in lets say indeed we h ew terminal tab going out to our main directory and within our main directory im just going to create a package json which is going to be accessible from all our different folders so you can see that was created here and then we just want to install cron as a dependency okay now inside a worker i just this is the way i pretty much start with every uh with every app you just copy kind of the basic usage and you build your way out from there so all this does is its running a node process where were importing the library and then this is called a um cron shorthand cron syntax something like that uh cronguru you can see it here okay so this site is really useful no not this site crown guru crontab guru okay so these asterisks are called a cron schedule expression and its basically symbols for  ur redis store and then you can serve it up in our case jumping back over to these um these job uh json that we got from github you can see every job has a uuid which is a guaranteed unique id because it has so many characters and its random characters so itll be guaranteed unique every time every job has one of these ids so thats a natural candidate to use as a redis key im thinking within redis our key is going to be this id and our value is just going to be the whole job object so it would be a string version of this that we can then pull out we know its already json so we can just stringify it and then when we pull it out well just parse it into a javascript object now the slightly tricky part of this is okay how do we access redis through our node um worker right because we have all t  length matches our our length that we put in lets parse it and then lets do dot lake okay so its kind of a lot that we did without testing it lets restart our server do the same curl oh man okay 245 i dont remember if thats how many we put in were definitely getting a list of 245 which is a good sign now lets just try sending this back and see if we get that full uh stringified object thats exactly what we want and its that easy so thats already done so i dont know how long its been guys but with this simple of an api we can literally weve literally been able to create all this like in such a short amount of time we have a data feed and you can replicate like before before i go further you can replicate this in any this is like 90 applications if you have a data feed you have storage you   function and um all right finally im going to just put this whole thing below our uh jobs map and um lets see if by pasting it directly in we we need to do anything else just going to make sure thats formatted correctly saved it and um lets just take this out classes direction rtl okay lets see how theme is being used because this is kind of a um darn all right i dont know why that would change from left to right but um it might be a mobile implementation specific thing so lets just take this and hard code it as right and well hard code this one as left and see if we run in any problems okay save that and cant find module oh i forgot about this material ui icons is a separate library so we have to also install that in our client yarn add material ui icons now while thats installing lets t configurations and ill put all these links in the description so like i mentioned first uh we need a domain i already bought one on google domains but you can buy a domain a ton of different places i just like google because the interface and its because where my uh other domains are so ive opened my domain i already bought it a normalcom domain is going to be about 14 a year and the first thing we need to do is use the digital ocean name servers so that domain can be directed over to our server so youll see that i have clicked on my domain ive opened the dns tab and then theres a name servers tab and ive clicked use custom name servers now im going to put that on pause and jump over to the digital ocean dashboard you can make a digitalocean account for free and itll give you this dashboar  one critical change that will allow us to access our api on our local server and that is to basically change this to just a relative path so im changing this to just slash jobs and actually i want to change it to be a little more clear to be slash api jobs now that means we also have to change our api route which shouldnt be a problem slash api slash jobs and we have to make one change in our package json and thats adding a proxy config which allows this to still work on local development this doesnt affect anything on production so um proxying all api routes to a target of localhost sorry need the double quotes there http localhost uh 3001 was our api server i believe and ill save that and again that just helps us you know still develop in the same way but we need to make one config chan ocal if i click into these jobs lets see uh it works theres our description logo and everything and then if i click apply itll take us to that position so thats pretty cool guys we just deployed our whole app and it didnt even take that long i feel like that was around 30 minutes or so all right guys thats it we basically built a whole production application in less than 100 minutes and we got it deployed and now its running on the public internet at entrylevelsoftwarejobscom so go check it out but um im thinking next theres a few critical things we have to do before were ready for like a full production launch so to speak this was just our base framework that were building more features off of but at minimum i think we need a few more data feeds and we need to make our algorithm a lot more robust prevent it from breaking on errors as well as filtering down those jobs a lot more because if we get in five feeds were just gonna have a huge amount of jobs and we really wanna make sure those are entry level jobs so im thinking that we parse through the description and look for like three to five years of experience and take those out i think anything less than three years would qualify as an entrylevel job so again were gonna have to keep working on the algorithm and then otherwise were gonna want to add a little bit of monitoring logging different things like that but that can all come later our mvp is done and our base of our application is done so thats pretty cool so anyway hope you learned something um let me know if you have any questions and i will talk to you guys soon 

#####################
Shot: mixtral8x7b
#####################
#####################
Shot: full-stack
#####################
#####################
Target: react
#####################
