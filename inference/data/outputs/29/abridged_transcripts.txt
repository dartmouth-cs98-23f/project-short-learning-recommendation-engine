hello guys welcome back to my channel today we are going to talk about mistral so as you know mistral is a new language model that came out a few months ago from mistal ai which is a one of the hottest startup right now in europe for language models it also became a unicorn recently and we will exploring both the models they released one is the 7 billion and one is the 8 by7 billion model so lets review the topics of today the first thing i will introduce you is the architectural differences between the vanilla transformer and the architecture of mistal later we will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we find in convolutional neural networks i will briefly review the kv cache because i want to introduce the concept of rolling buffer cache and also how it is done with prefilling and chunking uh we will see what is sparse mixture of experts model sharding with a little with a very brief introduction with the pipeline parallelism and last but not least we will also go through the code of mistal because there is a lot of innovations in the code especially when they use the x forers library with the block attention so i want to guide you into understanding the code because it can be really hard for be th is uh what is the context size of upon which it the model was trained upon uh and its much bigger for the 8x 7b the vocabulary size is the same for both and then the last two parameters you can see here are related to the sparse mixture of experts and we will see later uh how it works but we just remember that we have eight experts and for each token we use two experts but later i will clarify how it works lets proceed further so lets talk about the sliding window attention but before we talk about the sliding window attention i need to review a little bit of the self attention mechanism so what is self attention self attention is a mechan ding on the mask we have applied so our mask says that the first token can only watch itself so the first output token will be an embedding that will only depend on itself the second token will only depend on the first two tokens the third output token will only depend on the first three tokens the fourth will depend on the token number two because the first token is not used the token number two the token number three and the token number four etc etc until the last here the last token will depend only on the last three tokens because the first three tokens are masked out and this is the importance of the mask that we apply in the self atten pend it to the input for the temp step three and this will become the new input which will produce love that can then we take the last token of the output and we append it to the input for the time step four so it will become the new output will become love that can quickly then we take this word quickly we append it to the input for the next time step and it will produce the next token as output etc etc until the last token until we see the end of sentence token as output then we know that the model has stopped uh um has stopped producing new tokens and we can stop the inference now at every step the inference we are only interested in the l ache so we need to add the tokens of our prompt to the kv cache that so that we can then exploit this kv cache to build new tokens future tokens now the prompt is known in advance right because because its the input of our user its what you ask to ch gpd for example right tell me a poem tell me write me a poem or tell me a joke this is our prompt so its known in advance so we dont we dont need to generate it okay so what we can do is we can prefill the kv cache using the tokens of the prompt but there are many ways to do it like we were doing before when i was teaching you about the k cach we work with one token at a time so one way to um to   to change from one model to the next so to keep the range of the output stable they apply the soft marx after they have selected how many uh experts they want to work with and choosing the logits of the best two performing uh experts okay the next thing we are talking going to talk about is model sharding which is also implemented in the code of the mistal model so lets talk about it when we have a model that is too big to fit in a single gpu we can divide the model into groups of layers and place each group of layers in a single gpu for example in the case of mistal we have 32 layers of encoders you can see here one after another i didnt do in a transformer will result in n output tokens in the output so we have uh here we have three  4 so 7 7  5 12 tokens as input it will produce 12 tokens as output to understand what is the next token for each prompt we need to check the the we need to check the embedding corresponding to the token number three for the first prompt to the token number seven for the second prompt and the last token for the third prompt so we take all these embeddings we run them through the linear layer then we apply the soft marks and then we understand what is the next uh token from our vocabulary but you may be wondering how do we even produce an attention m i just run some random tensors to study all the shapes of the tensor and all the information passing so i dont know if the code works but i hope it will works i mean i didnt touch the logic i just add some comments uh anyway you can use the commented code by me to as um as a learning tool to complement with the official code of mistal so that you can understand uh more about the inner workings of this grd model i actually really enjoyed studying it i really enjoyed studying the code and i learned a lot of stuff you know um i think its very very good when you are doing something that is very complicated because it teaches you a lot because if something is simple then you dont learn much by the end of the day anyway guys thanks you for watching my video i hope you also enjoyed this journey with me even if it was very complicated i hope that you likeed this video and you will subscribe to my channel if you didnt please do it and the best way to support me guys is to share this video with all the people you know so share it on social media share it on linkedin on twitter etc because this is the best way to you can help me is to grow my channel and um please let me know if there is something that you dont understand i am always available to help and connect with me on linkedin byebye 

whats up guys back today with another longer video which is to build a job board app that could actually be a real startup that people use theres a lot of different places to find entry level or junior developer jobs on the internet for example github jobs stack overflow jobs you have angel list you have indeed but filtering through those especially because you kind of have to spam applications for your first job can take a long time so the good thing about all these job boards is they have public apis for the most part you always want to start with an mvp that is the minimum viable product today that means were going to start with a single data feed that is a single job board api feeding into our app and then finally were gonna have a super minimal simple front end were gonna do this in react and material ui on the front end were gonna use an express server redis for our data storage because were not writing or persisting any data that is not part of the job feed and then finally a node cron job to fetch these jobs and refresh them at a certain interval which are going to be filtered through an algorithm so lets draw this all out to get a better idea of our plan heres our front end what we basically want here is a list of jobs this is going to be pulling data from our api and th r tasks and i know already that were going to need the cron node library its just going to help us run simple cron jobs which is what i was saying before is the its the way you run a task on an interval basically forever if you want to opening up a new terminal tab going out to our main directory and within our main directory im just going to create a package json which is going to be accessible from all our different folders so you can see that was created here and then we just want to install cron as a dependency okay now inside a worker i just this is the way i pretty much start with every uh with every app you just copy kind of the basic   though that its really fast its kind of like an object in javascript you just if you have the key its constant time access you just pull it out of your redis store and then you can serve it up in our case jumping back over to these um these job uh json that we got from github you can see every job has a uuid which is a guaranteed unique id because it has so many characters and its random characters so itll be guaranteed unique every time every job has one of these ids so thats a natural candidate to use as a redis key im thinking within redis our key is going to be this id and our value is just going to be the whole job object so it would be  just to see the length and or rather see that the length matches our our length that we put in lets parse it and then lets do dot lake okay so its kind of a lot that we did without testing it lets restart our server do the same curl oh man okay 245 i dont remember if thats how many we put in were definitely getting a list of 245 which is a good sign now lets just try sending this back and see if we get that full uh stringified object thats exactly what we want and its that easy so thats already done so i dont know how long its been guys but with this simple of an api we can literally weve literally been able to create all this like in such a t put this whole thing below our uh jobs map and um lets see if by pasting it directly in we we need to do anything else just going to make sure thats formatted correctly saved it and um lets just take this out classes direction rtl okay lets see how theme is being used because this is kind of a um darn all right i dont know why that would change from left to right but um it might be a mobile implementation specific thing so lets just take this and hard code it as right and well hard code this one as left and see if we run in any problems okay save that and cant find module oh i forgot about this material ui icons is a separate library so we   you can buy a domain a ton of different places i just like google because the interface and its because where my uh other domains are so ive opened my domain i already bought it a normalcom domain is going to be about 14 a year and the first thing we need to do is use the digital ocean name servers so that domain can be directed over to our server so youll see that i have clicked on my domain ive opened the dns tab and then theres a name servers tab and ive clicked use custom name servers now im going to put that on pause and jump over to the digital ocean dashboard you can make a digitalocean account for free and itll give you this dashboar pi jobs now that means we also have to change our api route which shouldnt be a problem slash api slash jobs and we have to make one change in our package json and thats adding a proxy config which allows this to still work on local development this doesnt affect anything on production so um proxying all api routes to a target of localhost sorry need the double quotes there http localhost uh 3001 was our api server i believe and ill save that and again that just helps us you know still develop in the same way but we need to make one config change for the production server as well so that was our change in um in our files and as i mentioned im s thats it we basically built a whole production application in less than 100 minutes and we got it deployed and now its running on the public internet at entrylevelsoftwarejobscom so go check it out but um im thinking next theres a few critical things we have to do before were ready for like a full production launch so to speak this was just our base framework that were building more features off of but at minimum i think we need a few more data feeds and we need to make our algorithm a lot more robust prevent it from breaking on errors as well as filtering down those jobs a lot more because if we get in five feeds were just gonna have a huge amount of jobs and we really wanna make sure those are entry level jobs so im thinking that we parse through the description and look for like three to five years of experience and take those out i think anything less than three years would qualify as an entrylevel job so again were gonna have to keep working on the algorithm and then otherwise were gonna want to add a little bit of monitoring logging different things like that but that can all come later our mvp is done and our base of our application is done so thats pretty cool so anyway hope you learned something um let me know if you have any questions and i will talk to you guys soon 

this video is about a new approach to training language models specifically it allows you to move the probabilities of a language model away from bad answers and towards good answers direct preference optimization or dpo for short is a type of reinforcement learning its a more efficient type than has long been used by companies like open ai and meta in developing lama 2 and with this dpo technique it makes it much easier to find you models towards a chat format that is aligned in a similar way to those commercial models available for agenda were going to take a look at how normal training works standard finetuning or supervised finetuning as ive done in previous videos then ill contrast how direct preference optimization works and why its a bit different in terms of a technique and its useful in a different and complimentary way to standard finetuning then ill show you two dpo data sets the data sets need to be in a specific format if youre going to use the hugging face trainer that we will use today and then ill walk step by step through the training notebook ive developed for doing direct preference optimization by the end of this tutorial you should be able to take a model youve developed a language model ideally you do some supervised finetuning to get it into a chat format and this will allow you to do the final stage of aligning the model um another way to think of that is moving the model probabilities away from bad answers and towards good answers that will make sense have is the actual next token but here in direct preference optimization we have a pair of options we have a chosen and we have a rejected option and the way that we penalize the model now is not by comparing the actual to the predicted but rather we penalize the model so that the probability of dublin of the model were training we want that probability to be high relative to a ref reference model the reference model is just a copy of the model when we start this whole dpo step so imagine starting dpo we have a model we duplicate it so we now have a model and a reference model and during the training were going to penalize the model in such a way to incentivize the probability of the model being trained increasing for dublin relative to the e the ultrat set the other data set thats available is helpful and harmless from anthropic its 160rows about 40type and 120sets been around for a while and its been used to train lama 2 um its probably being used to train a lot of models and it does something a little different than just pointing uh the model from bad answers to good answers its also trying to achieve a level of safety and avoid the model spitting out uh perhaps s harmful dangerous type results just as an example one uh instruction set that you might see a type of instruction set you might see in the harmless part of the data set is as follows the prompt might be how do i do something dangerous and the chosen answer which is what we want to incentivize the model towards is  so im not sure the end of sequence token is required but just for consistency i think its good to format it like this the next thing were going to need is a reference model i have a reference model here which is the tiny lama model i said i would be training on this is a model that i have trained with supervised finetuning using the open assist data set its available here um for you to download if you like and as i said its a chat finetuned model tiny lama is only trained on a 2k context length i did the finetuning using 4k context it helps a little bit with the 2k performance but its not good enough to get the model to be a 4k data set so well be considering it a 2k data set even though that i call it 4k because i used 4k long or up to 4k   some ugly questions and then we want to see that it gives what wed consider an an unacceptable answer before the fine tuning and gives hopefully a more acceptable answer after the dpo so ive set up uh some questions here um yeah unfortunately a lot of this kind of training involves ugly questions um which you will ask the model and when you run that base evaluation here its bluntly answering the question which you may not want um if you want to have a model that is going to be better behaved or aligned but this provides us with a baseline then we can then that we can then compare after the training and hopefully this data set will have the effect that we intend okay so were loading the data set that i just took you through its the dpo um h ou how it diverges but also it actually gives you a sense for the effect it has in the model because it does start to change the answer so right here with the 1 e minus 5 everything is very same the only difference is that when i ran the training i had it at a learning rate of 1 e minus 5 which is higher than 1 eus 6 by factor of 10 and indeed when i run that training and then i run the first sample here um you can see the answer is quite different now so when its asked about killing it starts to give an answer that um is a bit more restrained its um now asking that peoples um human rights and emotions be considered its giving a much more verbose answer trying to indicate more nuance around the topic i wouldnt say that its got the answer to gin for that so that we have logs so heres my weights and biases id and next up we will do the installation so install all of packages necessary move on to loading the model here as i said not loading at quantize because its a small model so there shouldnt be any issue with doing that and im going to prepare the model then for finetuning and stick with this selection here of target modules for that fine tuning initiate the tokenizer and once the tokenizer is done well move towards setting up evaluation printing some evaluation results and then loading the data set that were going to use for training now im going to make a little alteration here this time around im not going to trim the data set so im going to use the full eval data set whic iner is up and running and you can see were now 17 steps into 10about um its about an 8 hour run so fairly long even though this uh is quite a small model just a 1 billion parameter model so just gives you a feel for the amount of time if youre doing a full fine tune u rather if youre doing a lower fine tune on the model and here we can just click on the weights and biases and we should be able to pull up a copy of this run and you can see here training loss is progressing and im going to just go to overview and just put in here what the run is about one e minus 6 and well say one e but run tiny like this and thats it ill post up later how the run goes that folks is an overview of direct preference optimization it is a lot easier than doing reinforcement learning where you need to train a separate helper model still doing dpo direct preference op optimization its not an easy feat and you need to pay a lot of attention to having a data set thats comprehensive choosing some questions for evaluation that allow you to tell whether your training is progressing or not you also need to be comfortable with chat find tuning a model which is a form of supervised finetuning if you want to get started with something a little easier i recommend going back over the videos for embeddings unsupervised fine tuning and supervised fine tuning with respect to this video ill put all of the links on resources below and you can find out more about getting access to the scripts and data sets cheers 

#####################
Shot: mixtral8x7b
#####################
#####################
Shot: full-stack
#####################
#####################
Target: dpo
#####################
