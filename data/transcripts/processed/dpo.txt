this video is about a new approach to training language models specifically it allows you to move the probabilities of a language model away from bad answers and towards good answers direct preference optimization or dpo for short is a type of reinforcement learning its a more efficient type than has long been used by companies like open ai and meta in developing lama 2 and with this dpo technique it makes it much easier to find you models towards a chat format that is aligned in a similar way to those commercial models available for agenda were going to take a look at how normal training works standard finetuning or supervised finetuning as ive done in previous videos then ill contrast how direct preference optimization works and why its a bit different in terms of a technique and its useful in a different and complimentary way to standard finetuning then ill show you two dpo data sets the data sets need to be in a specific format if youre going to use the hugging face trainer that we will use today and then ill walk step by step through the training notebook ive developed for doing direct preference optimization by the end of this tutorial you should be able to take a model youve developed a language model ideally you do some supervised finetuning to get it into a chat format and this will allow you to do the final stage of aligning the model um another way to think of that is moving the model probabilities away from bad answers and towards good answers that will make sense as we get a bit deeper into this video lets start off with a description very high level of how standard training works standard language model training involves penalizing the model based on predicted what it predicts for the next token versus the actual next token for example if you feed in a phrase or part of a phrase that says the capital of ireland is and you ask the model to predict the next token if the model predicts dublin and then you wouldnt free penalize it because that answer is the next token here in this sentence but if it says something like cork then you would penalize the model and back back propagate through so fundamentally language models are statistical models that look at the frequency of information that is in the training data set if i train a language model with the capital of ireland is dublin the capital of ireland is dublin the capital of ireland is cork well if will see dublin more frequently and so that is more likely to be returned when the model predicts the next token if i want to bias the model towards saying dublin with a very high probability i can just keep feeding in the capital of ireland is dublin the captain ireland is dublin and eventually the relative frequency it is seen for dublin will be much greater than cork and so it is going to respond with the word dublin as the answer and that if you generalize it is how language models work work theyre statistical the more data you feed them a certain format the more it will bias bias the model towards that answer lets just look at a simple example here where we have a data set with doublin as the next token 10 times and cork as the next token one time if we train on this data set the language model is very often going to predict dublin as the next token because it has seen it so many more times in a data set now if we wanted to increase the probability even higher we would just have to add more and more and more data that says the capital violent is doubling so that we increase the probability of dublin being the answer now by contrast lets take a look at direct preference optimization here instead of adding more data to the model and penalizing it based on the next token prediction were going to do something a little bit more nuanced at a high level were going to drag the probability distribution away from one answer like cork and towards another answer dublin so as an example we have a prompt the capital of arland is and then were waiting on the next token and were going to set up two responses were going to set up a chosen response which is dublin and were going to set up a rejected response which is cork so remember in standard training the only data we have is on the actual token and we just compare actual to what the model predicts which could be anything it could be russ common up in new york it could be any value the only input data we have is the actual next token but here in direct preference optimization we have a pair of options we have a chosen and we have a rejected option and the way that we penalize the model now is not by comparing the actual to the predicted but rather we penalize the model so that the probability of dublin of the model were training we want that probability to be high relative to a ref reference model the reference model is just a copy of the model when we start this whole dpo step so imagine starting dpo we have a model we duplicate it so we now have a model and a reference model and during the training were going to penalize the model in such a way to incentivize the probability of the model being trained increasing for dublin relative to the probability that the reference model predicts for dublin and likewise were going to want to incentivize the model in a way such that the probability it predicts for cork should be reduced relative to the probability of the reference model so you can see how these pairs are being used and were trying to increase the probability of the chosen answer and decrease the probability of the rejected answer really high level what were doing here is imagine a training data set so youve come along with the model youve done some supervised fine tuning its been trained on a trillion tokens there are lots and lots of data points here within um the data set and in that data set of course because weve trained it on the internet there are some bad answers some answers that you know people have written things on the internet that arent correct and then there are some answers we consider good answers uh where maybe theres more of these but theres not enough more to statistically get the right answer all of the time so we have this data set body with bad answers and great answers and how direct preference optimization works is by using pairs of answers by feeding it some pairs we bias the model away from bad answers and we bias it towards great ideally great but certainly better answers so again to contrast this with standard training if we want to bias the model towards better answers with standard training the only way we can do that is by feeding more great answers so with standard training you have to keep feeding in more of these great answers the bad answers will still be there in the data set but over time these great answers will start to dominate as we train on more and more sets of the capital of ireland is dublin whereas by contrast in dpo were penalizing the model for a bad answer and were incentivizing it for a good answer now im giving a very simple example here just with the capital of ireland but as it turns out when we look at these pairs on good of good and bad answers there are certain statistical as attributes about good and what we consider bad answers and those statistical attributes are then captured by the model and that helps us to generally drag uh drag the model away from worse portions of its training data set and towards better portions of its training data set and so when we go back to our graph here showing the frequencies you can imagine that in a data set of the worldwide web you again may have a lot of bad data and you have hopefully even more good data it might be hard to get rid of some of the bad answers it might be hard to overcome them rather by just feeding more and more data into the model until the right answers kind of win out and so direct pro preference optimization gives us a different approach where rather than just adding in more examples with dublin we specifically penalized the examples with cork so its like we can almost remove the cork example or at least move the model away from the cork example and moving towards the good example ill move now to talk about different data sets that are available for doing direct preference optimization there are two ones ill talk about one is ultra chat and the other one is anthropics helpful and harmless the way to think about ultra chat is a series of conversations that have been had in all sorts of models and there are conversations from falcon models from llama models from mpt models and some of these best conversations which we would consider good data um and which also consider uh what also include bad responses we take those and we form pairs and then we use that to train our model so this is a synthetic form of data it works well if you take very strong models largely the data comes from larger models like 40b or 70b lama and then we use that to train smaller models in fact thats how the zephier model which is a fine tb of mistr works so um the authors of zephier which is a hugging face team they took the mistral model and then they used an ultra chat data set which is generated by more powerful models and this provides um a strong compendium of what we can call great answers versus bad answers and using that data set it was possible to finetune mistral using um dpo in order to get an improved model called zeper one of the issues with the ultra chat data set is that because it uses other language models that are not completely permissively licensed that limits um the use of zere to noncommercial use furthermore in ranking which answers were great and which were bad gpt 4 was used and the terms of the gpt 4 li dont allow you to use it commercially and so zier is a great demonstration of how you can synthetically produce strong data with large models that provides you with a good versus bad data set as graded by gp4 in this case and then taking that data set you can train these weaker models and basically point the weaker models away from their bad answers and towards their good answers ill put up a link below to a data set you can use if you want to use the ultrat set the other data set thats available is helpful and harmless from anthropic its 160rows about 40type and 120sets been around for a while and its been used to train lama 2 um its probably being used to train a lot of models and it does something a little different than just pointing uh the model from bad answers to good answers its also trying to achieve a level of safety and avoid the model spitting out uh perhaps s harmful dangerous type results just as an example one uh instruction set that you might see a type of instruction set you might see in the harmless part of the data set is as follows the prompt might be how do i do something dangerous and the chosen answer which is what we want to incentivize the model towards is i dont know thats an interesting question so its kind of harmless certainly not helpful but its kind of harmless and the idea is to pull the model away from giving out dangerous answers and then the rejected answer might be something like go and blow xyz um and this clearly is some kind of a dangerous answer and this is what you would be trying to bias the model away from this is the data set that were going to use today for demonstrating dpo im going to demonstrate it on tiny lama which is a 1 billion model its a fairly small model which allows me to do the dpo training process fairly quickly i think in about 45 minutes i was able to train on 16the overall data set and so the time for doing training on dpo its roughly about twice the time that you need for doing supervised finetuning because you are running inference forward through two models the reference model which is duplicate of the original model and the model that youre training so you have twice the amount of forward passes and although youve got the same amount of backward passes probably is a rule of th your training in dpo is going to take twice the length as doing finetuning another caveat before i move to show you the notebook is that when you train a model to be harmless like this youre training the model when it gets a question to say something thats kind of benign and useless and while that can be helpful because commercially you dont want your model to be outputting harmless harmful or hateful uh content it also can get the way of the model statistically providing useful answers and it can affect the quality of useful answers that are generated and this has been a problem to some degree with the lama 2 chat model particularly when using the system prompt the model is so uh its so finetuned using reinforcement learning that it doesnt answer a lot of questions or it answers them in a kind of harmless uh but unhelpful way so thats a caveat to be aware of when youre choosing your data set just one last comment ive remembered which is that dpo its a very direct way of in achieving whats called reinforcement learning the previous approach uh the twostep approach to doing this step of training was to first get a totally new model and train it to recognize the difference between good and bad answers so you would take the same data set you would train a brand new model just to get accustomed to what we think is a chen answer and whats a rejected answer and then using that model you would run a loop on the model you actually want to train whereby the model you want to train would generate some answers and you would then run that and get a reward that says yes this should be a chosen answer or this should be rejected and then back propagate so you had the original model and then this reward model um and that required a lot of steps and made it particularly difficult uh except for the larger companies with dpo though this process is just done directly by having both the reference model and by having the model your training forward passing through both of them lets say for the chosen answer and then penalizing the probability of the chosen answer in a way that it will be incentivized to be increased if its the chosen one versus the reference model or it will be incentivized to decrease the probability of the rejected one versus the reference model let me quickly go through the materials we need before we do the dpo training the first thing we need is a data set and it needs to be formatted correctly with the prompt then the chosen answer and then the rejected answer were going to use anthropics helpful and harmless reinforcement learning data set its presented with only chosen and rejected rows uh rer columns so lets just take a look at the data you can see theres a chosen uh prompt here and the question is if youre going to steal from a convenience store and the assistant gives a harmless answer which is i really couldnt say and then the rejected answer is a more um intense or bad answer same question same prompt but the assistant says uh its good to consider the difference in human traffic at night and different reasoning there that you want to reject as i mentioned this isnt the right format so i have reformatted it here into a data set um you can purchase access to this if you like youll find the link on the repo youll see theres the chosen the rejected and the the prompt columns just to go through here we have a prompt if you were to steal from a convenience store and then you have the chosen and the rejected note that i formatted it for lama um with the inst at the start and at the end and ive also put in an end of sequence token i dont really think this is critical because ultimately were just comparing the probabilities of uh of the model being trained with the reference model and were adjusting where there is a difference or rather to create a differ difference to increase the likelihood of the chosen response and reduce the likelihood of the rejected response so im not sure the end of sequence token is required but just for consistency i think its good to format it like this the next thing were going to need is a reference model i have a reference model here which is the tiny lama model i said i would be training on this is a model that i have trained with supervised finetuning using the open assist data set its available here um for you to download if you like and as i said its a chat finetuned model tiny lama is only trained on a 2k context length i did the finetuning using 4k context it helps a little bit with the 2k performance but its not good enough to get the model to be a 4k data set so well be considering it a 2k data set even though that i call it 4k because i used 4k long or up to 4k long uh data set which is open assist and with that i think were ready to move on and do some finetuning in the notebook you can get access to the notebook by purchasing it through the link below or you can purchase access to the full advanced finetuning repository which now has not just dpo but scripts for embeddings function calling training long context uh quantization supervised finetuning and unsupervised fine tuning quite a lot of scripts in that advanced finetuning repo and quite a lot of members of it right now all right lets get started and here we are with the direct preference optimiz ation notebook im going to take you through it step by step so right up the top we are going to connect with hugging face this will allow us to push models and also if youre accessing a or finetuning a gated model its necessary to do the login lately ive always been also logging in with weights and biases it allows us to view the data and it saves the data for the future so you have it nicely organized for runs this is really handy because i use run pad oftenly often and it saves the data even if i shut down the pod so ive connected with weights and biases optionally if you have the model downloaded to google drive say youre working in google collab you can connect google drive next well do the installation note that im installing flash detention this is going to be the v2 and that will accelerate the training process it speeds up doing the attention part of the forward pass uh i additionally now typically run this command here to get the transformer transformers environment information this is useful if youre ever troubleshooting and you need to post an issue on github it tells you all about the platform youre running so well move on now to loading the model the model ill load is the tiny llama model in fact let me just increase the size of my font here something like this i am not going to load a quantise because its only a small model um so ive commented out the quantization im not using any rope scaling ive set the dei device map to auto which means that the layers will be put onto the gpu im running with an a6000 here its got 45 48 gab of ram um and that easily fits in the total model which is about 2 gbt in size um in fact if its run in bf 16 yeah its about 2 gb in size because it has about billion parameters in tiny lama you can see here that im using flash attention and this will reduce the memory requirements now you can load a me reference model by loading an exact copy but actually the trainer will handle that for us so im not going to load a reference model ill just load this single model here next up ill load the tokenizer i can run a quick uh test generation just to see that the model is working correctly which it is um note that im only going to train lowa parameters here so ill train a lura adapter im not going to train any non lowa parameters um typically i would list out all of the parameters in the model and turn on training for some of those in non lura format but i dont do that here its necessary if you want to train for longer context thats not what were doing here so im just going to train the lowa parameters which im going to apply in the next step so here im enabling gradient checkpointing and i have a function to print the trainable parameters im printing out the model just to show you exactly what ill be training within the attention ill train the q kv and o and ill also train in the linear lays the gate projection up down im not going to tr train the input lay armm which would not be done through laura im not going to train the post detention l armm im not going to train the embeddings either although training those will be important if you want to train for longer contexts which is not what were doing here so indeed you can see here the modules that ive chosen to train and im applying the adapter now to the model so that we are training the laura config this results in us training a lot less parameters than we normally would if we were doing a full fine tune now laura fine tunes generally perform just as well so hopefully well see thats the case here as well next up uh well set up this the tokenizer and pass pd in so ive just loaded the tokenizer here and im going to add a padding token you can use the unknown token as padding but since the supervised finetuning base model im using has been trained with pad token like this um im going to keep consistent here and use that same pad token okay so well continue on here and you can see that the special tokens are the beginning of sequence the end of sequence the unknown token and the pad token next up we need to set up some evaluation so we can evaluate the model before we do the dpo and afterwards and its important here you set some questions that are relevant to the data set that you choose ideally they should provide you some metric of how the model is performing were trying to fine tune here to have a model that is more harmless so were going to have to ask some ugly questions and then we want to see that it gives what wed consider an an unacceptable answer before the fine tuning and gives hopefully a more acceptable answer after the dpo so ive set up uh some questions here um yeah unfortunately a lot of this kind of training involves ugly questions um which you will ask the model and when you run that base evaluation here its bluntly answering the question which you may not want um if you want to have a model that is going to be better behaved or aligned but this provides us with a baseline then we can then that we can then compare after the training and hopefully this data set will have the effect that we intend okay so were loading the data set that i just took you through its the dpo um helpful and harmless reinforcement learning data set from anthropic so thats loaded and here you can see there is a prompt talking about dinosaurs so this is a more harmless question um but it ends with the user accusing the machine of not being able to read and the harmless answer is you can read question mark and rejected answer is a more aggressive retort which is theres a lot of stuff humans dont know okay so well move on and ive just got a test here where i tokenize check the token ids you can see that a special token thats the beginning of sequence token is added at the start um so everything looks correct here now were going to move on to the trainer im going to use an evaluation set and a training set and the evaluation set is the test split its quite a large split i think its got over 10running more quickly im going to reduce that so well only run with 960 of the test data set rows im going to run for 01 epox on the train data set the train data set is 160k so itll be out a 16on the training side and on the test side its about 960 rows okay setting some parameters here uh the context length will be 2k which is the tiny lama context length im setting the batch size to eight this does fit on an a6000 you can fit eight batches of 2fit a bit more you probably could fit 12 i think and usually the way i set gradient accumulation is to have the batch size times the gradient accumulation be about 32 that means that basically youre doing 32 forward passes um before accumulating the updates and then passing them backwards through the model all right so moving on here to the training arguments mostly we have already specified them im going to do evaluation steps every quarter of the full run so there should be three in uh intermediate evaluation steps the same with save steps this will save the adapters the lower adapters in case the run crashes ill at least have the adapters saved im setting uh a scheduler here type to constant so that means ive got constant learning rate throughout the run sometimes people will decrease that over time um i think with a small amount of data you can probably just do a constant um linear scheduler or sorry a con constant scheduler but you might consider linear or cosign if youre doing a longer run with a large amount of data now one of the most important parameters here is the learning rate typically this might be 1 e minus 5 for doing lower of fine tunes but with dpo its found that you need to have an even lower learning rate its recommended maybe 5e minus 7 ive got it slightly more here which is 1 eus 6 and ill actually show you an example of where it diverges when i try doing 1 e minus 5 okay so with that um the trainer can be called you can see im passing in the model im not passing in a reference model because the trainer will create a copy uh automatically as a reference im setting the beta of 01 if the beta was zero then it would basically ignore the reference model if the beta is 05 i think its going to very strongly consider the model so um or sorry it could be more than 05 but the higher beta is the more that the reference model is considered ie um you can think of the model youre training being more tethered to the starting point um so its recommended to have it between 01 and 05 i havent played around a lot with this but im leaving it at1 then im passing in the training set and the test set and then the tokenizer and then im starting to train now this model has already been trained so ive gone all the way through a th000 steps um this training has been completed and im going to walk you through some of the results its going to be easiest probably to look in weights and biases because we can see the full graphs over there and once were done with the training uh we can plot so we can show the training loss and the validation loss really you dont see a whole lot of change in the model here uh the learning rate is very slow so with this uh relatively small small amount of data and such a low training uh learning rate theres very little change in the model over the um over the over the training period and indeed when i look here at the base model evaluation after the training unfortunately in this case the data hasnt been enough to give me much change or any change in the answer that im getting so still the model is not giving um a more restrained response to the more dangerous questions so we havent achieved the goal here with dpo meaning that we would need to train perhaps for more uh more of an epoch ive only trained for 01 epochs its possible that simply the kind of nature of this question im asking here hasnt being covered in the training data set so its not being affected statistically by any of the samples that weve run through in dpo so running for a full epoch would be one way to start with this and maybe playing as well with having a little bit higher of learning rate so im going to show you the example i have with a little higher learning rate where i have 1 e minus 5 ill show you how it diverges but also it actually gives you a sense for the effect it has in the model because it does start to change the answer so right here with the 1 e minus 5 everything is very same the only difference is that when i ran the training i had it at a learning rate of 1 e minus 5 which is higher than 1 eus 6 by factor of 10 and indeed when i run that training and then i run the first sample here um you can see the answer is quite different now so when its asked about killing it starts to give an answer that um is a bit more restrained its um now asking that peoples um human rights and emotions be considered its giving a much more verbose answer trying to indicate more nuance around the topic i wouldnt say that its got the answer to as good of a level as we would hope for from a production level product but still you can clearly see that the answer is no longer just directly uh providing dangerous recommendation on this question but rather giving a little more nuance lets move over and um take a quick look at the results in fact even just looking at the training and the eval loss which should be kind of slowly falling you can see in this case the train in loss starts to diverge um its bouncing around pretty sharply because my learning rate is so high and the eval loss i wouldnt say its getting terrible but its its going up a little bit but this kind of instability here indicates that probably i have my learning rate a little bit too high so what well take a quick look at now uh to finish off is the weights and biases training when youve connected up weights and biases itll automatically generate all of these plots you can see here what the gpu was doing during this training session it was about 50 minutes um actually i trained on an a100 here when i was running with 1 to the e minus 6 and right up the top you can see some evaluation results its showing four plots here that ill explain a little bit um the two i want to focus on are uh the margins plot and the accuracies plot the accuracy plot is probably the easiest to understand i know its small font here let me see if if i could increase actually just this yeah i think that helps so the accuracies tells us given given a chosen response and giving a rejected response whats the likelihood that the trained model will pick the chosen rather than the rejected so ideally you would like the model to have a high probability more than 50 of choosing the chosen mo response and have a low probability of choosing the rejected one and so you can see here throughout the training theres some oscillation but at the end of the training in fact not at the end but at the 750th step you can see that um its got a slightly higher than 50 chance of picking the chosen rather than the rejected so indeed in this case the model has been biased towards the chosen response but you can see the bias is not very strong because its 5135 so its only a little bit more than half and so you would need to take the model quite a bit further if um you wanted to have the bias um definitely move towards the chosen response the next graph to look at is the margins graph this is the difference in the reward for the chosen minus the reward for the rejected so ideally we want a positive margin here you can see that actually the margin is negative and this is an average across um the whole evaluation so there were about 960 rads in that evaluation and um i think thats why its possible to have a negative margin when youre averaging across all of those according to value whereas the accuracies are averaging across according to the number that favor the chosen over the rejected so you can see that in any case were not seeing a very strong bias of the model unfortunately we would like to see a positive margin that shows us a clear bias in a value sense and wed like to see a clear bias as well in terms of the number of responses that are chosen over rejected and just for two more quick graphs you can always keep an eye on the training and the eval graphs the training loss should be going down over time as there should be similarity between all of the different samples were trying to train on so this should be dropping very slowly you can see it looks pretty constant over this range of training and then eval um this number here should also be falling you can see here that its gone from around 6929 to 6935 so theres really not any change here in the eval loss it looks like some change cu the y ais is very um compressed or very zoomed in on in fact were just not seeing a whole lot of training within the short number of epoch before i go im just going to show you exactly how i set things up on runp pod um what ill do here is use an a6000 so if i go to secure cloud i should see a list of all of the servers that are available like this and this i think is probably one of the best values per unit of vram its got 48 vram for 79 cents per hour um ill before i click deploy using a pytorch instance im just going to make sure i have plenty of memory here um 50 gb is way more than enough because the model is going to be about 2 gab and ill click continue and then ill deploy so this is now going to deploy a p p torch instance and you can see i have an exited instance of an a100 that i was working on earlier and once this is booted up ill be able to connect using a jupiter notebook once the notebook uh is ready once the pod is ready ill click connect and ill click connect to jupiter ive just uploaded a dp iynb notebook and ill open it here in the screen and im going to go through the steps that i discussed earlier starting off by and logging in with my hugging face id so here we have the hugging face id then doing the installation of weights and biases and using my login for that so that we have logs so heres my weights and biases id and next up we will do the installation so install all of packages necessary move on to loading the model here as i said not loading at quantize because its a small model so there shouldnt be any issue with doing that and im going to prepare the model then for finetuning and stick with this selection here of target modules for that fine tuning initiate the tokenizer and once the tokenizer is done well move towards setting up evaluation printing some evaluation results and then loading the data set that were going to use for training now im going to make a little alteration here this time around im not going to trim the data set so im going to use the full eval data set which is about 10slower training run because im going to use the full eval data set also here um im going to run for a full epoch so that should be um an improvement on what was previously done now lets just just check here to make sure the data set is imported it looks like i have an error for loading up here and unfortunately i forgot to run this one cell here which was necessary so now that ive run cached here ill be able to run through all of these again by the lower adapter set up the tokenizer set up the evaluation lo the data set but this time not truncate it as i previously had done so thats why ive commented out here where ive reduced the range of the ev val data set and then increased to one epoch for the training so lets see and we have a learning rate schedular type of a constant im actually going to move back to the original um recommended learning rate well you know i might leave 1 e minus 6 but instead of using constant i think i will use linear here so the learning rate will decrease as we move through all of the epoch and you can see ive set up the results folder here for one epoch and ill go ahead and get started now to train the model it should set up awaits and biases run you can see here its got 10steps to go through instead of 1its going to be quite a bit longer as a training and im going to install my plot lib so i can plot the results at the end ill do a quick evaluation this here i can remove can remove all of these and i will push the model to hugging face so that i have a copy of it so ill just go ahead and push that adapter and additionally i dont need to upload any other trainable parameters because theyre all included in the laura adapter so i should be free to just merge and unload the model and then i should be free to push the tokenizer to hugging face hub and push the model to hugging face hub um what i might do is push tokenizer do model which is a file in the original repo that file is needed to do gptq and also ggf quantizations so it can be handy to just push that as well so ill scroll up here now and take a look at the training which should be underway and here the trainer is up and running and you can see were now 17 steps into 10about um its about an 8 hour run so fairly long even though this uh is quite a small model just a 1 billion parameter model so just gives you a feel for the amount of time if youre doing a full fine tune u rather if youre doing a lower fine tune on the model and here we can just click on the weights and biases and we should be able to pull up a copy of this run and you can see here training loss is progressing and im going to just go to overview and just put in here what the run is about one e minus 6 and well say one e but run tiny like this and thats it ill post up later how the run goes that folks is an overview of direct preference optimization it is a lot easier than doing reinforcement learning where you need to train a separate helper model still doing dpo direct preference op optimization its not an easy feat and you need to pay a lot of attention to having a data set thats comprehensive choosing some questions for evaluation that allow you to tell whether your training is progressing or not you also need to be comfortable with chat find tuning a model which is a form of supervised finetuning if you want to get started with something a little easier i recommend going back over the videos for embeddings unsupervised fine tuning and supervised fine tuning with respect to this video ill put all of the links on resources below and you can find out more about getting access to the scripts and data sets cheers 