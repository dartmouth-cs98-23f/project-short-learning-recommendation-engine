second,duration,transcript
1.439,4.641,this video is about a new approach to
3.36,4.96,training language models specifically it
6.08,4.32,allows you to move the probabilities of
8.32,4.479,a language model away from Bad answers
10.4,5.439,and towards good answers direct
12.799,5.921,preference optimization or DPO for short
15.839,5.001,is a type of reinforcement learning it's
18.72,4.96,a more efficient type than has long been
20.84,5.96,used by companies like open Ai and meta
23.68,5.72,in developing Lama 2 and with this DPO
26.8,5.56,technique it makes it much easier to
29.4,5.08,find you models towards a chat format
32.36,5.16,that is aligned in a similar way to
34.48,5.44,those commercial models available for
37.52,4.16,agenda we're going to take a look at how
39.92,4.4,normal training Works standard
41.68,5.24,fine-tuning or supervised fine-tuning as
44.32,4.399,I've done in previous videos then I'll
46.92,4.76,contrast how direct preference
48.719,5.081,optimization works and why it's a bit
51.68,3.719,different in terms of a technique and
53.8,3.399,it's useful in a different and
55.399,4.601,complimentary way to standard
57.199,5.0,fine-tuning then I'll show you two DPO
60.0,3.8,data sets the data sets need to be in a
62.199,3.121,specific format if you're going to use
63.8,3.679,the hugging face trainer that we will
65.32,4.159,use today and then I'll walk step by
67.479,4.161,step through the training notebook I've
69.479,4.881,developed for doing direct preference
71.64,5.119,optimization by the end of this tutorial
74.36,4.48,you should be able to take a model
76.759,3.961,you've developed a language model
78.84,4.84,ideally you do some supervised
80.72,5.16,fine-tuning to get it into a chat format
83.68,4.84,and this will allow you to do the final
85.88,4.76,stage of aligning the model um another
88.52,4.16,way to think of that is moving the model
90.64,3.88,probabilities away from Bad answers and
92.68,4.04,towards good answers that will make
94.52,5.44,sense as we get a bit deeper into this
96.72,5.079,video let's start off with a description
99.96,4.28,very high level of how standard training
101.799,4.64,Works standard language model training
104.24,4.0,involves penalizing the model based on
106.439,4.121,predicted what it predicts for the next
108.24,5.0,token versus the actual next token for
110.56,4.12,example if you feed in a phrase or part
113.24,4.239,of a phrase that says the capital of
114.68,5.96,Ireland is and you ask the model to
117.479,5.121,predict the next token if the model
120.64,4.079,predicts Dublin and then you wouldn't
122.6,4.28,free penalize it because that answer is
124.719,4.481,the next token here in this sentence but
126.88,4.52,if it says something like cork then you
129.2,3.84,would penalize the model and back back
131.4,4.4,propagate
133.04,5.36,through so fundamentally language models
135.8,4.719,are statistical models that look at the
138.4,4.96,frequency of information that is in the
140.519,4.601,training data set if I train a language
143.36,3.879,model with the capital of Ireland is
145.12,4.96,Dublin the capital of Ireland is Dublin
147.239,6.241,the capital of Ireland is cork well if
150.08,5.48,will see Dublin more frequently and so
153.48,4.88,that is more likely to be returned when
155.56,5.24,the model predicts the next token if I
158.36,4.959,want to bias the model towards saying
160.8,4.32,Dublin with a very high probability I
163.319,3.28,can just keep feeding in the capital of
165.12,3.44,Ireland is Dublin the captain Ireland is
166.599,4.0,Dublin and eventually the relative
168.56,4.08,frequency it is seen for Dublin will be
170.599,4.36,much greater than Cork and so it is
172.64,5.72,going to respond with the word Dublin as
174.959,5.321,the answer and that if you generalize it
178.36,4.159,is how language models work work they're
180.28,4.64,statistical the more data you feed them
182.519,5.28,a certain format the more it will bias
184.92,5.12,bias the model towards that
187.799,5.241,answer let's just look at a simple
190.04,5.759,example here where we have a data set
193.04,5.64,with doublin as the next token 10 times
195.799,4.841,and cork as the next token one time if
198.68,4.32,we train on this data set the language
200.64,4.8,model is very often going to predict
203.0,5.0,Dublin as the next token because it has
205.44,4.32,seen it so many more times in a data set
208.0,3.72,now if we wanted to increase the
209.76,4.16,probability even higher we would just
211.72,3.599,have to add more and more and more data
213.92,3.399,that says the capital violent is
215.319,5.521,doubling so that we increase the
217.319,6.761,probability of Dublin being the
220.84,5.039,answer now by contrast let's take a look
224.08,4.719,at direct preference
225.879,5.401,optimization here instead of adding more
228.799,4.52,data to the model and penalizing it
231.28,3.679,based on the next token prediction we're
233.319,3.92,going to do something a little bit more
234.959,4.48,nuanced at a high level we're going to
237.239,5.121,drag the probability distribution away
239.439,6.52,from one answer like Cork and towards
242.36,5.879,another answer Dublin so as an example
245.959,4.521,we have a prompt the capital of Arland
248.239,4.681,is and then we're waiting on the next
250.48,4.92,token and we're going to set up two
252.92,4.879,responses we're going to set up a chosen
255.4,4.6,response which is Dublin and we're going
257.799,4.881,to set up a rejected response which is
260.0,4.639,cork so remember in standard training
262.68,3.92,the only data we have is on the actual
264.639,3.161,token and we just compare actual to what
266.6,3.28,the model predicts which could be
267.8,4.16,anything it could be Russ common up in
269.88,4.36,New York it could be any value the only
271.96,4.079,input data we have is the actual next
274.24,4.64,token but here in direct preference
276.039,7.16,optimization we have a pair of options
278.88,6.84,we have a Chosen and we have a rejected
283.199,4.641,option and the way that we penalize the
285.72,5.0,model now is not by comparing the actual
287.84,5.199,to the predicted but rather we penalize
290.72,5.199,the model so that the probability of
293.039,5.16,Dublin of the model we're training we
295.919,5.161,want that probability to be high
298.199,6.201,relative to a ref reference
301.08,5.32,model the reference model is just a copy
304.4,5.84,of the model when we start this whole
306.4,5.96,DPO step so imagine starting DPO we have
310.24,4.72,a model we duplicate it so we now have a
312.36,4.679,model and a reference model and during
314.96,4.519,the training we're going to penalize the
317.039,4.921,model in such a way to incentivize the
319.479,5.681,probability of the model being trained
321.96,5.079,increasing for Dublin relative to the
325.16,5.0,probability that the reference model
327.039,5.041,predicts for Dublin and likewise we're
330.16,4.0,going to want to incentivize the model
332.08,5.679,in a way such that the probability it
334.16,5.52,predicts for cork should be reduced
337.759,3.041,relative to the probability of the
339.68,4.04,reference
340.8,5.08,model so you can see how these pairs are
343.72,4.319,being used and we're trying to increase
345.88,5.4,the probability of the chosen answer and
348.039,5.361,decrease the probability of the rejected
351.28,4.52,answer really high level what we're
353.4,4.48,doing here is imagine a training data
355.8,4.16,set so you've come along with the model
357.88,4.72,you've done some supervised fine tuning
359.96,5.079,it's been trained on a trillion tokens
362.6,6.2,there are lots and lots of data points
365.039,5.16,here within um the data set and in that
368.8,3.56,data set of course because we've trained
370.199,4.801,it on the internet there are some bad
372.36,3.8,answers some answers that you know
375.0,3.16,people have written things on the
376.16,3.599,internet that aren't correct and then
378.16,4.8,there are some answers we consider good
379.759,5.56,answers uh where maybe there's more of
382.96,4.92,these but there's not enough more to
385.319,5.32,statistically get the right answer all
387.88,5.08,of the time so we have this data set
390.639,4.521,body with bad answers and great answers
392.96,4.519,and how direct preference optimization
395.16,4.64,works is by using pairs of Answers by
397.479,4.961,feeding it some pairs we bias the model
399.8,4.72,away from Bad answers and we bias it
402.44,5.28,towards great ideally great but
404.52,5.84,certainly better answers so again to
407.72,4.52,contrast this with standard training if
410.36,3.959,we want to bias the model towards better
412.24,3.88,answers with standard training the only
414.319,3.801,way we can do that is by feeding more
416.12,3.88,great answers so with standard training
418.12,3.96,you have to keep feeding in more of
420.0,4.12,these great answers the bad answers will
422.08,4.239,still be there in the data set but over
424.12,4.6,time these great answers will start to
426.319,5.081,dominate as we train on more and more
428.72,5.44,sets of the capital of Ireland is Dublin
431.4,4.96,whereas by contrast in DPO we're
434.16,5.2,penalizing the model for a bad answer
436.36,5.44,and we're incentivizing it for a good
439.36,4.959,answer now I'm giving a very simple
441.8,5.64,example here just with the capital of
444.319,5.841,Ireland but as it turns out when we look
447.44,5.56,at these pairs on good of good and bad
450.16,4.84,answers there are certain statistical as
453.0,4.52,attributes about good and what we
455.0,5.24,consider bad answers and those
457.52,4.72,statistical attributes are then captured
460.24,4.76,by the model and that helps us to
462.24,5.2,generally drag uh drag the model away
465.0,5.0,from worse portions of its training data
467.44,5.52,set and towards better portions of its
470.0,6.599,training data set and so when we go back
472.96,7.199,to our graph here showing the
476.599,6.681,frequencies you can imagine that in a
480.159,6.0,data set of the worldwide web you again
483.28,5.479,may have a lot of bad data and you have
486.159,4.6,hopefully even more good data it might
488.759,4.44,be hard to get rid of some of the bad
490.759,4.241,answers it might be hard to overcome
493.199,4.321,them rather by just feeding more and
495.0,5.479,more data into the model until the right
497.52,4.88,answers kind of win out and so direct
500.479,3.921,Pro preference optimization gives us a
502.4,4.519,different approach where rather than
504.4,4.359,just adding in more examples with Dublin
506.919,4.601,we specifically penalized the examples
508.759,5.241,with cork so it's like we can almost
511.52,4.72,remove the cork example or at least move
514.0,5.32,the model away from the cork example and
516.24,5.039,moving towards the good example I'll
519.32,4.36,move now to talk about different data
521.279,4.081,sets that are available for doing direct
523.68,3.56,preference
525.36,4.36,optimization there are two ones I'll
527.24,5.599,talk about one is ultra chat and the
529.72,5.4,other one is anthropics helpful and
532.839,5.081,harmless the way to think about Ultra
535.12,6.12,chat is a series of conversations that
537.92,5.0,have been had in all sorts of models and
541.24,5.36,there are conversations from Falcon
542.92,6.24,models from llama models from MPT models
546.6,5.479,and some of these best conversations
549.16,5.799,which we would consider good data um and
552.079,3.841,which also consider uh what also include
554.959,4.161,bad
555.92,6.4,responses we take those and we form
559.12,5.68,Pairs and then we use that to train our
562.32,5.4,model so this is a synthetic form of
564.8,4.599,data it works well if you take very
567.72,5.559,strong models largely the data comes
569.399,5.481,from larger models like 40b or 70b Lama
573.279,3.601,and then we use that to train smaller
574.88,4.92,models in fact that's how the zephier
576.88,6.48,model which is a fine TB of mistr works
579.8,5.88,so um the authors of zephier which is a
583.36,5.12,hugging face team they took the Mistral
585.68,4.76,model and then they used an ultra chat
588.48,5.4,data set which is generated by more
590.44,5.36,powerful models and this provides um a
593.88,4.68,strong compendium of what we can call
595.8,4.96,Great answers versus bad answers and
598.56,6.399,using that data set it was possible to
600.76,6.6,fine-tune Mistral using um DPO in order
604.959,4.721,to get an improved model called
607.36,4.56,zeper one of the issues with the ultra
609.68,3.719,chat data set is that because it uses
611.92,4.039,other language models that are not
613.399,5.12,completely permissively licensed that
615.959,6.0,limits um the use of zere to
618.519,5.601,non-commercial use furthermore in
621.959,5.681,ranking which answers were great and
624.12,6.44,which were bad GPT 4 was used and the
627.64,6.199,terms of the GPT 4 Li don't allow you to
630.56,5.12,use it commercially and so zier is a
633.839,3.721,great demonstration of how you can
635.68,4.52,synthetically produce strong data with
637.56,5.519,large models that provides you with a
640.2,5.28,good versus bad data set as graded by
643.079,3.88,gp4 in this case and then taking that
645.48,3.72,data set you can train these weaker
646.959,4.161,models and basically point the weaker
649.2,4.639,models away from their bad answers and
651.12,5.08,towards their good answers I'll put up a
653.839,5.8,link below to a data set you can use if
656.2,6.0,you want to use the ultrat set the other
659.639,5.521,data set that's available is helpful and
662.2,5.8,"harmless from anthropic it's 160,000"
665.16,5.96,"rows about 40,000 of those are harmless"
668.0,4.44,"type and 120,000 are helpful this data"
671.12,3.88,set's been around for a while and it's
672.44,4.16,been used to train Lama 2 um it's
675.0,4.079,probably being used to train a lot of
676.6,4.6,models and it does something a little
679.079,4.801,different than just pointing uh the
681.2,4.8,model from Bad answers to good answers
683.88,4.68,it's also trying to achieve a level of
686.0,6.12,safety and avoid the model spitting out
688.56,7.12,uh perhaps s harmful dangerous type
692.12,6.36,results just as an example one uh
695.68,4.399,instruction set that you might see a
698.48,3.56,type of instruction set you might see in
700.079,4.041,the harmless part of the data set is as
702.04,4.64,follows the prompt might be how do I do
704.12,3.92,something dangerous and the chosen
706.68,3.719,answer which is what we want to
708.04,5.64,incentivize the model towards is I don't
710.399,5.68,know that's an interesting question so
713.68,4.52,it's kind of harmless certainly not
716.079,3.721,helpful but it's kind of harmless and
718.2,3.72,the idea is to pull the model away from
719.8,4.08,giving out dangerous answers and then
721.92,4.12,the rejected answer might be something
723.88,5.079,like go and blow
726.04,5.12,XYZ um and this clearly is some kind of
728.959,4.401,a dangerous answer and this is what you
731.16,4.679,would be trying to bias the model away
733.36,5.76,from this is the data set that we're
735.839,5.201,going to use today for demonstrating DPO
739.12,4.04,I'm going to demonstrate it on Tiny Lama
741.04,4.039,which is a 1 billion model it's a fairly
743.16,4.119,small model which allows me to do the
745.079,4.32,DPO training process fairly quickly I
747.279,5.601,think in about 45 minutes I was able to
749.399,7.281,"train on 16,000 samples which is 10% of"
752.88,6.759,the overall data set and so the time for
756.68,4.56,doing training on DPO it's roughly about
759.639,4.841,twice the time that you need for doing
761.24,5.719,supervised finetuning because you are
764.48,4.359,running inference forward through two
766.959,4.32,models the reference model which is
768.839,4.24,duplicate of the original model and the
771.279,4.321,model that you're training so you have
773.079,3.921,twice the amount of forward passes and
775.6,3.56,although you've got the same amount of
777.0,3.88,backward passes probably is a rule of th
779.16,4.4,your training in DPO is going to take
780.88,5.24,twice the length as doing
783.56,5.24,fine-tuning another caveat before I move
786.12,4.839,to show you the notebook is that when
788.8,5.279,you train a model to be harmless like
790.959,5.961,this you're training the model when it
794.079,5.081,gets a question to say something that's
796.92,4.599,kind of benign and useless and while
799.16,3.76,that can be helpful because commercially
801.519,4.841,you don't want your model to be
802.92,6.32,outputting harmless harmful or hateful
806.36,5.839,uh content it also can get the way of
809.24,5.8,the model statistically providing useful
812.199,5.521,answers and it can affect the quality of
815.04,4.2,useful answers that are generated and
817.72,3.6,this has been a problem to some degree
819.24,4.76,with the Lama 2 chat model particularly
821.32,6.28,when using the system prompt the model
824.0,6.68,is so uh it's so fine-tuned using
827.6,5.039,reinforcement learning that it doesn't
830.68,4.44,answer a lot of questions or it answers
832.639,4.841,them in a kind of harmless uh but
835.12,4.2,unhelpful way so that's a caveat to be
837.48,4.12,aware of when you're choosing your data
839.32,5.879,set just one last comment I've
841.6,5.88,remembered which is that DPO it's a very
845.199,4.361,direct way of in achieving what's called
847.48,4.52,reinforcement learning the previous
849.56,6.199,approach uh the two-step approach to
852.0,6.399,doing this step of training was to first
855.759,4.601,get a totally new model and train it to
858.399,4.321,recognize the difference between good
860.36,4.32,and bad answers so you would take the
862.72,4.88,same data set you would train a brand
864.68,5.159,new model just to get accustomed to what
867.6,4.76,we think is a Chen answer and what's a
869.839,4.68,rejected answer and then using that
872.36,4.44,model you would run a loop on the model
874.519,3.841,you actually want to train whereby the
876.8,4.719,model you want to train would generate
878.36,6.36,some answers and you would then run that
881.519,4.601,and get a reward that says yes this
884.72,4.32,should be a chosen answer or this should
886.12,5.04,be rejected and then back propagate so
889.04,4.68,you had the original model and then this
891.16,4.359,reward model um and that required a lot
893.72,3.559,of steps and made it particularly
895.519,4.601,difficult uh except for the larger
897.279,5.321,companies with DPO though this process
900.12,5.0,is just done directly by having both the
902.6,4.52,reference model and by having the model
905.12,3.88,your training forward passing through
907.12,4.399,both of them let's say for the chosen
909.0,5.04,answer and then penalizing the
911.519,4.88,probability of the chosen answer in a
914.04,4.64,way that it will be incentivized to be
916.399,4.081,increased if it's the chosen one versus
918.68,4.24,the reference model or it will be
920.48,4.919,incentivized to decrease the probability
922.92,4.68,of the rejected one versus the reference
925.399,4.36,model let me quickly go through the
927.6,4.0,materials we need before we do the DPO
929.759,3.921,training the first thing we need is a
931.6,3.479,data set and it needs to be formatted
933.68,4.079,correctly with the prompt then the
935.079,5.081,chosen answer and then the rejected
937.759,4.44,answer we're going to use anthropics
940.16,4.2,helpful and harmless reinforcement
942.199,6.08,learning data set it's presented with
944.36,6.36,only Chosen and rejected rows uh rer
948.279,5.48,columns so let's just take a look at the
950.72,5.76,data you can see there's a chosen uh
953.759,4.08,prompt here and the question is if
956.48,3.44,you're going to steal from a convenience
957.839,4.161,store and the assistant gives a harmless
959.92,5.76,answer which is I really couldn't say
962.0,6.079,and then the rejected answer is a more
965.68,4.8,um Intense or bad answer same question
968.079,4.401,same prompt but the assistant says uh
970.48,4.08,it's good to consider the difference in
972.48,4.599,human traffic at night and different
974.56,3.839,reasoning there that you want to reject
977.079,3.481,as I mentioned this isn't the right
978.399,4.961,format so I have reformatted it here
980.56,4.36,into a data set um you can purchase
983.36,3.52,access to this if you like you'll find
984.92,3.96,the link on the repo you'll see there's
986.88,5.12,the chosen the rejected and the The
988.88,4.68,Prompt columns just to go through here
992.0,3.8,we have a prompt if you were to steal
993.56,4.32,from a convenience store and then you
995.8,5.279,have the Chosen and the rejected note
997.88,5.759,that I formatted it for Lama um with the
1001.079,4.081,inst at the start and at the end and
1003.639,3.281,I've also put in an end of sequence
1005.16,3.599,token I don't really think this is
1006.92,5.8,critical because ultimately we're just
1008.759,5.601,comparing the probabilities of uh of the
1012.72,4.28,model being trained with the reference
1014.36,3.959,model and we're adjusting where there is
1017.0,2.399,a difference or rather to create a
1018.319,3.08,differ difference to increase the
1019.399,3.841,likelihood of the chosen response and
1021.399,3.121,reduce the likelihood of the rejected
1023.24,3.4,response so I'm not sure the end of
1024.52,4.08,sequence token is required but just for
1026.64,4.399,consistency I think it's good to format
1028.6,5.439,it like this the next thing we're going
1031.039,4.64,to need is a reference model I have a
1034.039,3.88,reference model here which is the tiny
1035.679,5.321,Lama model I said I would be training
1037.919,5.04,on this is a model that I have trained
1041.0,5.28,with supervised finetuning using the
1042.959,6.161,open assist data set it's available here
1046.28,5.08,um for you to download if you like and
1049.12,4.24,as I said it's a chat fine-tuned model
1051.36,3.96,tiny Lama is only trained on a 2K
1053.36,4.72,context length I did the fine-tuning
1055.32,4.719,using 4K context it helps a little bit
1058.08,4.08,with the 2K performance but it's not
1060.039,4.52,good enough to get the model to be a 4K
1062.16,5.04,data set so we'll be considering it a 2K
1064.559,6.521,data set even though that I call it 4K
1067.2,6.719,because I used 4K long or up to 4K long
1071.08,5.52,uh data set which is open
1073.919,5.561,assist and with that I think we're ready
1076.6,5.28,to move on and do some fine-tuning in
1079.48,4.319,the notebook you can get access to the
1081.88,4.64,notebook by purchasing it through the
1083.799,5.321,link below or you can purchase access to
1086.52,5.32,the full Advanced fine-tuning repository
1089.12,4.679,which now has not just DPO but scripts
1091.84,5.24,for embeddings function calling training
1093.799,5.36,long context uh quantization supervised
1097.08,3.8,fine-tuning and unsupervised fine tuning
1099.159,3.361,quite a lot of scripts in that advanced
1100.88,3.679,fine-tuning repo and quite a lot of
1102.52,4.639,members of it right now all right let's
1104.559,4.961,get started and here we are with the
1107.159,4.041,direct preference optimiz ation notebook
1109.52,5.56,I'm going to take you through it step by
1111.2,6.479,step so right up the top we are going to
1115.08,4.599,connect with hugging face this will
1117.679,3.721,allow us to push models and also if
1119.679,4.24,you're accessing a or fine-tuning a
1121.4,4.759,gated model it's necessary to do the
1123.919,4.041,login lately I've always been also
1126.159,4.0,logging in with weights and biases it
1127.96,3.68,allows us to view the data and it saves
1130.159,3.921,the data for the future so you have it
1131.64,4.44,nicely organized for runs this is really
1134.08,4.8,handy because I use run pad oftenly
1136.08,5.24,often and it saves the data even if I
1138.88,5.039,shut down the Pod so I've connected with
1141.32,4.12,weights and biases optionally if you
1143.919,2.961,have the model downloaded to Google
1145.44,4.4,Drive say you're working in Google
1146.88,5.679,collab you can connect Google
1149.84,4.959,Drive next we'll do the installation
1152.559,4.761,note that I'm installing flash detention
1154.799,5.441,this is going to be the V2 and that will
1157.32,6.239,accelerate the training process it
1160.24,6.0,speeds up doing the attention part of
1163.559,5.36,the forward
1166.24,5.36,pass uh I additionally now typically run
1168.919,4.401,this command here to get the Transformer
1171.6,2.8,Transformers environment information
1173.32,2.8,this is useful if you're ever
1174.4,3.88,troubleshooting and you need to post an
1176.12,3.919,issue on GitHub it tells you all about
1178.28,4.0,the platform you're
1180.039,4.481,running so we'll move on now to loading
1182.28,4.759,the model the model I'll load is the
1184.52,5.92,tiny llama model in fact let me just
1187.039,7.041,increase the size of my font
1190.44,6.479,here something like
1194.08,5.4,this I am not going to load a quantise
1196.919,4.321,because it's only a small model um so
1199.48,3.76,I've commented out the
1201.24,3.919,quantization I'm not using any rope
1203.24,4.28,scaling I've set the Dei device map to
1205.159,5.52,Auto which means that the layers will be
1207.52,8.8,put onto the GPU I'm running with an
1210.679,10.041,a6000 here it's got 45 48 gab of ram um
1216.32,7.2,and that easily fits in the total model
1220.72,5.92,which is about 2 gbt in size um in fact
1223.52,5.039,if it's run in BF 16 yeah it's about 2
1226.64,3.519,GB in size because it has about billion
1228.559,3.961,parameters in tiny
1230.159,3.481,Lama you can see here that I'm using
1232.52,3.32,flash
1233.64,3.72,attention and this will reduce the
1235.84,4.12,memory
1237.36,5.28,requirements now you can load a me
1239.96,4.8,reference model by loading an exact copy
1242.64,3.48,but actually the trainer will handle
1244.76,2.799,that for us so I'm not going to load a
1246.12,3.28,reference model I'll just load this
1247.559,5.321,single model
1249.4,6.159,here next up I'll load the tokenizer I
1252.88,4.32,can run a quick uh test generation just
1255.559,3.841,to see that the model is working
1257.2,3.76,correctly which it is
1259.4,3.279,um note that I'm only going to train
1260.96,3.88,Lowa parameters here so I'll train a
1262.679,5.281,Lura adapter I'm not going to train any
1264.84,4.6,non Lowa parameters um typically I would
1267.96,3.319,list out all of the parameters in the
1269.44,5.16,model and turn on training for some of
1271.279,5.201,those in non Lura format but I don't do
1274.6,4.04,that here it's necessary if you want to
1276.48,3.559,train for longer context that's not what
1278.64,3.48,we're doing here so I'm just going to
1280.039,4.321,train the Lowa parameters which I'm
1282.12,3.799,going to apply in The Next Step so here
1284.36,4.0,I'm enabling gradient
1285.919,3.681,checkpointing and I have a function to
1288.36,3.679,print the trainable
1289.6,4.079,parameters I'm printing out the model
1292.039,4.201,just to show you exactly what I'll be
1293.679,6.401,training within the attention I'll train
1296.24,7.28,the Q KV and O and I'll also train in
1300.08,6.36,the linear lays the gate projection up
1303.52,4.6,down I'm not going to tr train the input
1306.44,3.119,lay armm which would not be done through
1308.12,4.0,Laura I'm not going to train the post
1309.559,4.281,detention L armm I'm not going to train
1312.12,3.4,the embeddings either although training
1313.84,3.719,those will be important if you want to
1315.52,3.6,train for longer contexts which is not
1317.559,4.161,what we're doing
1319.12,5.799,here so indeed you can see here the
1321.72,4.959,modules that I've chosen to train and
1324.919,4.721,I'm applying the adapter now to the
1326.679,5.24,model so that we are training the Laura
1329.64,4.72,config this results in US training a lot
1331.919,5.721,less parameters than we normally would
1334.36,5.799,if we were doing a full fine tune now
1337.64,4.919,Laura fine tunes generally perform just
1340.159,3.961,as well so hopefully we'll see that's
1342.559,4.401,the case here as
1344.12,5.039,well next up uh we'll set up this the
1346.96,6.12,tokenizer and pass PD
1349.159,7.281,in so I've just loaded the tokenizer
1353.08,6.64,here and I'm going to add a padding
1356.44,4.88,token you can use the unknown token as
1359.72,3.4,padding but since the supervised
1361.32,5.68,fine-tuning base model I'm using has
1363.12,5.76,been trained with pad token like this um
1367.0,4.919,I'm going to keep consistent here and
1368.88,3.039,use that same pad
1372.64,8.039,token okay so we'll continue on
1376.96,6.079,here and you can see that the special
1380.679,4.48,tokens are the beginning of sequence the
1383.039,3.721,end of sequence the unknown token and
1385.159,4.12,the pad
1386.76,4.88,token next up we need to set up some
1389.279,5.241,evaluation so we can evaluate the model
1391.64,4.2,before we do the DPO and afterwards and
1394.52,2.92,it's important here you set some
1395.84,3.68,questions that are relevant to the data
1397.44,3.839,set that you choose ideally they should
1399.52,4.0,provide you some Metric of how the model
1401.279,4.161,is performing we're trying to fine tune
1403.52,4.8,here to have a model that is more
1405.44,5.76,harmless so we're going to have to ask
1408.32,5.719,some ugly questions and then we want to
1411.2,4.839,see that it gives what we'd consider an
1414.039,3.921,an unacceptable answer before the fine
1416.039,4.52,tuning and gives hopefully a more
1417.96,5.44,acceptable answer after the
1420.559,6.24,DPO so I've set up uh some questions
1423.4,6.2,here um yeah unfortunately a lot of this
1426.799,5.801,kind of training involves ugly questions
1429.6,6.079,um which you will ask the model and when
1432.6,5.439,you run that base evaluation here it's
1435.679,5.6,bluntly answering the question which you
1438.039,5.321,may not want um if you want to have a
1441.279,4.561,model that is going to be better behaved
1443.36,4.24,or aligned but this provides us with a
1445.84,3.839,baseline then we can then that we can
1447.6,4.559,then compare after the training and
1449.679,5.081,hopefully this data set will have the
1452.159,4.921,effect that we
1454.76,3.96,intend okay so we're loading the data
1457.08,4.839,set that I just took you through it's
1458.72,6.36,the DPO um helpful and harmless
1461.919,6.48,reinforcement learning data set from
1465.08,6.92,anthropic so that's loaded and here you
1468.399,6.601,can see there is a prompt talking about
1472.0,6.12,dinosaurs so this is a more harmless
1475.0,5.679,question um but it ends with the user
1478.12,4.439,accusing the machine of not being able
1480.679,3.961,to read and the harmless answer is you
1482.559,4.401,can read question mark and rejected
1484.64,4.68,answer is a more aggressive retort which
1486.96,6.88,is there's a lot of stuff humans don't
1489.32,6.92,know okay so we'll move on and I've just
1493.84,4.4,got a test here where I tokenize check
1496.24,3.84,the token IDs you can see that a special
1498.24,4.72,token that's the beginning of sequence
1500.08,5.28,token is added at the start um so
1502.96,4.68,everything looks correct
1505.36,4.799,here now we're going to move on to the
1507.64,4.639,trainer I'm going to use an evaluation
1510.159,5.081,set and a training set and the
1512.279,5.161,evaluation set is the test split it's
1515.24,3.08,quite a large split I think it's got
1517.44,5.119,over
1518.32,6.0,"10,000 um rows and just for the sake of"
1522.559,4.12,running more quickly I'm going to reduce
1524.32,4.0,that so we'll only run with
1526.679,5.041,960
1528.32,5.8,of the test data set rows I'm going to
1531.72,4.72,run for 0.1 epox on the train data set
1534.12,5.72,the train data set is 160k so it'll be
1536.44,4.92,"out a 16,000 row training that we'll do"
1539.84,3.8,on the training side and on the test
1541.36,5.0,side it's about 960
1543.64,4.72,rows okay setting some parameters here
1546.36,5.16,uh the context length will be 2K which
1548.36,5.4,is the tiny Lama context length I'm
1551.52,5.36,setting the batch size to eight this
1553.76,6.519,does fit on an a6000 you can fit eight
1556.88,5.36,"batches of 2,000 tokens um you can maybe"
1560.279,4.041,fit a bit more you probably could fit 12
1562.24,4.08,I think and usually the way I set
1564.32,4.0,gradient accumulation is to have the
1566.32,4.599,batch size times the gradient
1568.32,6.28,accumulation be about 32 that means that
1570.919,6.0,basically you're doing 32 forward passes
1574.6,5.079,um before accumulating the updates and
1576.919,5.441,then passing them backwards through the
1579.679,5.521,model all right so moving on here to the
1582.36,5.4,training arguments mostly we have
1585.2,4.359,already specified them I'm going to do
1587.76,4.279,evaluation steps every quarter of the
1589.559,4.761,full run so there should be three in uh
1592.039,4.161,intermediate evaluation steps the same
1594.32,3.839,with save steps this will save the
1596.2,4.12,adapters the lower adapters in case the
1598.159,4.12,Run crashes I'll at least have the
1600.32,5.04,adapters
1602.279,4.76,saved I'm setting uh a scheduler here
1605.36,3.039,type to constant so that means I've got
1607.039,3.561,constant learning rate throughout the
1608.399,4.681,Run sometimes people will decrease that
1610.6,4.04,over time um I think with a small amount
1613.08,4.319,of data you can probably just do a
1614.64,4.12,constant um linear scheduler or sorry a
1617.399,3.721,con constant scheduler but you might
1618.76,3.919,consider linear or cosign if you're
1621.12,3.679,doing a longer run with a large amount
1622.679,4.041,of data now one of the most important
1624.799,5.321,parameters here is the learning rate
1626.72,8.0,typically this might be 1 E minus 5 for
1630.12,6.24,doing lower of fine tunes but with DPO
1634.72,3.959,it's found that you need to have an even
1636.36,5.199,lower learning rate it's recommended
1638.679,5.281,maybe 5e minus 7 I've got it slightly
1641.559,3.761,more here which is 1 eus 6 and I'll
1643.96,5.56,actually show you an example of where it
1645.32,4.2,diverges when I try doing 1 E minus 5
1650.0,4.679,okay so with that um the trainer can be
1652.76,3.76,called you can see I'm passing in the
1654.679,5.521,model I'm not passing in a reference
1656.52,6.48,model because the trainer will create a
1660.2,6.0,copy uh automatically as a
1663.0,6.279,reference I'm setting the beta of
1666.2,4.719,0.1 if the beta was Zero then it would
1669.279,4.24,basically ignore the reference model if
1670.919,5.681,the beta is 0.5 I think it's going to
1673.519,5.241,very strongly consider the model so um
1676.6,4.28,or sorry it could be more than 0.5 but
1678.76,5.12,the higher beta is the more that the
1680.88,4.72,reference model is considered I.E um you
1683.88,3.399,can think of the model you're training
1685.6,4.12,being more Tethered to the starting
1687.279,4.441,point um so it's recommended to have it
1689.72,3.24,between 0.1 and 0.5 I haven't played
1691.72,2.439,around a lot with this but I'm leaving
1692.96,3.68,it
1694.159,5.0,at1 then I'm passing in the training set
1696.64,4.919,and the test set and then the tokenizer
1699.159,5.64,and then I'm starting to
1701.559,4.881,train now this model has already been
1704.799,4.24,trained so I've gone all the way through
1706.44,5.88,a th000 steps
1709.039,4.36,um this training has been completed and
1712.32,4.079,I'm going to walk you through some of
1713.399,5.16,the results it's going to be easiest
1716.399,4.721,probably to look in weights and biases
1718.559,4.96,because we can see the full graphs over
1721.12,6.32,there and once we're done with the
1723.519,5.561,training uh we can plot so we can show
1727.44,3.719,the training loss and the validation
1729.08,4.28,loss really you don't see a whole lot of
1731.159,5.561,change in the model here uh the learning
1733.36,4.799,rate is very slow so with this uh
1736.72,4.24,relatively small small amount of data
1738.159,4.76,and such a low training uh learning rate
1740.96,5.36,there's very little change in the model
1742.919,6.12,over the um over the over the training
1746.32,4.68,period and indeed when I look here at
1749.039,4.721,the base model evaluation after the
1751.0,4.88,training unfortunately in this case the
1753.76,4.639,data hasn't been enough to give me much
1755.88,7.679,change or any change in the answer that
1758.399,8.52,I'm getting so still the model is not
1763.559,5.641,giving um a more restrained response to
1766.919,4.64,the more dangerous questions so we
1769.2,4.319,haven't achieved the goal here with DPO
1771.559,4.401,meaning that we would need to train
1773.519,5.201,perhaps for more uh more of an Epoch
1775.96,5.12,I've only trained for 0.1 epochs it's
1778.72,4.76,possible that simply the kind of nature
1781.08,4.4,of this question I'm asking here hasn't
1783.48,4.199,being covered in the training data set
1785.48,3.72,so it's not being affected statistically
1787.679,4.041,by any of the samples that we've run
1789.2,4.56,through in DPO so running for a full
1791.72,4.48,Epoch would be one way to start with
1793.76,5.279,this and maybe playing as well with
1796.2,5.079,having a little bit higher of learning
1799.039,4.201,rate so I'm going to show you the
1801.279,4.601,example I have with a little higher
1803.24,4.36,learning rate where I have 1 E minus 5
1805.88,3.12,I'll show you how it diverges but also
1807.6,2.84,it actually gives you a sense for the
1809.0,4.559,effect it has in the model because it
1810.44,5.32,does start to change the answer so right
1813.559,4.6,here with the 1 E minus 5 everything is
1815.76,4.84,very same the only difference is that
1818.159,4.52,when I ran the training I had it at a
1820.6,4.559,learning rate of 1 E minus 5 Which is
1822.679,5.72,higher than 1 eus 6 by factor of
1825.159,6.4,10 and indeed when I run that training
1828.399,4.481,and then I run the first sample here um
1831.559,3.801,you can see the answer is quite
1832.88,6.919,different now so when it's asked about
1835.36,9.24,killing it starts to give an answer that
1839.799,9.12,um is a bit more restrained it's um now
1844.6,6.76,asking that people's um human rights and
1848.919,4.76,emotions be considered it's giving a
1851.36,4.72,much more verbose answer trying to
1853.679,4.24,indicate more Nuance around the topic I
1856.08,4.559,wouldn't say that it's got
1857.919,5.161,the answer to as good of a level as we
1860.639,4.28,would hope for from a production level
1863.08,4.52,product but still you can clearly see
1864.919,4.76,that the answer is no longer just
1867.6,3.6,directly uh providing dangerous
1869.679,5.041,recommendation on this question but
1871.2,6.64,rather giving a little more
1874.72,5.88,Nuance let's move over and um take a
1877.84,4.36,quick look at the results in fact even
1880.6,4.16,just looking at the training and the
1882.2,4.719,eval loss which should be kind of slowly
1884.76,4.879,falling you can see in this case the
1886.919,5.12,train in loss starts to diverge um it's
1889.639,6.04,bouncing around pretty sharply because
1892.039,5.321,my learning rate is so high and the eval
1895.679,4.0,loss I wouldn't say it's getting
1897.36,5.0,terrible but it's it's going up a little
1899.679,4.161,bit but this kind of instability here
1902.36,4.159,indicates that probably I have my
1903.84,4.48,learning rate a little bit too high so
1906.519,4.681,what we'll take a quick look at now uh
1908.32,4.719,to finish off is the weights and biases
1911.2,3.839,training when you've connected up
1913.039,3.841,weights and biases it'll automatically
1915.039,4.24,generate all of these plots you can see
1916.88,5.519,here what the GPU was doing during this
1919.279,5.921,training session it was about 50 minutes
1922.399,6.24,um actually I trained on an a100 here
1925.2,6.319,when I was running with 1 to the E minus
1928.639,4.92,6 and right up the top you can see some
1931.519,4.76,evaluation results it's showing four
1933.559,6.521,plots here that I'll explain a little
1936.279,6.24,bit um the two I want to focus on are uh
1940.08,3.92,the margins plot and the accuracies plot
1942.519,3.28,the accuracy plot is probably the
1944.0,3.36,easiest to understand I know it's small
1945.799,5.681,font here let me see if if I could
1947.36,4.84,increase actually just this yeah I think
1951.48,5.48,that
1952.2,6.839,helps so the accuracies tells us given
1956.96,5.12,given a chosen response and giving a
1959.039,4.681,rejected response what's the likelihood
1962.08,4.479,that the trained model will pick the
1963.72,5.28,chosen rather than the rejected so
1966.559,5.521,ideally you would like the model to have
1969.0,6.039,a high probability more than 50% of
1972.08,4.4,choosing the chosen Mo response and have
1975.039,2.561,a low probability of choosing the
1976.48,2.799,rejected one
1977.6,3.52,and so you can see here throughout the
1979.279,4.12,training there's some oscillation but at
1981.12,5.08,the end of the training in fact not at
1983.399,5.4,the end but at the 750th step you can
1986.2,3.439,see that um it's got a slightly higher
1988.799,4.12,than
1989.639,6.441,50% chance of picking the chosen rather
1992.919,5.961,than the rejected so indeed in this case
1996.08,5.04,the model has been biased towards the
1998.88,5.279,chosen response but you can see the bias
2001.12,4.96,is not very strong because it's 5135 so
2004.159,3.961,it's only a little bit more than half
2006.08,4.839,and so you would need to take the model
2008.12,5.84,quite a bit further if um you wanted to
2010.919,5.801,have the bias um definitely move towards
2013.96,5.719,the chosen response the next graph to
2016.72,6.04,look at is the margins graph this is the
2019.679,5.081,difference in the reward for the chosen
2022.76,4.519,minus the reward for the rejected so
2024.76,4.36,ideally we want a positive margin here
2027.279,5.961,you can see that actually the margin is
2029.12,6.279,negative and this is an average across
2033.24,5.08,um the whole evaluation so there were
2035.399,5.601,about 960 RADS in that
2038.32,4.839,evaluation and um I think that's why
2041.0,3.6,it's possible to have a negative margin
2043.159,3.681,when you're averaging across all of
2044.6,3.92,those according to Value whereas the
2046.84,5.12,accuracies are averaging across
2048.52,7.28,according to the number that favor the
2051.96,6.08,chosen over the rejected so you can see
2055.8,4.279,that in any case we're not seeing a very
2058.04,4.039,strong bias of the model unfortunately
2060.079,4.241,we would like to see a positive margin
2062.079,5.04,that shows us a clear bias in a value
2064.32,5.039,sense and we'd like to see a clear bias
2067.119,5.76,as well in terms of the number of
2069.359,5.76,responses that are chosen over
2072.879,3.841,rejected and just for two more quick
2075.119,3.881,graphs you can always keep an eye on the
2076.72,4.639,training and the eval graphs the
2079.0,4.679,training loss should be going down over
2081.359,3.681,time as there should be similarity
2083.679,3.92,between all of the different samples
2085.04,4.28,we're trying to train on so this should
2087.599,3.601,be dropping very slowly you can see it
2089.32,5.2,looks pretty constant over this range of
2091.2,5.479,training and then eval um this number
2094.52,4.64,here should also be falling you can see
2096.679,6.121,here that it's gone from around
2099.16,5.919,6929 to 6935 so there's really not any
2102.8,5.039,change here in the eval loss it looks
2105.079,5.161,like some change cu the Y AIS is very um
2107.839,3.881,compressed or very zoomed in on in fact
2110.24,4.8,we're just not seeing a whole lot of
2111.72,5.6,training within the short number of
2115.04,4.72,epoch before I go I'm just going to show
2117.32,6.279,you exactly how I set things up on runp
2119.76,6.4,pod um what I'll do here is use an a6000
2123.599,4.961,so if I go to secure Cloud I should see
2126.16,6.199,a list of all of the servers that are
2128.56,5.519,available like this and this I think is
2132.359,4.601,probably one of the best values per unit
2134.079,5.76,of vram it's got 48 vram for 79 cents
2136.96,5.28,per hour um I'll before I click deploy
2139.839,4.041,using a pytorch instance I'm just going
2142.24,5.359,to make sure I have plenty of memory
2143.88,5.36,here um 50 GB is way more than enough
2147.599,4.361,because the model is going to be about 2
2149.24,3.839,gab and I'll click continue and then
2151.96,3.879,I'll
2153.079,5.121,deploy so this is now going to deploy a
2155.839,4.76,p p torch
2158.2,4.48,instance and you can see I have an
2160.599,4.561,exited instance of an a100 that I was
2162.68,4.72,working on earlier and once this is
2165.16,4.48,booted up I'll be able to connect using
2167.4,5.36,a Jupiter
2169.64,5.64,notebook once the notebook uh is ready
2172.76,4.4,once the Pod is ready I'll click connect
2175.28,5.96,and I'll click connect to
2177.16,6.32,Jupiter I've just uploaded a dp. iynb
2181.24,4.28,notebook and I'll open it here in the
2183.48,4.4,screen and I'm going to go through the
2185.52,4.799,steps that I discussed earlier starting
2187.88,5.4,off by and logging in with my hugging
2190.319,6.321,face ID so here we have the hugging face
2193.28,6.4,ID then doing the installation of
2196.64,4.959,weights and biases and using my login
2199.68,6.04,for that so that we have
2201.599,7.281,logs so here's my weights and biases
2205.72,6.32,ID and next up we will do the
2208.88,5.6,installation so install all of packages
2212.04,4.84,necessary move on to loading the model
2214.48,3.72,here as I said not loading at quantize
2216.88,3.92,because it's a small model so there
2218.2,4.52,shouldn't be any issue with doing that
2220.8,3.08,and I'm going to prepare the model then
2222.72,3.96,for
2223.88,4.84,fine-tuning and stick with this
2226.68,5.76,selection here of Target modules for
2228.72,6.48,that fine tuning initiate the
2232.44,5.0,tokenizer and once the tokenizer is done
2235.2,4.76,we'll move towards setting up
2237.44,5.0,evaluation printing some evaluation
2239.96,3.879,results and then loading the data set
2242.44,4.28,that we're going to use for
2243.839,6.161,training now I'm going to make a little
2246.72,5.8,alteration here this time around I'm not
2250.0,5.72,going to trim the data set so I'm going
2252.52,6.28,to use the full eval data set which is
2255.72,5.84,"about 10,000 so this is going to be a"
2258.8,5.36,slower training run because I'm going to
2261.56,6.08,use the full eval data
2264.16,8.199,set also here um I'm going to run for a
2267.64,7.28,full Epoch so that should be um an
2272.359,6.801,improvement on what was previously
2274.92,4.24,done now let's just just check
2279.72,5.08,here to make sure the data set is
2282.88,5.56,imported it looks like I have an error
2284.8,7.0,for loading up here and unfortunately I
2288.44,4.24,forgot to run this one cell here which
2291.8,3.319,was
2292.68,4.72,necessary so now that I've run cached
2295.119,4.121,here I'll be able to run through all of
2297.4,4.08,these
2299.24,6.28,again by the lower
2301.48,7.839,adapter set up the tokenizer set up the
2305.52,7.28,evaluation Lo the data set but this time
2309.319,5.76,not truncate it as I previously had done
2312.8,4.08,so that's why I've commented out here
2315.079,4.121,where I've reduced the range of the EV
2316.88,4.479,Val data set and then increased to one
2319.2,5.84,Epoch for the
2321.359,3.681,training so let's
2326.64,6.56,see and we have a learning rate
2329.76,4.72,schedular type of a constant I'm
2333.2,5.44,actually going to move back to the
2334.48,6.32,original um recommended learning rate
2338.64,4.56,well you know I might leave 1 E minus 6
2340.8,4.279,but instead of using constant I think I
2343.2,4.159,will use linear here so the learning
2345.079,3.321,rate will decrease as we move through
2347.359,3.72,all of the
2348.4,6.88,epoch and you can see I've set up the
2351.079,8.0,results folder here for one Epoch and
2355.28,5.4,I'll go ahead and get started now to
2359.079,5.361,train the
2360.68,6.52,model it should set up awaits and biases
2364.44,5.0,"run you can see here it's got 10,000"
2367.2,5.04,"steps to go through instead of 1,000 so"
2369.44,5.879,it's going to be quite a bit longer as a
2372.24,5.079,training and I'm going to install my
2375.319,5.76,plot lib so I can plot the results at
2377.319,8.081,the end I'll do a quick
2381.079,9.121,evaluation this here I can
2385.4,9.12,remove can remove all of these and I
2390.2,7.52,will push the model to hugging face so
2394.52,7.88,that I have a copy of it
2397.72,6.32,so I'll just go ahead and push that
2402.4,3.64,adapter
2404.04,4.64,and
2406.04,4.2,additionally I don't need to upload any
2408.68,3.919,other trainable parameters because
2410.24,4.96,they're all included in the Laura
2412.599,6.401,adapter so I should be free to just
2415.2,6.68,merge and unload the model and then I
2419.0,5.52,should be free to push the tokenizer to
2421.88,6.959,hugging face Hub and push the model to
2424.52,6.799,hugging face hub um what I might do is
2428.839,4.881,push tokenizer do model which is a file
2431.319,6.961,in the original repo that file is needed
2433.72,7.28,to do gptq and also ggf quantizations so
2438.28,5.92,it can be handy to just push that as
2441.0,6.839,well so I'll scroll up here
2444.2,7.04,now and take a look at the
2447.839,5.601,training which should be
2451.24,5.96,underway and here the trainer is up and
2453.44,8.08,running and you can see we're now 17
2457.2,8.36,"steps into 10,000 uh step run it's"
2461.52,6.88,about um it's about an 8 Hour run so
2465.56,4.72,fairly long even though this uh is quite
2468.4,3.679,a small model just a 1 billion parameter
2470.28,3.36,model so just gives you a feel for the
2472.079,4.321,amount of time if you're doing a full
2473.64,5.6,fine tune U rather if you're doing a
2476.4,5.679,lower fine tune on the model and here we
2479.24,5.96,can just click on the weights and
2482.079,5.881,biases and we should be able to pull up
2485.2,8.08,a copy of this run
2487.96,9.72,and you can see here training loss is
2493.28,6.68,progressing and I'm going to just go to
2497.68,6.84,overview and just put in here what the
2499.96,8.68,run is about one e minus 6 and we'll say
2504.52,4.12,one e but run
2510.76,5.28,tiny like
2512.56,4.799,this and that's it I'll post up later
2516.04,4.36,how the Run
2517.359,5.521,goes that folks is an overview of direct
2520.4,4.64,preference optimization it is a lot
2522.88,3.6,easier than doing reinforcement learning
2525.04,4.68,where you need to train a separate
2526.48,5.92,helper model still doing DPO direct
2529.72,4.56,preference op optimization it's not an
2532.4,3.679,easy feat and you need to pay a lot of
2534.28,4.039,attention to having a data set that's
2536.079,4.601,comprehensive choosing some questions
2538.319,4.24,for evaluation that allow you to tell
2540.68,4.8,whether your training is progressing or
2542.559,5.52,not you also need to be comfortable with
2545.48,4.119,chat find tuning a model which is a form
2548.079,3.721,of supervised
2549.599,3.841,fine-tuning if you want to get started
2551.8,3.68,with something a little easier I
2553.44,4.76,recommend going back over the videos for
2555.48,5.359,embeddings unsupervised fine tuning and
2558.2,4.639,supervised fine tuning with respect to
2560.839,4.081,this video I'll put all of the links on
2562.839,4.161,resources below and you can find out
2564.92,6.04,more about getting access to the scripts
2567.0,3.96,and data sets cheers
